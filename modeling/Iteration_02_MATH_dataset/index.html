
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://ironbar.github.io/aimo/modeling/Iteration_02_MATH_dataset/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.15">
    
    
      
        <title>Iteration 2. MATH dataset - aimo</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.c382b1dc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.cc9b2e1e.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue" data-md-color-accent="blue">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#iteration-2-math-dataset" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="aimo" class="md-header__button md-logo" aria-label="aimo" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            aimo
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Iteration 2. MATH dataset
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/ironbar/aimo" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../01_Business_Understanding/" class="md-tabs__link">
      Business Understanding
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../02_Data_Understanding/" class="md-tabs__link">
      Data Understanding
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../03_State_of_the_art/" class="md-tabs__link">
      State of the art
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../" class="md-tabs__link md-tabs__link--active">
        Modeling
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../utils/00_Challenge_Workflow/" class="md-tabs__link">
        Utils
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="aimo" class="md-nav__button md-logo" aria-label="aimo" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    aimo
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ironbar/aimo" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../01_Business_Understanding/" class="md-nav__link">
        Business Understanding
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../02_Data_Understanding/" class="md-nav__link">
        Data Understanding
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../03_State_of_the_art/" class="md-nav__link">
        State of the art
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../">Modeling</a>
          
            <label for="__nav_4">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="Modeling" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Modeling
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Iteration_01_overfit/" class="md-nav__link">
        Iteration 1. Overfit the train set
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Iteration 2. MATH dataset
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Iteration 2. MATH dataset
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#goal" class="md-nav__link">
    Goal
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#motivation" class="md-nav__link">
    Motivation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#development" class="md-nav__link">
    Development
  </a>
  
    <nav class="md-nav" aria-label="Development">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#about-the-math-dataset" class="md-nav__link">
    About the MATH dataset
  </a>
  
    <nav class="md-nav" aria-label="About the MATH dataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#token-len-distribution" class="md-nav__link">
    Token len distribution
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#removing-problems-with-non-positive-integer-answers" class="md-nav__link">
    Removing problems with non positive integer answers
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluating-the-math-dataset" class="md-nav__link">
    Evaluating the MATH dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#about-inference-loop-repetitions" class="md-nav__link">
    About inference loop repetitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimal-stop-criteria-for-self-consistency" class="md-nav__link">
    Optimal stop criteria for self-consistency
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#speedup-inference-with-p100-instead-of-2xt4" class="md-nav__link">
    Speedup inference with P100 instead of 2xT4
  </a>
  
    <nav class="md-nav" aria-label="Speedup inference with P100 instead of 2xT4">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#oom-issues-with-p100" class="md-nav__link">
    OOM issues with P100
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-the-right-evaluation" class="md-nav__link">
    Using the right evaluation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results" class="md-nav__link">
    Results
  </a>
  
    <nav class="md-nav" aria-label="Results">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#first-evaluations-on-math-dataset" class="md-nav__link">
    First evaluations on MATH dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#this-challenge-is-a-race-in-disguise" class="md-nav__link">
    This challenge is a race in disguise
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#few-shot-prompt-sources" class="md-nav__link">
    Few-shot prompt sources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#number-of-shots" class="md-nav__link">
    Number of shots
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#effect-of-saving-kv-values" class="md-nav__link">
    Effect of saving KV values
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#effect-of-quantization" class="md-nav__link">
    Effect of quantization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#effect-of-temperature" class="md-nav__link">
    Effect of temperature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-different-prompts" class="md-nav__link">
    Using different prompts
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-evaluation" class="md-nav__link">
    Full evaluation
  </a>
  
    <nav class="md-nav" aria-label="Full evaluation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-the-effect-of-the-number-of-repetitions" class="md-nav__link">
    What is the effect of the number of repetitions?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-is-the-effect-of-the-number-of-output-tokens" class="md-nav__link">
    What is the effect of the number of output tokens?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variability-in-evaluations-vs-lb-variability" class="md-nav__link">
    Variability in evaluations vs LB variability
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    Conclusion
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#next-steps" class="md-nav__link">
    Next steps
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#todo" class="md-nav__link">
    TODO
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#future-work-on-google-cloud" class="md-nav__link">
    Future work on Google Cloud
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Iteration_03_prompt_engineering/" class="md-nav__link">
        Iteration 3. Prompt engineering
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Iteration_04_answer_validation/" class="md-nav__link">
        Iteration 4. Answer validation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Iteration_05_vllm/" class="md-nav__link">
        Iteration 5. VLLM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Iteration_06_different_models/" class="md-nav__link">
        Iteration n. Iteration_title
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Iteration_07_tweak_inference_parameters/" class="md-nav__link">
        Iteration n. Iteration_title
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Iteration_08_fine-tuning/" class="md-nav__link">
        Iteration 8. Fine-tuning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Iteration_09_hinting_the_model/" class="md-nav__link">
        Iteration 9. Giving hints to the model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Iteration_10_select_submission/" class="md-nav__link">
        Iteration 10. Select submissions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" data-md-state="indeterminate" type="checkbox" id="__nav_5" checked>
      
      
      
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5">
          Utils
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Utils" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Utils
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../utils/00_Challenge_Workflow/" class="md-nav__link">
        Challenge workflow
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../utils/markdown_cheatsheet/" class="md-nav__link">
        Markdown cheatsheet
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../utils/methodology/" class="md-nav__link">
        Methodology
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#goal" class="md-nav__link">
    Goal
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#motivation" class="md-nav__link">
    Motivation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#development" class="md-nav__link">
    Development
  </a>
  
    <nav class="md-nav" aria-label="Development">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#about-the-math-dataset" class="md-nav__link">
    About the MATH dataset
  </a>
  
    <nav class="md-nav" aria-label="About the MATH dataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#token-len-distribution" class="md-nav__link">
    Token len distribution
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#removing-problems-with-non-positive-integer-answers" class="md-nav__link">
    Removing problems with non positive integer answers
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluating-the-math-dataset" class="md-nav__link">
    Evaluating the MATH dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#about-inference-loop-repetitions" class="md-nav__link">
    About inference loop repetitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimal-stop-criteria-for-self-consistency" class="md-nav__link">
    Optimal stop criteria for self-consistency
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#speedup-inference-with-p100-instead-of-2xt4" class="md-nav__link">
    Speedup inference with P100 instead of 2xT4
  </a>
  
    <nav class="md-nav" aria-label="Speedup inference with P100 instead of 2xT4">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#oom-issues-with-p100" class="md-nav__link">
    OOM issues with P100
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-the-right-evaluation" class="md-nav__link">
    Using the right evaluation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results" class="md-nav__link">
    Results
  </a>
  
    <nav class="md-nav" aria-label="Results">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#first-evaluations-on-math-dataset" class="md-nav__link">
    First evaluations on MATH dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#this-challenge-is-a-race-in-disguise" class="md-nav__link">
    This challenge is a race in disguise
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#few-shot-prompt-sources" class="md-nav__link">
    Few-shot prompt sources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#number-of-shots" class="md-nav__link">
    Number of shots
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#effect-of-saving-kv-values" class="md-nav__link">
    Effect of saving KV values
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#effect-of-quantization" class="md-nav__link">
    Effect of quantization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#effect-of-temperature" class="md-nav__link">
    Effect of temperature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-different-prompts" class="md-nav__link">
    Using different prompts
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#full-evaluation" class="md-nav__link">
    Full evaluation
  </a>
  
    <nav class="md-nav" aria-label="Full evaluation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-the-effect-of-the-number-of-repetitions" class="md-nav__link">
    What is the effect of the number of repetitions?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-is-the-effect-of-the-number-of-output-tokens" class="md-nav__link">
    What is the effect of the number of output tokens?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variability-in-evaluations-vs-lb-variability" class="md-nav__link">
    Variability in evaluations vs LB variability
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    Conclusion
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#next-steps" class="md-nav__link">
    Next steps
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#todo" class="md-nav__link">
    TODO
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#future-work-on-google-cloud" class="md-nav__link">
    Future work on Google Cloud
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
  <a href="https://github.com/ironbar/aimo/edit/master/docs/modeling/Iteration_02_MATH_dataset.md" title="Edit this page" class="md-content__button md-icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>



<h1 id="iteration-2-math-dataset">Iteration 2. MATH dataset</h1>
<p><em>06/05/2024</em></p>
<h2 id="goal">Goal</h2>
<p>Can we improve LB score by focusing on MATH dataset?</p>
<h2 id="motivation">Motivation</h2>
<p>There is a <a href="https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/discussion/499464">conversation</a> on the forum that suggests that the correlation between train score and
leaderboard score is very weak. The host said:</p>
<blockquote>
<p>10 public problems were intentionally chosen to be at or above average difficulty. 50 test problems have a range of difficulties from relatively simple problems to those approaching national Olympiad level</p>
</blockquote>
<p>This implies that claiming the overall prize winner is going to be very hard, because progress on the leaderboard
is not expected to be linear because the difficulty of the problems is increasingly harder.</p>
<p>Futhermore the train set is very small, just 10 problems. Using a bigger dataset for validation will
allow to measure small improvements.</p>
<h2 id="development">Development</h2>
<h3 id="about-the-math-dataset">About the MATH dataset</h3>
<p>MATH is a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations.</p>
<p>The problems belong to these categories: algebra, counting and probability, geometry, intermediate algebra, number theory, prealgebra and precalculus. They also have levels of difficulty from 1 to 5. The problems are
written in latex and they have also a text response in latex.</p>
<p>The proposed division has 7500 train problems and 5000 test problems. If I were to evaluate all the
test problems once using Kaggle's hardware it would take 90 hours (current submission takes 9 hours to solve 50 problems with 10 repetitions each). So this is a problem. Evaluation is slow. But it is a problem that
all the teams are going to have. And I could use my resources from consultant job to pay for computation.
The prize is 130k$, it might be worth spending a few thousand dollars in evaluation to get the prize.
But I have to first find something that works and is scalable.</p>
<p>Links:</p>
<ul>
<li><a href="https://github.com/hendrycks/math?tab=readme-ov-file">Original repo of the MATH dataset</a></li>
<li><a href="https://www.kaggle.com/datasets/alejopaullier/aimo-external-dataset">Kaggle dataset with MATH and GSM8K datasets</a></li>
</ul>
<h4 id="token-len-distribution">Token len distribution</h4>
<p><img alt="input_token_len_distribution" src="../res/input_token_len_distribution.png" /></p>
<p><img alt="output_token_len_distribution" src="../res/output_token_len_distribution.png" /></p>
<p>As expected the more difficult problems have longer answers and descriptions. Very beautiful graph.</p>
<h4 id="removing-problems-with-non-positive-integer-answers">Removing problems with non positive integer answers</h4>
<p>I'm left with 4345 train problems and 2828 test problems. The distribution of problems is more or less
balanced except for level 1 that has half the problems than the other categories.</p>
<h3 id="evaluating-the-math-dataset">Evaluating the MATH dataset</h3>
<p>If making a single prediction per problem evaluating 566 test problems in 85 minutes. That is around 6 problems per minute.
GPU usage is high, so I don't think there is much room for improvement. I can run two evaluations in parallel
with good GPU usage.</p>
<p>Uncertainty in the results is big, f.e. for an accuracy of 54% the uncertainty is around 4%. Using the whole
test set will reduce the uncertainty below 2%, but evaluation would take 7 hours on my PC.</p>
<p>However if I only evaluate the problems of difficulty level 5 then evaluating 580 problems takes around 131 minutes.</p>
<h3 id="about-inference-loop-repetitions">About inference loop repetitions</h3>
<p>Sometimes I have seen repetitions on inference, f.e. the following text was repeated multiple times:</p>
<pre><code>We can then try each factor to see if it gives us a valid solution for $x$ and $y$.\n\nIf we try $10x + 15 = 1$, we get $10x = -14$, which is not possible since $x$ must be a positive integer.\n\nIf we try $10x + 15 = 2$, we get $10x = -13$, which is not possible.\n\nIf we try $10x + 15 = 7$, we get $10x = -8$, which is not possible.\n\nIf we try $10x + 15 = 14$, we get $10x = -1$, which is not possible.\n\nIf we try $10x + 15 = 49$, we get $10x = 34$, so $x = 3.4$, which is not an integer.\n\nIf we try $10x + 15 = 98$, we get $10x = 83$, so $x = 8.3$, which is not an integer.\n\nSo, we must have made a mistake somewhere. Let's go back and check our work.\n\nWe factored the left side of the equation as $(10x + 15)(y - 4) = 98$.\n\n
</code></pre>
<p>I have seen that Huggingface has tools such as <a href="https://discuss.huggingface.co/t/transformers-repetition-penalty-parameter/43638">repetition_penalty</a> or <code>no_repeat_ngram_size</code>.</p>
<p>However using those options can be dangerous. In the response some tokens will be repeated very often, using a penalty on them will hurt the accuracy of the model. Documentation is not
clear and I believe it is better to play with the temperature of the model than touching
those parameters.</p>
<h3 id="optimal-stop-criteria-for-self-consistency">Optimal stop criteria for self-consistency</h3>
<p>On the public notebook that won the early prize the stopping criteria was <code>count &gt; sqrt(n_tries</code>. When
I saw it I didn't liked and replaced it with a simpler majority check, f.e. if I was running 8 repetitions
I required the most voted answer to be 4 to stop sampling more answers. But I felt that it was not
a good allocation of resources.</p>
<p>I have implemented a better stop criteria based on statistics. My intuition says that I should
stop sampling more answers when the frequency of the most voted answer is statistically higher
than the second most voted answer. The beauty of this definition is that I can choose how much
statistical confidence I want to use: 80%, 90%, 95%...</p>
<p><img alt="stop criteria" src="../res/2024-05-14-13-54-11.png" /></p>
<p>I found this <a href="https://arxiv.org/html/2401.10480v1">paper</a> about the topic, but didn't have the time to read it because I believe my approach
is the correct one.</p>
<h3 id="speedup-inference-with-p100-instead-of-2xt4">Speedup inference with P100 instead of 2xT4</h3>
<p>Although using 2xT4 in parallel is likely to be faster than a a single P100, I could try using the P100
now because I know it is faster.</p>
<p>I have verified that when using 1024 output tokens on P100 I can use 1 or 2 shots, but using 3 or more
results on Out of Memory errors.</p>
<p>The inference speed is twice as 2xT4 as the information below shows.</p>
<pre><code class="language-bash">P100 [19:05&lt;00:00, 21.62s/it]
Correct: 18/53 (0.34 ± 0.13)
Unanswered: 0/53 (0.00 ± 0.00)
Wrong: 35/53 (0.66 ± 0.13)

T4x2 34/53 53/53 [42:16&lt;00:00, 47.87s/it]
Correct: 21/53 (0.40 ± 0.13)
Unanswered: 0/53 (0.00 ± 0.00)
Wrong: 32/53 (0.60 ± 0.13)
</code></pre>
<p>It is a little bit weird because apparently the token speed generation was not so different (<a href="../Iteration_01_overfit/#p100-vs-2xt4">18.7 vs 15.2</a>)</p>
<h4 id="oom-issues-with-p100">OOM issues with P100</h4>
<pre><code class="language-bash">2 shots
63/580 out of memory 10%
https://www.kaggle.com/code/ironbar/deepseekmath-with-code-interpreter?scriptVersionId=177616865

1 shot
0/580
https://www.kaggle.com/code/ironbar/deepseekmath-with-code-interpreter?scriptVersionId=177756625
</code></pre>
<p>Despite not having any OOM issues, the metrics are almost the same. That could point to the fact
that OOM errors might be happening similarly to achieving the max output tokens, thus not affecting the
accuracy of the model.</p>
<h3 id="using-the-right-evaluation">Using the right evaluation</h3>
<p>If I just want to evaluate the goodness of the base model is better to not aggregate the predictions.
For example if I make a single prediction on the 580 MATH level 5 problems I will get an uncertainty of around 4% on the accuracy metrics. If I make 4 predictions on each problem that uncertainty will drop to 2%.</p>
<p>In the other hand to evaluate the whole system I need to aggregate the predictions on each problem.
However this evaluation is much slower, because f.e. making 10 prediction on 50 problems will take a similar amount of time to the proposed evaluation in the previous paragraph, and the uncertainty will be huge: 13%
I need to use the whole dataset for evaluation, otherwise the uncertainty is too big. And that could take around 16 hours.</p>
<p>A model that has greater accuracy and less errors on individual problems evaluation will also have higher
accuracy when the inferences are aggregated per problem.</p>
<h2 id="results">Results</h2>
<h3 id="first-evaluations-on-math-dataset">First evaluations on MATH dataset</h3>
<p>I have evaluated 1/5 of the test dataset in around 85 minutes.</p>
<p><img alt="results_grouped_by_level" src="../res/2024-05-07-15-06-07.png" /></p>
<p>The accuracy of the model decreases with the level of difficulty. Maybe I should focus on problems
of level 3 and above. The averaged accuracy of those categories is 42%, which equals to 21 problems solved
on the public leaderboard.</p>
<p>How an ideal validation set should be?</p>
<ul>
<li>As similar as possible to the test set. Correlation should be as high as possible. Improvements on the
  validation set should translate to improvements on the test set.</li>
<li>The validation process should be as similar to the submission as possible. This implies that I should
  do many repetitions on each problem. Otherwise I won't be optimizing the same thing as in submission.</li>
<li>As precise as possible. A small dataset with high uncertainty does not allow to measure small improvements.</li>
</ul>
<p>This implies that I should select the levels with higher correlation to leaderboard, and use a lot of compute
to evaluate all the problems with a few repetitions.</p>
<h3 id="this-challenge-is-a-race-in-disguise">This challenge is a race in disguise</h3>
<blockquote>
<p>In the event of a tie, the Submission that was entered first to the Competition will be the winner.</p>
</blockquote>
<p>The private test set is tiny and thus uncertainty on the scores is very high. I have created a <a href="https://www.kaggle.com/code/ironbar/uncertainty-private-test-score?scriptVersionId=176648016">notebook</a>
to estimate the effect of uncertainty.</p>
<p>A team with a score of just 22 will have a 30% chance of equaling or winning a team with a score of 25.
Luck will play a big role in the competition. And reaching a good score first will be very valuable.</p>
<p>However this is calculated with the assumption that all the problems have the same difficulty, which is likely untrue.</p>
<h3 id="few-shot-prompt-sources">Few-shot prompt sources</h3>
<table>
<thead>
<tr>
<th>few-shot source</th>
<th>MATH5 accuracy</th>
<th>LB score</th>
</tr>
</thead>
<tbody>
<tr>
<td>MATH</td>
<td>30%</td>
<td>7</td>
</tr>
<tr>
<td>AIMO train</td>
<td>51%</td>
<td>20</td>
</tr>
<tr>
<td>MathInstruct</td>
<td>49%</td>
<td>21</td>
</tr>
</tbody>
</table>
<p>Using prompts with code clearly beats the text approach. On a following iteration I should try using
RAG to find similar problems.</p>
<h3 id="number-of-shots">Number of shots</h3>
<p>The table below shows the evaluation on Math level 5 problems with a single inference on each.</p>
<table>
<thead>
<tr>
<th>few-shot</th>
<th>runtime (min)</th>
<th>correct</th>
<th>unanswered</th>
<th>wrong</th>
<th>boxed_answers</th>
<th>mean code interpreter calls</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>131</td>
<td>34%</td>
<td>28%</td>
<td>38%</td>
<td>65%</td>
<td>0.9</td>
</tr>
<tr>
<td>2</td>
<td>131</td>
<td>33%</td>
<td>27%</td>
<td>40%</td>
<td>70%</td>
<td>0.9</td>
</tr>
<tr>
<td>3</td>
<td>148</td>
<td>33%</td>
<td>28%</td>
<td>39%</td>
<td>69%</td>
<td>0.9</td>
</tr>
<tr>
<td>5</td>
<td>164</td>
<td>36%</td>
<td>27%</td>
<td>37%</td>
<td>71%</td>
<td>0.9</td>
</tr>
</tbody>
</table>
<ul>
<li>The changes are not significative (at least without a pairwise comparison), uncertainty is around 4%</li>
<li>The only significative change is that boxed answers are more frequent</li>
</ul>
<h3 id="effect-of-saving-kv-values">Effect of saving KV values</h3>
<p>I have run the exact same evaluation with and without saving KV values. The first run was 131 minutes, and without saving KV values the time grew to 139 minutes. Thus it is a small difference (at least for 1 shot).
This opens the door to using an LLM server, however memory usage was almost the same.</p>
<h3 id="effect-of-quantization">Effect of quantization</h3>
<p>I have run the same evaluation with and without quantization. It was an experiment with 2 shots from AIMO train
dataset and 4 repetitions for each problem.</p>
<table>
<thead>
<tr>
<th>precision</th>
<th>runtime (min)</th>
<th>correct</th>
<th>unanswered</th>
<th>wrong</th>
</tr>
</thead>
<tbody>
<tr>
<td>16 bit</td>
<td>469</td>
<td>39%</td>
<td>23%</td>
<td>38%</td>
</tr>
<tr>
<td>4 bit</td>
<td>783</td>
<td>40%</td>
<td>22%</td>
<td>39%</td>
</tr>
</tbody>
</table>
<p>It seems that the quantization does not have any effect on precision, considering that the uncertainty
in the results is around 2%.</p>
<p>However it does affect inference speed, the model is 67% slower when quantized.</p>
<h3 id="effect-of-temperature">Effect of temperature</h3>
<p>I have made experiments with 2 shots from AIMO train dataset and 4 repetitions for each problem.
The only variation between experiments was the temperature.</p>
<table>
<thead>
<tr>
<th>temperature</th>
<th>runtime (min)</th>
<th>correct</th>
<th>unanswered</th>
<th>wrong</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>506</td>
<td>35%</td>
<td>24%</td>
<td>40%</td>
</tr>
<tr>
<td>0.5</td>
<td>508</td>
<td>39%</td>
<td>24%</td>
<td>37%</td>
</tr>
<tr>
<td>0.25</td>
<td>483</td>
<td>40%</td>
<td>23%</td>
<td>37%</td>
</tr>
<tr>
<td>0.12</td>
<td>492</td>
<td>40%</td>
<td>23%</td>
<td>37%</td>
</tr>
<tr>
<td>0.06</td>
<td>484</td>
<td>41%</td>
<td>23%</td>
<td>35%</td>
</tr>
<tr>
<td>0</td>
<td>469</td>
<td>39%</td>
<td>23%</td>
<td>38%</td>
</tr>
</tbody>
</table>
<p>The uncertainty of this metrics is around 2%, so temperature 1 is worse but the differences between
the other experiments might not be significative.</p>
<h3 id="using-different-prompts">Using different prompts</h3>
<table>
<thead>
<tr>
<th>prompts</th>
<th>correct</th>
<th>unanswered</th>
<th>wrong</th>
</tr>
</thead>
<tbody>
<tr>
<td>original code prompt</td>
<td>31%</td>
<td>29%</td>
<td>40%</td>
</tr>
<tr>
<td>original code prompt forced python</td>
<td>36%</td>
<td>28%</td>
<td>36%</td>
</tr>
<tr>
<td>original cot prompt</td>
<td>34%</td>
<td>23%</td>
<td>43%</td>
</tr>
<tr>
<td>original cot prompt forced python</td>
<td>40%</td>
<td>26%</td>
<td>34%</td>
</tr>
<tr>
<td>MATHCodeInstruct 2 shots</td>
<td>35%</td>
<td>22%</td>
<td>44%</td>
</tr>
<tr>
<td>AIMO_train 2 shots</td>
<td>40%</td>
<td>22%</td>
<td>38%</td>
</tr>
<tr>
<td>custom prompt</td>
<td>38%</td>
<td>25%</td>
<td>38%</td>
</tr>
</tbody>
</table>
<ul>
<li>Forcing python clearly improves the results, thus I should be able to improve results with my prompts.
  This is achieved by starting the response with the markdown format of python code.</li>
<li>No evidence that using few shot prompt gives best results, as the paper already says.</li>
<li>AIMO train prompts are better than MATHCodeInstruct, my hypothesis is because they favour to use code
  because the response starts with python code. On MATHCodeInstruct the response can start with some text.</li>
</ul>
<p>The uncertainty of this metrics is around 4%, and this are individual problem scores (no self-consistency results)</p>
<p>¿Could I craft better prompts that achieve more correct results and less wrong predictions?</p>
<h3 id="full-evaluation">Full evaluation</h3>
<p>I have prepared a full evaluation of the 580 MATH level 5 problems using the same configuration as the submission: 25 repetitions and a confidence level of 95% for stopping doing inference and returning an answer. I have done
two evaluations:</p>
<ol>
<li>Using the prompts from the early prize public notebook. The evaluation has taken 42 hours (21 hours on 2x3090 GPUs)</li>
<li>Using those prompts with forced python code and one custom prompt. The evaluation has taken 34 hours (17 hours on 2x3090 GPUs)</li>
</ol>
<table>
<thead>
<tr>
<th>prompts</th>
<th>MATH level 5 accuracy</th>
<th>LB score</th>
</tr>
</thead>
<tbody>
<tr>
<td>early prize</td>
<td>54%</td>
<td>16, 21</td>
</tr>
<tr>
<td>forced python</td>
<td>57%</td>
<td>15, 17, 18, 17</td>
</tr>
</tbody>
</table>
<p>The uncertainty on MATH is around ±4%, despite evaluating on 580 problems. On leaderboard there is an
uncertainty of ±7 problems. Notice that uncertainty is not equal to variability. We could have a deterministic
system that always scores the same on the public test set, but the expected score on the private test set is
related to the uncertainty.</p>
<p>I have run a pairwise comparison and there is no significative difference between the results.</p>
<p>This is very interesting, because in the previous section we saw significative differences when using
a single repetition for evaluation. Thus we see differences on a single repetition but no difference
when using multiple repetitions. <strong>This implies that we need to do the costly full evaluation to optimize our system</strong>.</p>
<h4 id="what-is-the-effect-of-the-number-of-repetitions">What is the effect of the number of repetitions?</h4>
<p><img alt="01_3_python_prompts" src="../res/2024-05-23-11-24-58.png" /></p>
<p><img alt="02_public_notebook_prompts" src="../res/2024-05-23-11-25-03.png" /></p>
<p>In both cases using more repetitions improves the results as expected. Also there are diminishing
returns as expected as well.</p>
<p>We can see significative differences up to 3 repetitions, but after that the difference is not significative.
So maybe we can do evaluations with 4-5 repetitions and the results will hold up to 25 repetitions?</p>
<h4 id="what-is-the-effect-of-the-number-of-output-tokens">What is the effect of the number of output tokens?</h4>
<p><img alt="01_3_python_prompts" src="../res/2024-05-23-11-26-52.png" /></p>
<p><img alt="02_public_notebook_prompts" src="../res/2024-05-23-11-26-57.png" /></p>
<p>These plots suggests that we might not need 1024 output tokens, let's have a deeper look at the
distribution of output tokens. Maybe an approach that uses more repetitions but shorter answers could
be better.</p>
<p><img alt="output token distribution" src="../res/2024-05-23-11-56-22.png" /></p>
<p>My estimate is that if we reduce the number of output tokens from 1024 to 512 we could increase the repetitions
from 25 to 35 while maintaining the execution time.</p>
<h3 id="variability-in-evaluations-vs-lb-variability">Variability in evaluations vs LB variability</h3>
<table>
<thead>
<tr>
<th>prompts</th>
<th>val scores</th>
<th>LB scores</th>
</tr>
</thead>
<tbody>
<tr>
<td>python_prompts</td>
<td>25, 25, 28, 28</td>
<td>15, 17, 17, 18</td>
</tr>
<tr>
<td>public notebook prompts</td>
<td>23, 23, 24, 25, 26</td>
<td>16, 21, 21, 21</td>
</tr>
</tbody>
</table>
<p>On my validation scores I see a variability of 3 problems, in the leaderboard it increases to 5.
I have the feeling that submissions made at the same time get the same score.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The most important conclusion from this iteration is that I need to do a full evaluation (580 level 5 MATH problems with 25 repetitions each) if I want to evaluate the model correctly. I need to evaluate 580 problems
to lower the uncertainty of the evaluation, and despite of that it is still high in the order of 4% accuracy.
I need to do 25 repetitions on each problem because doing a single repetition is not enough. Maybe I could
do just 4-5 repetitions but I don't know yet.</p>
<p>This has been a very hard iteration because despite all my work I haven't been able to improve over the
baseline. But now I feel I have a solid foundation and a sound evaluation.</p>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>RAG to select the best shots for the prompt</li>
<li>Prompt tuning to help the model use the correct output format</li>
<li>Could I reduce the errors of the model? F.e. validating the answers</li>
<li>Maybe I could use a model that given two possible answers chooses which one seems to be correct.
  On a first step I would gather as much possible answers as possible and on a second step I will
  filter them out.</li>
<li>Analyze errors to find ways of fixing them</li>
<li>Maybe code prompts and non-code prompts are complementary when using self-consistency?</li>
<li>Why I don't see the same variance of LB scores with my submissions, public notebook ranges from 15 to 23?</li>
<li>Idea: a code only approach. The LLM is only allowed to generate one snippet of code.
  This might speedup generation. What would be my score if simulating that condition? This will work
  if the model is not able to recover from coding errors</li>
<li>What if I reduce the number of tokens to 512 or 640 and increase the number of repetitions?</li>
<li>What is the weight of None answers? Can I reduce them?</li>
</ul>
<h2 id="todo">TODO</h2>
<ul class="task-list">
<li class="task-list-item"><input type="checkbox" disabled checked/> How to evaluate the MATH dataset</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> What if I use the MATH dataset to create few-shot prompts for the train dataset?</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> Analyze evaluation results based on difficulty level, how do they correlate with LB score?</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> Use kaggle notebooks for evaluation, I have 30 hours per week.</li>
<li class="task-list-item"><input type="checkbox" disabled/> Evaluate on Veridas cluster?</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> Measure effect of MathInstruct</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> Is the number of shots relevant when using MathInstruct? Currently evaluating</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> What is the effect of temperature?</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> Does quantization affect to speed and accuracy? Currently measuring on Kaggle.</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> How much time is saved by saving the kv values?</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> Prompt that forces to use python code, AIMO train prompts use more code than MathInstruct and the paper says that few-shot prompt is not worth it. The prompt should finish with the start of python code.</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> Implement better parsing for non boxed answers, by looking at how the model answers. Then
  revisit the different styles of prompting.</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> Reanalyze the results with the new parsing</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> Evaluate AIMO train with new configuration on Kaggle notebook. <a href="https://www.kaggle.com/code/ironbar/deepseekmath-with-code-interpreter?scriptVersionId=178743530">DONE</a></li>
<li class="task-list-item"><input type="checkbox" disabled checked/> Compare all the prompts -&gt; How good are forum's prompts on my evaluation?</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> What if I use markdown section in the prompt?. Does not have sense once I have seen that few-shot is not helpful.</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> Can I improve my LB score by using more repetitions with P100 gpu? I can indeed do more repetitions but still haven't improved the LB score yet</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> Study the effect of confidence level and repetitions on runtime and accuracy. The problem is that
  I'm currently using 25 repetitions for the submission. That evaluation could take more than one day
  on my machine. But that data could be useful for later analysis.</li>
<li class="task-list-item"><input type="checkbox" disabled/> Document youtube video about reasoning, there was some interesting paper.</li>
<li class="task-list-item"><input type="checkbox" disabled/> I need actionable insights</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> What if I do a full evaluation with the public notebook prompts?</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> Can I reproduce locally the variability of LB score? By selecting 50 problems and running multiple evaluations.</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> Update Kaggle notebook with gpu memory check</li>
<li class="task-list-item"><input type="checkbox" disabled checked/> Need a notebook to do pairwise comparison of inference</li>
</ul>
<h2 id="future-work-on-google-cloud">Future work on Google Cloud</h2>
<p>T4 -&gt; 0.3<span class="arithmatex">\(/hour
V100 -&gt; 1.92
L4 -&gt; 0.5
A100 40GB -&gt; 2.5\)</span>/hour</p>
<p>Imagine that the evaluation takes 20 hours on A100, that would be 50$ per evaluation.</p>
<p>https://www.xcelerit.com/computing-benchmarks/insights/benchmarks-deep-learning-nvidia-p100-vs-v100-gpu/
https://technical.city/en/video/Tesla-P100-PCIe-16-GB-vs-Tesla-A100</p>

  <hr>
<div class="md-source-file">
  <small>
    
      Last update:
      2024-05-24
    
  </small>
</div>

              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../Iteration_01_overfit/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Iteration 1. Overfit the train set" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Iteration 1. Overfit the train set
            </div>
          </div>
        </a>
      
      
        
        <a href="../Iteration_03_prompt_engineering/" class="md-footer__link md-footer__link--next" aria-label="Next: Iteration 3. Prompt engineering" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Iteration 3. Prompt engineering
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://www.linkedin.com/in/guillermobarbadillo/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://twitter.com/guille_bar" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://www.youtube.com/channel/UCOHmUwHnd2hmUpiDzaQ1Isg" target="_blank" rel="noopener" title="www.youtube.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://www.kaggle.com/ironbar" target="_blank" rel="noopener" title="www.kaggle.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M304.2 501.5 158.4 320.3 298.2 185c2.6-2.7 1.7-10.5-5.3-10.5h-69.2c-3.5 0-7 1.8-10.5 5.3L80.9 313.5V7.5q0-7.5-7.5-7.5H21.5Q14 0 14 7.5v497q0 7.5 7.5 7.5h51.9q7.5 0 7.5-7.5v-109l30.8-29.3 110.5 140.6c3 3.5 6.5 5.3 10.5 5.3h66.9q5.25 0 6-3z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.top"], "search": "../../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.a6c66575.min.js"></script>
      
        <script src="../../javascript/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>