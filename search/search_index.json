{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"aimo Solve national-level math challenges using artificial intelligence models https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize Docs https://ironbar.github.io/aimo Code structure |_ aimo: library with code developed for the challenge |_ forum: all the scripts and notebooks taken from the forum |_ logs: folder for storing all kind of stats and logs. For example the score of each model, results from experiments |_ notebooks: jupyter notebooks made during the challenge. They start by number for easier sorting. |_ reports: documents made during the challenge according to CRISP-DM methodology |_ tests: folder with tests for the library |_ data: folder with light data from the challenge |_ rules: the official rules of the challenge |_ scripts: scripts made during the challenge for training, data processing...","title":"aimo"},{"location":"#aimo","text":"Solve national-level math challenges using artificial intelligence models https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize","title":"aimo"},{"location":"#docs","text":"https://ironbar.github.io/aimo","title":"Docs"},{"location":"#code-structure","text":"|_ aimo: library with code developed for the challenge |_ forum: all the scripts and notebooks taken from the forum |_ logs: folder for storing all kind of stats and logs. For example the score of each model, results from experiments |_ notebooks: jupyter notebooks made during the challenge. They start by number for easier sorting. |_ reports: documents made during the challenge according to CRISP-DM methodology |_ tests: folder with tests for the library |_ data: folder with light data from the challenge |_ rules: the official rules of the challenge |_ scripts: scripts made during the challenge for training, data processing...","title":"Code structure"},{"location":"01_Business_Understanding/","text":"Business Understanding Challenge description The goal of this competition is to create algorithms and models that can solve tricky math problems written in LaTeX format. Your participation will help to advance AI models\u2019 mathematical reasoning skills and drive frontier knowledge. Super interesting goal for a competition. The ability to reason mathematically is a critical milestone for AI. Mathematical reasoning is the foundation for solving many complex problems, from engineering marvels to intricate financial models. However, current AI capabilities are limited in this area. This competition includes 110 problems similar to an intermediate-level high school math challenge. The Gemma 7B benchmark for these problems is 3/50 on the public and private test sets. So the task is to solve 110 problems similar to an intermediate-level high school math challenge. The assessment of AI models' mathematical reasoning skills faces a significant hurdle, the issue of train-test leakage. Models trained on Internet-scale datasets may inadvertently encounter test questions during training, skewing the evaluation process. To address this challenge, this competition uses a dataset of 110 novel math problems, created by an international team of problem solvers, recognizing the need for a transparent and fair evaluation framework. The dataset encompasses a range of difficulty levels, from simple arithmetic to algebraic thinking and geometric reasoning. This will help to strengthen the benchmarks for assessing AI models' mathematical reasoning skills, without the risk of contamination from training data They have created new problems to address the problem of data contamination. It is a code competition and the submission needs to run in less than 9 hours of time. So we have to create a system that solves mathematical problems with limited hardware constraints. Because of the limited number of problems available, we are taking special precautions to secure the test set against probing. Among other things, during the submission period the test set will comprise only the 50 public set problems. Once the competition ends, when we rerun submissions, the test set will comprise only the 50 private set problems. You should attempt to make sure your submission will complete successfully on the 50 new private set problems. This may mean ensuring your submission is robust to unexpected inputs, or managing runtime and memory usage. The system has to solve 50 problems in 9 hours, so that is around 10 minutes per problem. Alexander Gerko, competition host Evaluation Submissions are evaluated on the accuracy between their predicted labels and the ground-truth labels. In other words, submissions are ranked by the fraction of predicted labels that exactly match the ground-truth labels. In this competition, every ground-truth label is an integer between 0 and 999, inclusive. So the model has to guess the exact answer to each problem. The answer to each problem is a non-negative integer, which you should report modulo 1000. If, for instance, you believe the answer to a problem is 2034, your prediction should be 34. This is very interesting, instead of giving the answer we have to submit modulo 1000. It is likely to keep numbers within a manageable range, in this case, between 0 and 999, inclusive. Assess situation I already know that I can fine-tune Mixtral on my PC. But summer is coming and maybe I will need more computing power so I will have to deal with that. It is possible that I might have to do a project for Veridas and stop working on the challenge, we will see. Restrictions Individual participants and Teams may use automated machine learning tool(s) (\u201cAMLT\u201d) (e.g., Google AutoML, H2O Driverless AI, etc.) to create a Submission, provided that the participant or Team ensures that they have an appropriate license to the AMLT such that they are able to comply with the Competition Rules. Teams may only use AI models and tools that are open source and were released prior to 23 February 2024. For example, programming languages, such as Python and Lean, and LLMs with publicly available weights, such as Llama or Gemma. We cannot use models released after 23 February 2024 as a start point of research. We can generate data for fine-tuning or training models. Generating data with GPT4 is allowed We can generate any dataset that we want, but it will have to be shared after the competition ends. Clarification about the use of ChatGPT to generate training data. LoRA and fine-tuning are allowed LoRA is allowed? Is fine-tuning allowed in this competition? Llama 3 is not allowed It was released after 23 February 2024, so it is not allowed. Is a fine-tuned model based on LLAMA3 eligible? Speed race In the event of a tie, the Submission that was entered first to the Competition will be the winner. In the event a potential winner is disqualified for any reason, the Submission that received the next highest score rank will be chosen as the potential winner. Although the test set is private, this competition is kind of similar to capture the flag competition because the number of problems is small. Thus the first one to achieve a good score wins if there is a tie. Terminology AIME . The American Invitational Mathematics Examination (AIME) is a selective and prestigious 15-question 3-hour test given since 1983 to those who rank in the top 5% on the AMC 12 high school mathematics examination (formerly known as the AHSME), and starting in 2010, those who rank in the top 2.5% on the AMC 10. AMC . The American Mathematics Competitions (AMC) are the first of a series of competitions in secondary school mathematics that determine the United States of America's team for the International Mathematical Olympiad (IMO). AMC 12, for students under the age of 19.5 and in grades 12 and below. Common Crawl Common Crawl is a nonprofit 501 organization that crawls the web and freely provides its archives and datasets to the public. Common Crawl's web archive consists of petabytes of data collected since 2008. It completes crawls generally every month. program-of-though (PoT) . In PoT the computation can be delegated to a program interpreter, thus decoupling complex computation from reasoning and language understanding. chain-of-though (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding. tool-integrated reasoning We propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. speculative decoding It is a method to accelerate inference by combining a fast and a slow model. MATH Dataset a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. The MATH dataset consists of problems from mathematics competitions including the AMC 10, AMC 12, AIME, and more. Unlike most prior work, most problems in MATH cannot be solved with a straightforward application of standard K-12 mathematics tools. Instead, humans often solve such problem by applying problem solving techniques and \u201cheuristics\u201d [K-12 mathematics tool] GSM8k dataset a dataset of middle-school level math word problems. Self-Consistency Self-consistency1 is an approach that simply asks a model the same prompt multiple times and takes the majority result as the final answer. It is a follow up to CoT prompting, and is more powerful when used in conjunction with it. SymPy is a Python library for symbolic mathematics. It can be used for example to solve equations . Few-shot prompting . Create a prompt with a few examples of how to solve the task. OpenAI Codex AI system that translates natural language to code. Generated Knowledge Prompting Use the LLM to generate knowledge before making a prediction. This could be useful to recall some equations that might be relevant to solve the problem. I might have an specialized model for this task. Isabelle Isabelle is a generic proof assistant. It allows mathematical formulas to be expressed in a formal language and provides tools for proving those formulas in a logical calculus. Questions Project Plan On a first step I need to read about the state of the art. I need more information to create a plan for the challenge. Initial ideas Tool use. Calculator. LLMs ar not good at arithmetics. GPT4 with chain of thought to generate training data https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/ https://alphacode.deepmind.com/ In this competition the key is probably the data and how to use the data (loss function) ORCA paper Output python code could be a good way to avoid using calculators. Can ChatGPT solve more problems with that setup? I believe there was some model specialized on latex Chain of though, tree of thought, speculative decoding are some techniques that might help On AlphaCode there is a cheap way to filter bad solutions, and they can make 10 submits. Here I need to create the highest quality possible solution. Maybe I need a judge to score the candidate solutions. Judging might be easier than generating. Self-rewarding LLMs. A good validation set is needed The tokenizer might be relevant to have a good representation of the problem Inference speed will be relevant, a team capable of doing 20 predictions will beat another team doing 10 predictions. What if we search for similar problems in our dataset for few-shot prompting?","title":"Business Understanding"},{"location":"01_Business_Understanding/#business-understanding","text":"","title":"Business Understanding"},{"location":"01_Business_Understanding/#challenge-description","text":"The goal of this competition is to create algorithms and models that can solve tricky math problems written in LaTeX format. Your participation will help to advance AI models\u2019 mathematical reasoning skills and drive frontier knowledge. Super interesting goal for a competition. The ability to reason mathematically is a critical milestone for AI. Mathematical reasoning is the foundation for solving many complex problems, from engineering marvels to intricate financial models. However, current AI capabilities are limited in this area. This competition includes 110 problems similar to an intermediate-level high school math challenge. The Gemma 7B benchmark for these problems is 3/50 on the public and private test sets. So the task is to solve 110 problems similar to an intermediate-level high school math challenge. The assessment of AI models' mathematical reasoning skills faces a significant hurdle, the issue of train-test leakage. Models trained on Internet-scale datasets may inadvertently encounter test questions during training, skewing the evaluation process. To address this challenge, this competition uses a dataset of 110 novel math problems, created by an international team of problem solvers, recognizing the need for a transparent and fair evaluation framework. The dataset encompasses a range of difficulty levels, from simple arithmetic to algebraic thinking and geometric reasoning. This will help to strengthen the benchmarks for assessing AI models' mathematical reasoning skills, without the risk of contamination from training data They have created new problems to address the problem of data contamination. It is a code competition and the submission needs to run in less than 9 hours of time. So we have to create a system that solves mathematical problems with limited hardware constraints. Because of the limited number of problems available, we are taking special precautions to secure the test set against probing. Among other things, during the submission period the test set will comprise only the 50 public set problems. Once the competition ends, when we rerun submissions, the test set will comprise only the 50 private set problems. You should attempt to make sure your submission will complete successfully on the 50 new private set problems. This may mean ensuring your submission is robust to unexpected inputs, or managing runtime and memory usage. The system has to solve 50 problems in 9 hours, so that is around 10 minutes per problem. Alexander Gerko, competition host","title":"Challenge description"},{"location":"01_Business_Understanding/#evaluation","text":"Submissions are evaluated on the accuracy between their predicted labels and the ground-truth labels. In other words, submissions are ranked by the fraction of predicted labels that exactly match the ground-truth labels. In this competition, every ground-truth label is an integer between 0 and 999, inclusive. So the model has to guess the exact answer to each problem. The answer to each problem is a non-negative integer, which you should report modulo 1000. If, for instance, you believe the answer to a problem is 2034, your prediction should be 34. This is very interesting, instead of giving the answer we have to submit modulo 1000. It is likely to keep numbers within a manageable range, in this case, between 0 and 999, inclusive.","title":"Evaluation"},{"location":"01_Business_Understanding/#assess-situation","text":"I already know that I can fine-tune Mixtral on my PC. But summer is coming and maybe I will need more computing power so I will have to deal with that. It is possible that I might have to do a project for Veridas and stop working on the challenge, we will see.","title":"Assess situation"},{"location":"01_Business_Understanding/#restrictions","text":"Individual participants and Teams may use automated machine learning tool(s) (\u201cAMLT\u201d) (e.g., Google AutoML, H2O Driverless AI, etc.) to create a Submission, provided that the participant or Team ensures that they have an appropriate license to the AMLT such that they are able to comply with the Competition Rules. Teams may only use AI models and tools that are open source and were released prior to 23 February 2024. For example, programming languages, such as Python and Lean, and LLMs with publicly available weights, such as Llama or Gemma. We cannot use models released after 23 February 2024 as a start point of research. We can generate data for fine-tuning or training models.","title":"Restrictions"},{"location":"01_Business_Understanding/#generating-data-with-gpt4-is-allowed","text":"We can generate any dataset that we want, but it will have to be shared after the competition ends. Clarification about the use of ChatGPT to generate training data.","title":"Generating data with GPT4 is allowed"},{"location":"01_Business_Understanding/#lora-and-fine-tuning-are-allowed","text":"LoRA is allowed? Is fine-tuning allowed in this competition?","title":"LoRA and fine-tuning are allowed"},{"location":"01_Business_Understanding/#llama-3-is-not-allowed","text":"It was released after 23 February 2024, so it is not allowed. Is a fine-tuned model based on LLAMA3 eligible?","title":"Llama 3 is not allowed"},{"location":"01_Business_Understanding/#speed-race","text":"In the event of a tie, the Submission that was entered first to the Competition will be the winner. In the event a potential winner is disqualified for any reason, the Submission that received the next highest score rank will be chosen as the potential winner. Although the test set is private, this competition is kind of similar to capture the flag competition because the number of problems is small. Thus the first one to achieve a good score wins if there is a tie.","title":"Speed race"},{"location":"01_Business_Understanding/#terminology","text":"AIME . The American Invitational Mathematics Examination (AIME) is a selective and prestigious 15-question 3-hour test given since 1983 to those who rank in the top 5% on the AMC 12 high school mathematics examination (formerly known as the AHSME), and starting in 2010, those who rank in the top 2.5% on the AMC 10. AMC . The American Mathematics Competitions (AMC) are the first of a series of competitions in secondary school mathematics that determine the United States of America's team for the International Mathematical Olympiad (IMO). AMC 12, for students under the age of 19.5 and in grades 12 and below. Common Crawl Common Crawl is a nonprofit 501 organization that crawls the web and freely provides its archives and datasets to the public. Common Crawl's web archive consists of petabytes of data collected since 2008. It completes crawls generally every month. program-of-though (PoT) . In PoT the computation can be delegated to a program interpreter, thus decoupling complex computation from reasoning and language understanding. chain-of-though (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding. tool-integrated reasoning We propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. speculative decoding It is a method to accelerate inference by combining a fast and a slow model. MATH Dataset a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. The MATH dataset consists of problems from mathematics competitions including the AMC 10, AMC 12, AIME, and more. Unlike most prior work, most problems in MATH cannot be solved with a straightforward application of standard K-12 mathematics tools. Instead, humans often solve such problem by applying problem solving techniques and \u201cheuristics\u201d [K-12 mathematics tool] GSM8k dataset a dataset of middle-school level math word problems. Self-Consistency Self-consistency1 is an approach that simply asks a model the same prompt multiple times and takes the majority result as the final answer. It is a follow up to CoT prompting, and is more powerful when used in conjunction with it. SymPy is a Python library for symbolic mathematics. It can be used for example to solve equations . Few-shot prompting . Create a prompt with a few examples of how to solve the task. OpenAI Codex AI system that translates natural language to code. Generated Knowledge Prompting Use the LLM to generate knowledge before making a prediction. This could be useful to recall some equations that might be relevant to solve the problem. I might have an specialized model for this task. Isabelle Isabelle is a generic proof assistant. It allows mathematical formulas to be expressed in a formal language and provides tools for proving those formulas in a logical calculus.","title":"Terminology"},{"location":"01_Business_Understanding/#questions","text":"","title":"Questions"},{"location":"01_Business_Understanding/#project-plan","text":"On a first step I need to read about the state of the art. I need more information to create a plan for the challenge.","title":"Project Plan"},{"location":"01_Business_Understanding/#initial-ideas","text":"Tool use. Calculator. LLMs ar not good at arithmetics. GPT4 with chain of thought to generate training data https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/ https://alphacode.deepmind.com/ In this competition the key is probably the data and how to use the data (loss function) ORCA paper Output python code could be a good way to avoid using calculators. Can ChatGPT solve more problems with that setup? I believe there was some model specialized on latex Chain of though, tree of thought, speculative decoding are some techniques that might help On AlphaCode there is a cheap way to filter bad solutions, and they can make 10 submits. Here I need to create the highest quality possible solution. Maybe I need a judge to score the candidate solutions. Judging might be easier than generating. Self-rewarding LLMs. A good validation set is needed The tokenizer might be relevant to have a good representation of the problem Inference speed will be relevant, a team capable of doing 20 predictions will beat another team doing 10 predictions. What if we search for similar problems in our dataset for few-shot prompting?","title":"Initial ideas"},{"location":"02_Data_Understanding/","text":"Data Understanding Collect initial data Test data There are 100 problems in the test set, 50 in the public and 50 in the private set. The answer to each problem is a non-negative integer, which you should report modulo 1000. To receive the super prize I should aim to achieve a score of at least 94/100, current leaderboard score is 20/50 so it might be possible. Train data This is the train data, 10 hard mathematical problems. We can see that the problem description is short, in the longest case around 100 tokens. id problem answer 229ee8 Let \\(k, l > 0\\) be parameters. The parabola \\(y = kx^2 - 2kx + l\\) intersects the line \\(y = 4\\) at two points \\(A\\) and \\(B\\) . These points are distance 6 apart. What is the sum of the squares of the distances from \\(A\\) and \\(B\\) to the origin? 52 246d26 Each of the three-digits numbers \\(111\\) to \\(999\\) is coloured blue or yellow in such a way that the sum of any two (not necessarily different) yellow numbers is equal to a blue number. What is the maximum possible number of yellow numbers there can be? 250 2fc4ad Let the sparkle operation on positive integer \\(n\\) consist of calculating the sum of the digits of \\(n\\) and taking its factorial, e.g. the sparkle of 13 is \\(4! = 24\\) . A robot starts with a positive integer on a blackboard, then after each second for the rest of eternity, replaces the number on the board with its sparkle. For some special numbers, if they're the first number, then eventually every number that appears will be less than 6. How many such special numbers are there with at most 36 digits? 702 430b63 What is the minimum value of \\(5x^2+5y^2-8xy\\) when \\(x\\) and \\(y\\) range over all real numbers such that \\(\\|x-2y\\| + \\|y-2x\\| = 40\\) ? 800 5277ed There exists a unique increasing geometric sequence of five 2-digit positive integers. What is their sum? 211 739bc9 For how many positive integers \\(m\\) does the equation \\(\\vert \\vert x-1 \\vert -2 \\vert=\\frac{m}{100}\\) have \\(4\\) distinct solutions? 199 82e2a0 Suppose that we roll four 6-sided fair dice with faces numbered 1 to~6. Let \\(a/b\\) be the probability that the highest roll is a 5, where \\(a\\) and \\(b\\) are relatively prime positive integers. Find \\(a + b\\) . 185 8ee6f3 The points \\(\\left(x, y\\right)\\) satisfying \\(((\\vert x + y \\vert - 10)^2 + ( \\vert x - y \\vert - 10)^2)((\\vert x \\vert - 8)^2 + ( \\vert y \\vert - 8)^2) = 0\\) enclose a convex polygon. What is the area of this convex polygon? 320 bedda4 Let \\(ABCD\\) be a unit square. Let \\(P\\) be the point on \\(AB\\) such that \\(\\|AP\\| = 1/{20}\\) and let \\(Q\\) be the point on \\(AD\\) such that \\(\\|AQ\\| = 1/{24}\\) . The lines \\(DP\\) and \\(BQ\\) divide the square into four regions. Find the ratio between the areas of the largest region and the smallest region. 480 d7e9c9 A function \\(f: \\mathbb N \\to \\mathbb N\\) satisfies the following two conditions for all positive integers \\(n\\) : \\(f(f(f(n)))=8n-7\\) and \\(f(2n)=2f(n)+1\\) . Calculate \\(f(100)\\) . 199 External data External data is going to be crucial in this challenge since the training data is tiny. AMC 12 Problems and Solutions https://artofproblemsolving.com/wiki/index.php/AMC_12_Problems_and_Solutions AIME Problems and Solutions https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions Other https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/discussion/488473 https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/discussion/492945 https://www.kaggle.com/datasets/pedromoya/math-problems-solved-dataset-andersonbcdefg-hf/data MATH dataset https://github.com/hendrycks/math https://github.com/kipok/nemo-skills https://huggingface.kxxx.link/datasets/pharaouk/math-orca-arch OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset https://github.com/OpenBMB/OlympiadBench https://www.kaggle.com/datasets/pedromoya/math-and-python-code-datasets-hf-collection Describe data All problems are text-only with mathematical notation in LaTeX. Please see the AIMO Prize - Note on Language and Notation.pdf handout for details on the notational conventions used. Although some problems may involve geometry, diagrams are not used in any problem. Verify data quality Since the data is just 110 problems I assume they are all correct. The train set has been solved manually in the forum thus I assume all the problems are correct.","title":"Data Understanding"},{"location":"02_Data_Understanding/#data-understanding","text":"","title":"Data Understanding"},{"location":"02_Data_Understanding/#collect-initial-data","text":"","title":"Collect initial data"},{"location":"02_Data_Understanding/#test-data","text":"There are 100 problems in the test set, 50 in the public and 50 in the private set. The answer to each problem is a non-negative integer, which you should report modulo 1000. To receive the super prize I should aim to achieve a score of at least 94/100, current leaderboard score is 20/50 so it might be possible.","title":"Test data"},{"location":"02_Data_Understanding/#train-data","text":"This is the train data, 10 hard mathematical problems. We can see that the problem description is short, in the longest case around 100 tokens. id problem answer 229ee8 Let \\(k, l > 0\\) be parameters. The parabola \\(y = kx^2 - 2kx + l\\) intersects the line \\(y = 4\\) at two points \\(A\\) and \\(B\\) . These points are distance 6 apart. What is the sum of the squares of the distances from \\(A\\) and \\(B\\) to the origin? 52 246d26 Each of the three-digits numbers \\(111\\) to \\(999\\) is coloured blue or yellow in such a way that the sum of any two (not necessarily different) yellow numbers is equal to a blue number. What is the maximum possible number of yellow numbers there can be? 250 2fc4ad Let the sparkle operation on positive integer \\(n\\) consist of calculating the sum of the digits of \\(n\\) and taking its factorial, e.g. the sparkle of 13 is \\(4! = 24\\) . A robot starts with a positive integer on a blackboard, then after each second for the rest of eternity, replaces the number on the board with its sparkle. For some special numbers, if they're the first number, then eventually every number that appears will be less than 6. How many such special numbers are there with at most 36 digits? 702 430b63 What is the minimum value of \\(5x^2+5y^2-8xy\\) when \\(x\\) and \\(y\\) range over all real numbers such that \\(\\|x-2y\\| + \\|y-2x\\| = 40\\) ? 800 5277ed There exists a unique increasing geometric sequence of five 2-digit positive integers. What is their sum? 211 739bc9 For how many positive integers \\(m\\) does the equation \\(\\vert \\vert x-1 \\vert -2 \\vert=\\frac{m}{100}\\) have \\(4\\) distinct solutions? 199 82e2a0 Suppose that we roll four 6-sided fair dice with faces numbered 1 to~6. Let \\(a/b\\) be the probability that the highest roll is a 5, where \\(a\\) and \\(b\\) are relatively prime positive integers. Find \\(a + b\\) . 185 8ee6f3 The points \\(\\left(x, y\\right)\\) satisfying \\(((\\vert x + y \\vert - 10)^2 + ( \\vert x - y \\vert - 10)^2)((\\vert x \\vert - 8)^2 + ( \\vert y \\vert - 8)^2) = 0\\) enclose a convex polygon. What is the area of this convex polygon? 320 bedda4 Let \\(ABCD\\) be a unit square. Let \\(P\\) be the point on \\(AB\\) such that \\(\\|AP\\| = 1/{20}\\) and let \\(Q\\) be the point on \\(AD\\) such that \\(\\|AQ\\| = 1/{24}\\) . The lines \\(DP\\) and \\(BQ\\) divide the square into four regions. Find the ratio between the areas of the largest region and the smallest region. 480 d7e9c9 A function \\(f: \\mathbb N \\to \\mathbb N\\) satisfies the following two conditions for all positive integers \\(n\\) : \\(f(f(f(n)))=8n-7\\) and \\(f(2n)=2f(n)+1\\) . Calculate \\(f(100)\\) . 199","title":"Train data"},{"location":"02_Data_Understanding/#external-data","text":"External data is going to be crucial in this challenge since the training data is tiny.","title":"External data"},{"location":"02_Data_Understanding/#amc-12-problems-and-solutions","text":"https://artofproblemsolving.com/wiki/index.php/AMC_12_Problems_and_Solutions","title":"AMC 12 Problems and Solutions"},{"location":"02_Data_Understanding/#aime-problems-and-solutions","text":"https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions","title":"AIME Problems and Solutions"},{"location":"02_Data_Understanding/#other","text":"https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/discussion/488473 https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/discussion/492945 https://www.kaggle.com/datasets/pedromoya/math-problems-solved-dataset-andersonbcdefg-hf/data MATH dataset https://github.com/hendrycks/math https://github.com/kipok/nemo-skills https://huggingface.kxxx.link/datasets/pharaouk/math-orca-arch OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset https://github.com/OpenBMB/OlympiadBench https://www.kaggle.com/datasets/pedromoya/math-and-python-code-datasets-hf-collection","title":"Other"},{"location":"02_Data_Understanding/#describe-data","text":"All problems are text-only with mathematical notation in LaTeX. Please see the AIMO Prize - Note on Language and Notation.pdf handout for details on the notational conventions used. Although some problems may involve geometry, diagrams are not used in any problem.","title":"Describe data"},{"location":"02_Data_Understanding/#verify-data-quality","text":"Since the data is just 110 problems I assume they are all correct. The train set has been solved manually in the forum thus I assume all the problems are correct.","title":"Verify data quality"},{"location":"03_State_of_the_art/","text":"State of the art AlphaCode 2 Finetune many models to generate code given a problem description For each problem it generates up to 1M possible solutions. We don't have that much compute for our task. Filter all the solutions that do not pass the public test or compile. This removes 95% of the solutions. We cannot do this on AIMO because we don't have any input and output data. If we decide to use code as the output of the model we might discard code with problems, but nothing more. Clustering of the solutions. They generate new input data and use the output on that data to clusterize the solutions. This again will be hard to do, or impossible for our problem because not all the problems have this property. Score the candidates. They have another model to estimate correctness of the solutions. This is something we can replicate. They used Gemini Pro as the base model. This resulted on a big improvement over previous version of AlphaCode that I believe it used Palm. It is likely that even better results would have been obtained with Gemini Ultra, but at a higher computational cost. Learnings Generating a lot of solutions and scoring them could be a good strategy Having a good base model is very important, changing from Palm to Gemini brought big improvements. AlphaGeometry AlphaGeometry is a neuro-symbolic system made up of a neural language model and a symbolic deduction engine, which work together to find proofs for complex geometry theorems. Akin to the idea of \u201cthinking, fast and slow\u201d, one system provides fast, \u201cintuitive\u201d ideas, and the other, more deliberate, rational decision-making. AlphaGeometry solving a simple problem: Given the problem diagram and its theorem premises (left), AlphaGeometry (middle) first uses its symbolic engine to deduce new statements about the diagram until the solution is found or new statements are exhausted. If no solution is found, AlphaGeometry\u2019s language model adds one potentially useful construct (blue), opening new paths of deduction for the symbolic engine. This loop continues until a solution is found (right). In this example, just one construct is required. AlphaGeometry\u2019s system combines the predictive power of a neural language model with a rule-bound deduction engine, which work in tandem to find solutions. And by developing a method to generate a vast pool of synthetic training data - 100 million unique examples - we can train AlphaGeometry without any human demonstrations, sidestepping the data bottleneck. Learnings It generates a huge dataset for training of 100M of samples By using a deduction engine they can check if they have arrived at the required solution Is there any deduction engine to solve math problems? Reasoning is done with the deduction engine, AI does the intuition. DeepSeek Math They start with DeepSeek-Coder model: Pre-train on 120B math tokens Instruction tuning with: Chain of thought Program of thought Tool integrated reasoning Reinforcement learning with Group Relative Policy Optimization restricted by the model scale, DeepSeekMath is worse than GPT-4 on few-shot capability. GPT-4 could improve its performance with few-shot inputs, while DeepSeekMath shows similar performance in zero-shot and few-shot evaluation. On my first read of the paper I missed this quote, because it was on the future work section. There isn't any other mention to few-shot prompting in the paper. My initial experiments suggest that this quote is true, I'm getting the same or better results with zero shot than with few shot. Orca: Progressive Learning from Complex Explanation Traces of GPT-4 They fine-tune a Llama model in 5M chain of thought responses from ChatGPT and GPT4. This results on much powerful model than simply fine-tuning on query, response pairs because it has traces of the reasoning. Training required around 3.2k A100-hours. Orca: The Model Few Saw Coming, by AI Explained Llemma: An Open Language Model For Mathematics They take Code Llama and fine-tune on 55B tokens math datasets. Training requires 23k A100 hours. Minerva: Solving Quantitative Reasoning Problems with Language Models Minerva is based on the PaLM models and it trains them further with 38.5B tokens of math content. It uses a 4-shot-example prompt strategy. It says that majority voting considerably outperforms greedy decoding. Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification This paper is the state of the art on MATH dataset achieving an accuracy of 84.3%. It uses GPT4-Code Interpreter which by default gets 69.7%. To improve over the baseline it asks GPT4 to verify the answer. So GPT4 uses the interpreter to generate an answer, and once it has the answer it verifies that the answer is valid according to the problem description. The output of this verification can be used later to improve voting, giving different weights to the solutions depending if the verification was True, Uncertain or False. The analysis reveals that GPT4-Code\u2019s strong performance is not solely due to its code generation and execution abilities, but also its capacity to adjust its problem-solving strategies based on feedback from code execution, a process we term self-debugging. Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks Program-of-thoughts (PoT) prompting delegates computation steps to an external language interpreter. In PoT, LMs can express reasoning steps as Python programs, and the computation can be accomplished by a Python interpreter. Results show that PoT outperforms CoT. Also Few-shot beats zero-shot and self-consistency beats single response. There is a similar work called PAL: Program-aided Language Models Comparison of closed source models gpt-4-turbo-2024-04-09 72.2% 0-shot CoT Claude 3 60.1% 0-shot CoT Gemini 1.5 58.5% 4-shot Minerva prompt Seems that gpt-4-turbo-2024-04-09 is currently the king. I believe those numbers are without using code-interpreter, so it is likely that it is much powerful. F.e. gpt-4-0125-preview has a 64.5% score which is more similar to the reported in the papers. So the new model seems to be much better at math than the previous one, it might score around 90% when using the code interpreter. Large Language Models for Mathematical Reasoning: Progresses and Challenges It is not an in-depth paper, probably the most relevant information is the table below. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models Improving Factuality and Reasoning in Language Models through Multiagent Debate They give the same problem to different agents The answers are concatenated and they ask the agents for an updated response using the information of those answers They do many rounds until consensus is achieved However improvements on Grade School Math problems are not significative over using majority voting. Videos about reasoning Mathematical Reasoning in Language Models by OpenAI This is a very good talk. They train a model to rate if the reasoning steps are correct. They also open-source the training data: prm800k . They say that judging math problem solutions is a hard problem. Can LLMs Reason & Plan? (Talk @Google_DeepMind LLM Reasoning Seminar) . Not very good. Reasoning Using Large Language Models Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision. Reasoning involves making inferences, composing multiple steps of inferences, evaluating arguments, and drawing logical conclusions based on available information. Conclusions The state of the art of open source models is far from the super-prize threshold of solving 94% of the problems. DeepSeekMath scores 60.9% with self-consistency over 64 samples. However closed-source models might be close to the goal. The last version of gpt-4-turbo-2024-04-09 achieves a score of 72.2% 0-shot CoT. It is very likely that adding a code interpreter and self-consistency could get a score close to 90%. A previous version of GPT4 reached 60% without code interpreter, 70% with code interpreter and 84% with 16 samples and weighted voting. MATH dataset might be the most similar dataset to the one in the challenge. It seems that code training improves reasoning abilities, that is why code models are used as a base. Trainings are expensive, requiring in the order of thousand of gpu hours. I cannot train a model myself, but fine-tuning a model for the task using LoRA might be beneficial. PoT > CoT == Using code is better than not using it Few-shot > Zero-shot Self-consistency > Single answer or Greedy decoding Vision The solution is very likely to include DeepSeekMath since apparently is the most powerful model and it is already solving 20/50 problems on the leaderboard. This model has been already fine-tuned on MATH dataset, so the room for improvements using additional fine-tuning might be small. Using code interpreter and few-shot prompting improves the results. Being able to do multiple inferences and combining them is very likely to bring improvements. Optimizing the model or the code to be as fast as possible could be a relevant advantage. GPT4 is the best model so far, learning to imitate GPT4 might improve the base DeepSeekMath model. There might be new models or data that could change everything, f.e. GPT5. TODO Search about the topic of LLMs and maths Search papers in Kaggle https://paperswithcode.com/sota/math-word-problem-solving-on-math to watch Download MATH dataset Is there some tool, library to do PoT or PAL?","title":"State of the art"},{"location":"03_State_of_the_art/#state-of-the-art","text":"","title":"State of the art"},{"location":"03_State_of_the_art/#alphacode-2","text":"Finetune many models to generate code given a problem description For each problem it generates up to 1M possible solutions. We don't have that much compute for our task. Filter all the solutions that do not pass the public test or compile. This removes 95% of the solutions. We cannot do this on AIMO because we don't have any input and output data. If we decide to use code as the output of the model we might discard code with problems, but nothing more. Clustering of the solutions. They generate new input data and use the output on that data to clusterize the solutions. This again will be hard to do, or impossible for our problem because not all the problems have this property. Score the candidates. They have another model to estimate correctness of the solutions. This is something we can replicate. They used Gemini Pro as the base model. This resulted on a big improvement over previous version of AlphaCode that I believe it used Palm. It is likely that even better results would have been obtained with Gemini Ultra, but at a higher computational cost.","title":"AlphaCode 2"},{"location":"03_State_of_the_art/#learnings","text":"Generating a lot of solutions and scoring them could be a good strategy Having a good base model is very important, changing from Palm to Gemini brought big improvements.","title":"Learnings"},{"location":"03_State_of_the_art/#alphageometry","text":"AlphaGeometry is a neuro-symbolic system made up of a neural language model and a symbolic deduction engine, which work together to find proofs for complex geometry theorems. Akin to the idea of \u201cthinking, fast and slow\u201d, one system provides fast, \u201cintuitive\u201d ideas, and the other, more deliberate, rational decision-making. AlphaGeometry solving a simple problem: Given the problem diagram and its theorem premises (left), AlphaGeometry (middle) first uses its symbolic engine to deduce new statements about the diagram until the solution is found or new statements are exhausted. If no solution is found, AlphaGeometry\u2019s language model adds one potentially useful construct (blue), opening new paths of deduction for the symbolic engine. This loop continues until a solution is found (right). In this example, just one construct is required. AlphaGeometry\u2019s system combines the predictive power of a neural language model with a rule-bound deduction engine, which work in tandem to find solutions. And by developing a method to generate a vast pool of synthetic training data - 100 million unique examples - we can train AlphaGeometry without any human demonstrations, sidestepping the data bottleneck.","title":"AlphaGeometry"},{"location":"03_State_of_the_art/#learnings_1","text":"It generates a huge dataset for training of 100M of samples By using a deduction engine they can check if they have arrived at the required solution Is there any deduction engine to solve math problems? Reasoning is done with the deduction engine, AI does the intuition.","title":"Learnings"},{"location":"03_State_of_the_art/#deepseek-math","text":"They start with DeepSeek-Coder model: Pre-train on 120B math tokens Instruction tuning with: Chain of thought Program of thought Tool integrated reasoning Reinforcement learning with Group Relative Policy Optimization restricted by the model scale, DeepSeekMath is worse than GPT-4 on few-shot capability. GPT-4 could improve its performance with few-shot inputs, while DeepSeekMath shows similar performance in zero-shot and few-shot evaluation. On my first read of the paper I missed this quote, because it was on the future work section. There isn't any other mention to few-shot prompting in the paper. My initial experiments suggest that this quote is true, I'm getting the same or better results with zero shot than with few shot.","title":"DeepSeek Math"},{"location":"03_State_of_the_art/#orca-progressive-learning-from-complex-explanation-traces-of-gpt-4","text":"They fine-tune a Llama model in 5M chain of thought responses from ChatGPT and GPT4. This results on much powerful model than simply fine-tuning on query, response pairs because it has traces of the reasoning. Training required around 3.2k A100-hours. Orca: The Model Few Saw Coming, by AI Explained","title":"Orca: Progressive Learning from Complex Explanation Traces of GPT-4"},{"location":"03_State_of_the_art/#llemma-an-open-language-model-for-mathematics","text":"They take Code Llama and fine-tune on 55B tokens math datasets. Training requires 23k A100 hours.","title":"Llemma: An Open Language Model For Mathematics"},{"location":"03_State_of_the_art/#minerva-solving-quantitative-reasoning-problems-with-language-models","text":"Minerva is based on the PaLM models and it trains them further with 38.5B tokens of math content. It uses a 4-shot-example prompt strategy. It says that majority voting considerably outperforms greedy decoding.","title":"Minerva: Solving Quantitative Reasoning Problems with Language Models"},{"location":"03_State_of_the_art/#solving-challenging-math-word-problems-using-gpt-4-code-interpreter-with-code-based-self-verification","text":"This paper is the state of the art on MATH dataset achieving an accuracy of 84.3%. It uses GPT4-Code Interpreter which by default gets 69.7%. To improve over the baseline it asks GPT4 to verify the answer. So GPT4 uses the interpreter to generate an answer, and once it has the answer it verifies that the answer is valid according to the problem description. The output of this verification can be used later to improve voting, giving different weights to the solutions depending if the verification was True, Uncertain or False. The analysis reveals that GPT4-Code\u2019s strong performance is not solely due to its code generation and execution abilities, but also its capacity to adjust its problem-solving strategies based on feedback from code execution, a process we term self-debugging.","title":"Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification"},{"location":"03_State_of_the_art/#program-of-thoughts-prompting-disentangling-computation-from-reasoning-for-numerical-reasoning-tasks","text":"Program-of-thoughts (PoT) prompting delegates computation steps to an external language interpreter. In PoT, LMs can express reasoning steps as Python programs, and the computation can be accomplished by a Python interpreter. Results show that PoT outperforms CoT. Also Few-shot beats zero-shot and self-consistency beats single response. There is a similar work called PAL: Program-aided Language Models","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"location":"03_State_of_the_art/#comparison-of-closed-source-models","text":"gpt-4-turbo-2024-04-09 72.2% 0-shot CoT Claude 3 60.1% 0-shot CoT Gemini 1.5 58.5% 4-shot Minerva prompt Seems that gpt-4-turbo-2024-04-09 is currently the king. I believe those numbers are without using code-interpreter, so it is likely that it is much powerful. F.e. gpt-4-0125-preview has a 64.5% score which is more similar to the reported in the papers. So the new model seems to be much better at math than the previous one, it might score around 90% when using the code interpreter.","title":"Comparison of closed source models"},{"location":"03_State_of_the_art/#large-language-models-for-mathematical-reasoning-progresses-and-challenges","text":"It is not an in-depth paper, probably the most relevant information is the table below.","title":"Large Language Models for Mathematical Reasoning: Progresses and Challenges"},{"location":"03_State_of_the_art/#least-to-most-prompting-enables-complex-reasoning-in-large-language-models","text":"","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"location":"03_State_of_the_art/#improving-factuality-and-reasoning-in-language-models-through-multiagent-debate","text":"They give the same problem to different agents The answers are concatenated and they ask the agents for an updated response using the information of those answers They do many rounds until consensus is achieved However improvements on Grade School Math problems are not significative over using majority voting.","title":"Improving Factuality and Reasoning in Language Models through Multiagent Debate"},{"location":"03_State_of_the_art/#videos-about-reasoning","text":"Mathematical Reasoning in Language Models by OpenAI This is a very good talk. They train a model to rate if the reasoning steps are correct. They also open-source the training data: prm800k . They say that judging math problem solutions is a hard problem. Can LLMs Reason & Plan? (Talk @Google_DeepMind LLM Reasoning Seminar) . Not very good.","title":"Videos about reasoning"},{"location":"03_State_of_the_art/#reasoning-using-large-language-models","text":"Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision. Reasoning involves making inferences, composing multiple steps of inferences, evaluating arguments, and drawing logical conclusions based on available information.","title":"Reasoning Using Large Language Models"},{"location":"03_State_of_the_art/#conclusions","text":"The state of the art of open source models is far from the super-prize threshold of solving 94% of the problems. DeepSeekMath scores 60.9% with self-consistency over 64 samples. However closed-source models might be close to the goal. The last version of gpt-4-turbo-2024-04-09 achieves a score of 72.2% 0-shot CoT. It is very likely that adding a code interpreter and self-consistency could get a score close to 90%. A previous version of GPT4 reached 60% without code interpreter, 70% with code interpreter and 84% with 16 samples and weighted voting. MATH dataset might be the most similar dataset to the one in the challenge. It seems that code training improves reasoning abilities, that is why code models are used as a base. Trainings are expensive, requiring in the order of thousand of gpu hours. I cannot train a model myself, but fine-tuning a model for the task using LoRA might be beneficial. PoT > CoT == Using code is better than not using it Few-shot > Zero-shot Self-consistency > Single answer or Greedy decoding","title":"Conclusions"},{"location":"03_State_of_the_art/#vision","text":"The solution is very likely to include DeepSeekMath since apparently is the most powerful model and it is already solving 20/50 problems on the leaderboard. This model has been already fine-tuned on MATH dataset, so the room for improvements using additional fine-tuning might be small. Using code interpreter and few-shot prompting improves the results. Being able to do multiple inferences and combining them is very likely to bring improvements. Optimizing the model or the code to be as fast as possible could be a relevant advantage. GPT4 is the best model so far, learning to imitate GPT4 might improve the base DeepSeekMath model. There might be new models or data that could change everything, f.e. GPT5.","title":"Vision"},{"location":"03_State_of_the_art/#todo","text":"Search about the topic of LLMs and maths Search papers in Kaggle https://paperswithcode.com/sota/math-word-problem-solving-on-math to watch Download MATH dataset Is there some tool, library to do PoT or PAL?","title":"TODO"},{"location":"05_Solution_Summary/","text":"Solution Summary Solution summary","title":"Solution Summary"},{"location":"05_Solution_Summary/#solution-summary","text":"","title":"Solution Summary"},{"location":"05_Solution_Summary/#solution-summary_1","text":"","title":"Solution summary"},{"location":"06_Winning_Model_Documentation/","text":"Winning model documentation Winning Model Documentation Guidelines A. MODEL SUMMARY A1. Background on you/your team Competition Name: Team Name: Private Leaderboard Score: Private Leaderboard Place: Name: Guillermo Barbadillo Location: Pamplona, SPAIN Email: guilllermobarbadillo@gmail.com A2. Background on you/your team What is your academic/professional background? Did you have any prior experience that helped you succeed in this competition? What made you decide to enter this competition? How much time did you spend on the competition? If part of a team, how did you decide to team up? If you competed as part of a team, who did what? A3. Summary A4. Features Selection / Engineering What were the most important features? How did you select features? Did you make any important feature transformations? Did you find any interesting interactions between features? Did you use external data? (if permitted) A5. Training Method(s) What training methods did you use? Did you ensemble the models? If you did ensemble, how did you weight the different models? A6. Interesting findings What was the most important trick you used? What do you think set you apart from others in the competition? Did you find any interesting relationships in the data that don't fit in the sections above? A7. Simple Features and Methods A8. Model Execution Time How long does it take to train your model? How long does it take to generate predictions using your model? How long does it take to train the simplified model (referenced in section A6)? How long does it take to generate predictions from the simplified model? A9. References B. SUBMISSION MODEL B1. All code, data, and your trained model goes in a single archive B2. README.md B3. Configuration files B4. requirements.txt B5. directory_structure.txt B6. SETTINGS.json B7. Serialized copy of the trained model B8. entry_points.md","title":"Winning model documentation"},{"location":"06_Winning_Model_Documentation/#winning-model-documentation","text":"Winning Model Documentation Guidelines","title":"Winning model documentation"},{"location":"06_Winning_Model_Documentation/#a-model-summary","text":"","title":"A. MODEL SUMMARY"},{"location":"06_Winning_Model_Documentation/#a1-background-on-youyour-team","text":"Competition Name: Team Name: Private Leaderboard Score: Private Leaderboard Place: Name: Guillermo Barbadillo Location: Pamplona, SPAIN Email: guilllermobarbadillo@gmail.com","title":"A1. Background on you/your team"},{"location":"06_Winning_Model_Documentation/#a2-background-on-youyour-team","text":"","title":"A2. Background on you/your team"},{"location":"06_Winning_Model_Documentation/#what-is-your-academicprofessional-background","text":"","title":"What is your academic/professional background?"},{"location":"06_Winning_Model_Documentation/#did-you-have-any-prior-experience-that-helped-you-succeed-in-this-competition","text":"","title":"Did you have any prior experience that helped you succeed in this competition?"},{"location":"06_Winning_Model_Documentation/#what-made-you-decide-to-enter-this-competition","text":"","title":"What made you decide to enter this competition?"},{"location":"06_Winning_Model_Documentation/#how-much-time-did-you-spend-on-the-competition","text":"","title":"How much time did you spend on the competition?"},{"location":"06_Winning_Model_Documentation/#if-part-of-a-team-how-did-you-decide-to-team-up","text":"","title":"If part of a team, how did you decide to team up?"},{"location":"06_Winning_Model_Documentation/#if-you-competed-as-part-of-a-team-who-did-what","text":"","title":"If you competed as part of a team, who did what?"},{"location":"06_Winning_Model_Documentation/#a3-summary","text":"","title":"A3. Summary"},{"location":"06_Winning_Model_Documentation/#a4-features-selection-engineering","text":"","title":"A4. Features Selection / Engineering"},{"location":"06_Winning_Model_Documentation/#what-were-the-most-important-features","text":"","title":"What were the most important features?"},{"location":"06_Winning_Model_Documentation/#how-did-you-select-features","text":"","title":"How did you select features?"},{"location":"06_Winning_Model_Documentation/#did-you-make-any-important-feature-transformations","text":"","title":"Did you make any important feature transformations?"},{"location":"06_Winning_Model_Documentation/#did-you-find-any-interesting-interactions-between-features","text":"","title":"Did you find any interesting interactions between features?"},{"location":"06_Winning_Model_Documentation/#did-you-use-external-data-if-permitted","text":"","title":"Did you use external data? (if permitted)"},{"location":"06_Winning_Model_Documentation/#a5-training-methods","text":"","title":"A5. Training Method(s)"},{"location":"06_Winning_Model_Documentation/#what-training-methods-did-you-use","text":"","title":"What training methods did you use?"},{"location":"06_Winning_Model_Documentation/#did-you-ensemble-the-models","text":"","title":"Did you ensemble the models?"},{"location":"06_Winning_Model_Documentation/#if-you-did-ensemble-how-did-you-weight-the-different-models","text":"","title":"If you did ensemble, how did you weight the different models?"},{"location":"06_Winning_Model_Documentation/#a6-interesting-findings","text":"","title":"A6. Interesting findings"},{"location":"06_Winning_Model_Documentation/#what-was-the-most-important-trick-you-used","text":"","title":"What was the most important trick you used?"},{"location":"06_Winning_Model_Documentation/#what-do-you-think-set-you-apart-from-others-in-the-competition","text":"","title":"What do you think set you apart from others in the competition?"},{"location":"06_Winning_Model_Documentation/#did-you-find-any-interesting-relationships-in-the-data-that-dont-fit-in-the-sections-above","text":"","title":"Did you find any interesting relationships in the data that don't fit in the sections above?"},{"location":"06_Winning_Model_Documentation/#a7-simple-features-and-methods","text":"","title":"A7. Simple Features and Methods"},{"location":"06_Winning_Model_Documentation/#a8-model-execution-time","text":"","title":"A8. Model Execution Time"},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-train-your-model","text":"","title":"How long does it take to train your model?"},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-generate-predictions-using-your-model","text":"","title":"How long does it take to generate predictions using your model?"},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-train-the-simplified-model-referenced-in-section-a6","text":"","title":"How long does it take to train the simplified model (referenced in section A6)?"},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-generate-predictions-from-the-simplified-model","text":"","title":"How long does it take to generate predictions from the simplified model?"},{"location":"06_Winning_Model_Documentation/#a9-references","text":"","title":"A9. References"},{"location":"06_Winning_Model_Documentation/#b-submission-model","text":"","title":"B. SUBMISSION MODEL"},{"location":"06_Winning_Model_Documentation/#b1-all-code-data-and-your-trained-model-goes-in-a-single-archive","text":"","title":"B1. All code, data, and your trained model goes in a single archive"},{"location":"06_Winning_Model_Documentation/#b2-readmemd","text":"","title":"B2. README.md"},{"location":"06_Winning_Model_Documentation/#b3-configuration-files","text":"","title":"B3. Configuration files"},{"location":"06_Winning_Model_Documentation/#b4-requirementstxt","text":"","title":"B4. requirements.txt"},{"location":"06_Winning_Model_Documentation/#b5-directory_structuretxt","text":"","title":"B5. directory_structure.txt"},{"location":"06_Winning_Model_Documentation/#b6-settingsjson","text":"","title":"B6. SETTINGS.json"},{"location":"06_Winning_Model_Documentation/#b7-serialized-copy-of-the-trained-model","text":"","title":"B7. Serialized copy of the trained model"},{"location":"06_Winning_Model_Documentation/#b8-entry_pointsmd","text":"","title":"B8. entry_points.md"},{"location":"modeling/","text":"Modeling Select modeling technique https://www.kaggle.com/code/nayjest/gigabenchmark-llm-accuracy-math-problems Available models Mistral and Mixtral Gemma Llama 2 Code llama , built on top of Llama 2. DeepSeek Math WizardMath-70B Generate experimentation design","title":"Modeling"},{"location":"modeling/#modeling","text":"","title":"Modeling"},{"location":"modeling/#select-modeling-technique","text":"https://www.kaggle.com/code/nayjest/gigabenchmark-llm-accuracy-math-problems","title":"Select modeling technique"},{"location":"modeling/#available-models","text":"Mistral and Mixtral Gemma Llama 2 Code llama , built on top of Llama 2. DeepSeek Math WizardMath-70B","title":"Available models"},{"location":"modeling/#generate-experimentation-design","text":"","title":"Generate experimentation design"},{"location":"modeling/Iteration_01_overfit/","text":"Iteration 1. Overfit the train set 25-04-2024 Goal Can I get a perfect score on the train set? Motivation I have spend two days refactoring the Early Sharing Prize Notebook and now I have a clean code notebook that will allow to do agile changes and iterations. The notebook uses DeepSeekMath model with a code interpreter to solve math problems. There are many parameters and parts of the code to tweak. The train set is small, but that would enable faster evaluation than using other datasets. Once I'm able to overfit to the train set I could expand to other datasets. Development Which parameters or changes I can make to improve the results? Prompt. I can modify the prompt or use few-shot prompting. Temperature. I could modify the temperature and top_p parameters that guide text generation. Dealing with the output. There are many output: boxed output, code output, text output... Can I find a better way to aggregate the information? Does quantization affect to the results? Dealing with code. I have seen problems when parsing the code from the model. Maybe have two conditions: one for starting code generation and another for ending. How many repetitions? Finally being able to increase inference speed would enable for faster evaluation and also to run more repetitions on inference. What is the best way to use DeepSeekMath? DeepSeekMath Paper . Are there clues in the paper of how to prompt the model, inference configuration...? No, the paper does not give too much details of the evaluation. It only mentions that few-shot chain of thought prompting was used. It does not show which prompts or parameters were used. DeepSeekMath Github repo . Explore the repo to find examples of use. OlympiadBench Github repo . Here they use DeepSeekMath. Idea: Most authoritative result is from boxed If I use examples that output boxed that should be the relevant result, because it is an intelligent LLM parsing the result for me. Naive parsing the response from the text does not have sense. Especially if the limit of characters is reached. Results P100 vs 2xT4 The following table shows inference speed in tokens/second. GPU \\ Precision 4bit 16bit P100 12.5 18.7 2xT4 11.7 15.2 My experiments show that P100 is slightly faster than 2xT4. The drawback is that OOM errors can arise when using the 16 bit model. It is likely that using the 2xT4 in parallel (loading the model in both gpus and making inferences like a server) will beat this speeds. Links to forum posts: https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/discussion/492578 https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/discussion/493345 Zero shot vs Few shot instruction following I have found that the model tends to ignore the instructions. It is much better at imitating the style of the few-shot prompt. The most clear evidence comes from the instruction that asks to use boxed format for highlighting the answer. prompt temperature responses with boxed zero-shot 1 31% 3-shot 1 74% 3-shot 0.5 79% 3-shot 0.1 80% Despite the model being clearly required to use the format in the response it ignored it on 69% of the responses, which is wild for an assistant. When being given 3 examples it doubles the responses with boxed, but it is still far from 100%. Lowering the temperature also seems to force the model to follow the instructions closely. Thus it seems pretty clear that the correct way to prompt this model is using few-shot prompts. That might enable to get rid of naively parsing the last integer. Single problem optimization: Problem 1 I have downloaded the notebook to my PC and it is running at around 30 token/s. I'm thinking of doing a single problem optimization. Focus on a specific problem and run the solution search multiple times maximizing the probability of getting the correct answers. Each run could take a few minutes so the optimization speed would be nice. The goal is to write python code that solves the problem. Initial attempts with few-shot prompt from DeepSeekMath repo cannot solve the problem If I add the solution as a few-shot prompt it is able to solve the problems, but it forgets to simplify the result many times. I can automate the simplification in the print, and that leads to perfect answers. It returns the correct answer 5 out of 5 runs. If I try the baseline few-shot prompt with automatic simplification the problem is not solved anytime despite trying with different temperatures. If I ask GPT4 to create a similar problem, it reaches perfect solution. This suggests that if I give similar problems in the few-shot prompt I could boost the score. That also works with humans, if we are shown sample problems or sample code solutions that is very helpful for solving the problem. It is worth saying that among all the experiments the model was able to reach the correct answer twice without using code. It wasn't able to do the same with code unless receiving a very similar problem in the few-shot prompt. Cheating to overfit on train set I have solved all the train set problems. Now I'm going to create a few shot prompt with random solutions from the train set. That is cheating, because the solution is on the prompt, but it is worth trying. The outcome is the expected, the system is able to reach the correct answer for all the problems. I'm going to use this few-shot prompt to make submissions. Number of repetitions Self-consistency is a technique that makes multiple predictions for the same problem and returns the most frequent answer. Intuition says that the higher the number of predictions the most likely the returned answer will be correct. repetitions LB score 4 15 8 20 12 19 The results show an improvement when increasing the repetitions from 4 to 8, but not from 8 to 12. It might be randomness or maybe we need more resolution. Max output tokens max output tokens LB score 512 20 1024 19 2048 19 It seems that for the problems that the system is currently solving the number of output tokens does not need to be big. Temperature TODO: table with results Conclusion DeepSeekMath seems to be capable of solving the train problems if given a similar problem as context in a few-shot prompt. I have been able to achieve perfect score by giving the exact same problems as input, but I also have seen that givin similar problems works. It does not follow the instructions too well, is better to give examples with few-shot prompt. Next steps I might try to replicate or even improve the results of DeepSeekMath on MATH dataset. Bigger context window will increase the chance of having a similar problem in the prompt A good embedding model will also increase the likelihood of solving the problem, because more relevant problems will be given as context. Could I verify the answers to the problems? Instead of relying on self-consistency. Optimizing the inference speed will be very helpful. TODO Problems when generating loops. https://huggingface.co/docs/transformers/en/main_classes/text_generation. That is frequency_penalty on OpenAI but on huggingface there are other options. Analyze deepseekmath library and define experiments Lower temperature when using few-shot to see if it returns more boxed answers. Shuffle the input examples to induce variability. Submission with 9 few-shot examples Submission with 12 repetitions Document results Modify the temperature in the submissions","title":"Iteration 1. Overfit the train set"},{"location":"modeling/Iteration_01_overfit/#iteration-1-overfit-the-train-set","text":"25-04-2024","title":"Iteration 1. Overfit the train set"},{"location":"modeling/Iteration_01_overfit/#goal","text":"Can I get a perfect score on the train set?","title":"Goal"},{"location":"modeling/Iteration_01_overfit/#motivation","text":"I have spend two days refactoring the Early Sharing Prize Notebook and now I have a clean code notebook that will allow to do agile changes and iterations. The notebook uses DeepSeekMath model with a code interpreter to solve math problems. There are many parameters and parts of the code to tweak. The train set is small, but that would enable faster evaluation than using other datasets. Once I'm able to overfit to the train set I could expand to other datasets.","title":"Motivation"},{"location":"modeling/Iteration_01_overfit/#development","text":"","title":"Development"},{"location":"modeling/Iteration_01_overfit/#which-parameters-or-changes-i-can-make-to-improve-the-results","text":"Prompt. I can modify the prompt or use few-shot prompting. Temperature. I could modify the temperature and top_p parameters that guide text generation. Dealing with the output. There are many output: boxed output, code output, text output... Can I find a better way to aggregate the information? Does quantization affect to the results? Dealing with code. I have seen problems when parsing the code from the model. Maybe have two conditions: one for starting code generation and another for ending. How many repetitions? Finally being able to increase inference speed would enable for faster evaluation and also to run more repetitions on inference.","title":"Which parameters or changes I can make to improve the results?"},{"location":"modeling/Iteration_01_overfit/#what-is-the-best-way-to-use-deepseekmath","text":"DeepSeekMath Paper . Are there clues in the paper of how to prompt the model, inference configuration...? No, the paper does not give too much details of the evaluation. It only mentions that few-shot chain of thought prompting was used. It does not show which prompts or parameters were used. DeepSeekMath Github repo . Explore the repo to find examples of use. OlympiadBench Github repo . Here they use DeepSeekMath.","title":"What is the best way to use DeepSeekMath?"},{"location":"modeling/Iteration_01_overfit/#idea-most-authoritative-result-is-from-boxed","text":"If I use examples that output boxed that should be the relevant result, because it is an intelligent LLM parsing the result for me. Naive parsing the response from the text does not have sense. Especially if the limit of characters is reached.","title":"Idea: Most authoritative result is from boxed"},{"location":"modeling/Iteration_01_overfit/#results","text":"","title":"Results"},{"location":"modeling/Iteration_01_overfit/#p100-vs-2xt4","text":"The following table shows inference speed in tokens/second. GPU \\ Precision 4bit 16bit P100 12.5 18.7 2xT4 11.7 15.2 My experiments show that P100 is slightly faster than 2xT4. The drawback is that OOM errors can arise when using the 16 bit model. It is likely that using the 2xT4 in parallel (loading the model in both gpus and making inferences like a server) will beat this speeds. Links to forum posts: https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/discussion/492578 https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/discussion/493345","title":"P100 vs 2xT4"},{"location":"modeling/Iteration_01_overfit/#zero-shot-vs-few-shot-instruction-following","text":"I have found that the model tends to ignore the instructions. It is much better at imitating the style of the few-shot prompt. The most clear evidence comes from the instruction that asks to use boxed format for highlighting the answer. prompt temperature responses with boxed zero-shot 1 31% 3-shot 1 74% 3-shot 0.5 79% 3-shot 0.1 80% Despite the model being clearly required to use the format in the response it ignored it on 69% of the responses, which is wild for an assistant. When being given 3 examples it doubles the responses with boxed, but it is still far from 100%. Lowering the temperature also seems to force the model to follow the instructions closely. Thus it seems pretty clear that the correct way to prompt this model is using few-shot prompts. That might enable to get rid of naively parsing the last integer.","title":"Zero shot vs Few shot instruction following"},{"location":"modeling/Iteration_01_overfit/#single-problem-optimization-problem-1","text":"I have downloaded the notebook to my PC and it is running at around 30 token/s. I'm thinking of doing a single problem optimization. Focus on a specific problem and run the solution search multiple times maximizing the probability of getting the correct answers. Each run could take a few minutes so the optimization speed would be nice. The goal is to write python code that solves the problem. Initial attempts with few-shot prompt from DeepSeekMath repo cannot solve the problem If I add the solution as a few-shot prompt it is able to solve the problems, but it forgets to simplify the result many times. I can automate the simplification in the print, and that leads to perfect answers. It returns the correct answer 5 out of 5 runs. If I try the baseline few-shot prompt with automatic simplification the problem is not solved anytime despite trying with different temperatures. If I ask GPT4 to create a similar problem, it reaches perfect solution. This suggests that if I give similar problems in the few-shot prompt I could boost the score. That also works with humans, if we are shown sample problems or sample code solutions that is very helpful for solving the problem. It is worth saying that among all the experiments the model was able to reach the correct answer twice without using code. It wasn't able to do the same with code unless receiving a very similar problem in the few-shot prompt.","title":"Single problem optimization: Problem 1"},{"location":"modeling/Iteration_01_overfit/#cheating-to-overfit-on-train-set","text":"I have solved all the train set problems. Now I'm going to create a few shot prompt with random solutions from the train set. That is cheating, because the solution is on the prompt, but it is worth trying. The outcome is the expected, the system is able to reach the correct answer for all the problems. I'm going to use this few-shot prompt to make submissions.","title":"Cheating to overfit on train set"},{"location":"modeling/Iteration_01_overfit/#number-of-repetitions","text":"Self-consistency is a technique that makes multiple predictions for the same problem and returns the most frequent answer. Intuition says that the higher the number of predictions the most likely the returned answer will be correct. repetitions LB score 4 15 8 20 12 19 The results show an improvement when increasing the repetitions from 4 to 8, but not from 8 to 12. It might be randomness or maybe we need more resolution.","title":"Number of repetitions"},{"location":"modeling/Iteration_01_overfit/#max-output-tokens","text":"max output tokens LB score 512 20 1024 19 2048 19 It seems that for the problems that the system is currently solving the number of output tokens does not need to be big.","title":"Max output tokens"},{"location":"modeling/Iteration_01_overfit/#temperature","text":"TODO: table with results","title":"Temperature"},{"location":"modeling/Iteration_01_overfit/#conclusion","text":"DeepSeekMath seems to be capable of solving the train problems if given a similar problem as context in a few-shot prompt. I have been able to achieve perfect score by giving the exact same problems as input, but I also have seen that givin similar problems works. It does not follow the instructions too well, is better to give examples with few-shot prompt.","title":"Conclusion"},{"location":"modeling/Iteration_01_overfit/#next-steps","text":"I might try to replicate or even improve the results of DeepSeekMath on MATH dataset. Bigger context window will increase the chance of having a similar problem in the prompt A good embedding model will also increase the likelihood of solving the problem, because more relevant problems will be given as context. Could I verify the answers to the problems? Instead of relying on self-consistency. Optimizing the inference speed will be very helpful.","title":"Next steps"},{"location":"modeling/Iteration_01_overfit/#todo","text":"Problems when generating loops. https://huggingface.co/docs/transformers/en/main_classes/text_generation. That is frequency_penalty on OpenAI but on huggingface there are other options. Analyze deepseekmath library and define experiments Lower temperature when using few-shot to see if it returns more boxed answers. Shuffle the input examples to induce variability. Submission with 9 few-shot examples Submission with 12 repetitions Document results Modify the temperature in the submissions","title":"TODO"},{"location":"modeling/Iteration_02_MATH_dataset/","text":"Iteration 2. MATH dataset 06/05/2024 Goal Can we improve LB score by focusing on MATH dataset? Motivation There is a conversation on the forum that suggests that the correlation between train score and leaderboard score is very weak. The host said: 10 public problems were intentionally chosen to be at or above average difficulty. 50 test problems have a range of difficulties from relatively simple problems to those approaching national Olympiad level This implies that claiming the overall prize winner is going to be very hard, because progress on the leaderboard is not expected to be linear because the difficulty of the problems is increasingly harder. Futhermore the train set is very small, just 10 problems. Using a bigger dataset for validation will allow to measure small improvements. Development About the MATH dataset MATH is a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. The problems belong to these categories: algebra, counting and probability, geometry, intermediate algebra, number theory, prealgebra and precalculus. They also have levels of difficulty from 1 to 5. The problems are written in latex and they have also a text response in latex. The proposed division has 7500 train problems and 5000 test problems. If I were to evaluate all the test problems once using Kaggle's hardware it would take 90 hours (current submission takes 9 hours to solve 50 problems with 10 repetitions each). So this is a problem. Evaluation is slow. But it is a problem that all the teams are going to have. And I could use my resources from consultant job to pay for computation. The prize is 130k$, it might be worth spending a few thousand dollars in evaluation to get the prize. But I have to first find something that works and is scalable. Links: Original repo of the MATH dataset Kaggle dataset with MATH and GSM8K datasets Token len distribution As expected the more difficult problems have longer answers and descriptions. Very beautiful graph. Removing problems with non positive integer answers I'm left with 4345 train problems and 2828 test problems. The distribution of problems is more or less balanced except for level 1 that has half the problems than the other categories. Evaluating the MATH dataset If making a single prediction per problem evaluating 566 test problems in 85 minutes. That is around 6 problems per minute. GPU usage is high, so I don't think there is much room for improvement. I can run two evaluations in parallel with good GPU usage. Uncertainty in the results is big, f.e. for an accuracy of 54% the uncertainty is around 4%. Using the whole test set will reduce the uncertainty below 2%, but evaluation would take 7 hours on my PC. However if I only evaluate the problems of difficulty level 5 then evaluating 580 problems takes around 131 minutes. About inference loop repetitions Sometimes I have seen repetitions on inference, f.e. the following text was repeated multiple times: We can then try each factor to see if it gives us a valid solution for $x$ and $y$.\\n\\nIf we try $10x + 15 = 1$, we get $10x = -14$, which is not possible since $x$ must be a positive integer.\\n\\nIf we try $10x + 15 = 2$, we get $10x = -13$, which is not possible.\\n\\nIf we try $10x + 15 = 7$, we get $10x = -8$, which is not possible.\\n\\nIf we try $10x + 15 = 14$, we get $10x = -1$, which is not possible.\\n\\nIf we try $10x + 15 = 49$, we get $10x = 34$, so $x = 3.4$, which is not an integer.\\n\\nIf we try $10x + 15 = 98$, we get $10x = 83$, so $x = 8.3$, which is not an integer.\\n\\nSo, we must have made a mistake somewhere. Let's go back and check our work.\\n\\nWe factored the left side of the equation as $(10x + 15)(y - 4) = 98$.\\n\\n I have seen that Huggingface has tools such as repetition_penalty or no_repeat_ngram_size . However using those options can be dangerous. In the response some tokens will be repeated very often, using a penalty on them will hurt the accuracy of the model. Documentation is not clear and I believe it is better to play with the temperature of the model than touching those parameters. Optimal stop criteria for self-consistency On the public notebook that won the early prize the stopping criteria was count > sqrt(n_tries . When I saw it I didn't liked and replaced it with a simpler majority check, f.e. if I was running 8 repetitions I required the most voted answer to be 4 to stop sampling more answers. But I felt that it was not a good allocation of resources. I have implemented a better stop criteria based on statistics. My intuition says that I should stop sampling more answers when the frequency of the most voted answer is statistically higher than the second most voted answer. The beauty of this definition is that I can choose how much statistical confidence I want to use: 80%, 90%, 95%... I found this paper about the topic, but didn't have the time to read it because I believe my approach is the correct one. Speedup inference with P100 instead of 2xT4 Although using 2xT4 in parallel is likely to be faster than a a single P100, I could try using the P100 now because I know it is faster. I have verified that when using 1024 output tokens on P100 I can use 1 or 2 shots, but using 3 or more results on Out of Memory errors. The inference speed is twice as 2xT4 as the information below shows. P100 [19:05<00:00, 21.62s/it] Correct: 18/53 (0.34 \u00b1 0.13) Unanswered: 0/53 (0.00 \u00b1 0.00) Wrong: 35/53 (0.66 \u00b1 0.13) T4x2 34/53 53/53 [42:16<00:00, 47.87s/it] Correct: 21/53 (0.40 \u00b1 0.13) Unanswered: 0/53 (0.00 \u00b1 0.00) Wrong: 32/53 (0.60 \u00b1 0.13) It is a little bit weird because apparently the token speed generation was not so different ( 18.7 vs 15.2 ) OOM issues with P100 2 shots 63/580 out of memory 10% https://www.kaggle.com/code/ironbar/deepseekmath-with-code-interpreter?scriptVersionId=177616865 1 shot 0/580 https://www.kaggle.com/code/ironbar/deepseekmath-with-code-interpreter?scriptVersionId=177756625 Despite not having any OOM issues, the metrics are almost the same. That could point to the fact that OOM errors might be happening similarly to achieving the max output tokens, thus not affecting the accuracy of the model. Using the right evaluation If I just want to evaluate the goodness of the base model is better to not aggregate the predictions. For example if I make a single prediction on the 580 MATH level 5 problems I will get an uncertainty of around 4% on the accuracy metrics. If I make 4 predictions on each problem that uncertainty will drop to 2%. In the other hand to evaluate the whole system I need to aggregate the predictions on each problem. However this evaluation is much slower, because f.e. making 10 prediction on 50 problems will take a similar amount of time to the proposed evaluation in the previous paragraph, and the uncertainty will be huge: 13% I need to use the whole dataset for evaluation, otherwise the uncertainty is too big. And that could take around 16 hours. A model that has greater accuracy and less errors on individual problems evaluation will also have higher accuracy when the inferences are aggregated per problem. Results First evaluations on MATH dataset I have evaluated 1/5 of the test dataset in around 85 minutes. The accuracy of the model decreases with the level of difficulty. Maybe I should focus on problems of level 3 and above. The averaged accuracy of those categories is 42%, which equals to 21 problems solved on the public leaderboard. How an ideal validation set should be? As similar as possible to the test set. Correlation should be as high as possible. Improvements on the validation set should translate to improvements on the test set. The validation process should be as similar to the submission as possible. This implies that I should do many repetitions on each problem. Otherwise I won't be optimizing the same thing as in submission. As precise as possible. A small dataset with high uncertainty does not allow to measure small improvements. This implies that I should select the levels with higher correlation to leaderboard, and use a lot of compute to evaluate all the problems with a few repetitions. This challenge is a race in disguise In the event of a tie, the Submission that was entered first to the Competition will be the winner. The private test set is tiny and thus uncertainty on the scores is very high. I have created a notebook to estimate the effect of uncertainty. A team with a score of just 22 will have a 30% chance of equaling or winning a team with a score of 25. Luck will play a big role in the competition. And reaching a good score first will be very valuable. However this is calculated with the assumption that all the problems have the same difficulty, which is likely untrue. Few-shot prompt sources few-shot source MATH5 accuracy LB score MATH 30% 7 AIMO train 51% 20 MathInstruct 49% 21 Using prompts with code clearly beats the text approach. On a following iteration I should try using RAG to find similar problems. Number of shots The table below shows the evaluation on Math level 5 problems with a single inference on each. few-shot runtime (min) correct unanswered wrong boxed_answers mean code interpreter calls 1 131 34% 28% 38% 65% 0.9 2 131 33% 27% 40% 70% 0.9 3 148 33% 28% 39% 69% 0.9 5 164 36% 27% 37% 71% 0.9 The changes are not significative (at least without a pairwise comparison), uncertainty is around 4% The only significative change is that boxed answers are more frequent Effect of saving KV values I have run the exact same evaluation with and without saving KV values. The first run was 131 minutes, and without saving KV values the time grew to 139 minutes. Thus it is a small difference (at least for 1 shot). This opens the door to using an LLM server, however memory usage was almost the same. Effect of quantization I have run the same evaluation with and without quantization. It was an experiment with 2 shots from AIMO train dataset and 4 repetitions for each problem. precision runtime (min) correct unanswered wrong 16 bit 469 39% 23% 38% 4 bit 783 40% 22% 39% It seems that the quantization does not have any effect on precision, considering that the uncertainty in the results is around 2%. However it does affect inference speed, the model is 67% slower when quantized. Effect of temperature I have made experiments with 2 shots from AIMO train dataset and 4 repetitions for each problem. The only variation between experiments was the temperature. temperature runtime (min) correct unanswered wrong 1 506 35% 24% 40% 0.5 508 39% 24% 37% 0.25 483 40% 23% 37% 0.12 492 40% 23% 37% 0.06 484 41% 23% 35% 0 469 39% 23% 38% The uncertainty of this metrics is around 2%, so temperature 1 is worse but the differences between the other experiments might not be significative. Using different prompts prompts correct unanswered wrong original code prompt 31% 29% 40% original code prompt forced python 36% 28% 36% original cot prompt 34% 23% 43% original cot prompt forced python 40% 26% 34% MATHCodeInstruct 2 shots 35% 22% 44% AIMO_train 2 shots 40% 22% 38% custom prompt 38% 25% 38% Forcing python clearly improves the results, thus I should be able to improve results with my prompts. This is achieved by starting the response with the markdown format of python code. No evidence that using few shot prompt gives best results, as the paper already says. AIMO train prompts are better than MATHCodeInstruct, my hypothesis is because they favour to use code because the response starts with python code. On MATHCodeInstruct the response can start with some text. The uncertainty of this metrics is around 4%, and this are individual problem scores (no self-consistency results) \u00bfCould I craft better prompts that achieve more correct results and less wrong predictions? Full evaluation I have prepared a full evaluation of the 580 MATH level 5 problems using the same configuration as the submission: 25 repetitions and a confidence level of 95% for stopping doing inference and returning an answer. I have done two evaluations: Using the prompts from the early prize public notebook. The evaluation has taken 42 hours (21 hours on 2x3090 GPUs) Using those prompts with forced python code and one custom prompt. The evaluation has taken 34 hours (17 hours on 2x3090 GPUs) prompts MATH level 5 accuracy LB score early prize 54% 16, 21 forced python 57% 15, 17, 18, 17 The uncertainty on MATH is around \u00b14%, despite evaluating on 580 problems. On leaderboard there is an uncertainty of \u00b17 problems. Notice that uncertainty is not equal to variability. We could have a deterministic system that always scores the same on the public test set, but the expected score on the private test set is related to the uncertainty. I have run a pairwise comparison and there is no significative difference between the results. This is very interesting, because in the previous section we saw significative differences when using a single repetition for evaluation. Thus we see differences on a single repetition but no difference when using multiple repetitions. This implies that we need to do the costly full evaluation to optimize our system . What is the effect of the number of repetitions? In both cases using more repetitions improves the results as expected. Also there are diminishing returns as expected as well. We can see significative differences up to 3 repetitions, but after that the difference is not significative. So maybe we can do evaluations with 4-5 repetitions and the results will hold up to 25 repetitions? What is the effect of the number of output tokens? These plots suggests that we might not need 1024 output tokens, let's have a deeper look at the distribution of output tokens. Maybe an approach that uses more repetitions but shorter answers could be better. My estimate is that if we reduce the number of output tokens from 1024 to 512 we could increase the repetitions from 25 to 35 while maintaining the execution time. Variability in evaluations vs LB variability prompts val scores LB scores python_prompts 25, 25, 28, 28 15, 17, 17, 18 public notebook prompts 23, 23, 24, 25, 26 16, 21, 21, 21 On my validation scores I see a variability of 3 problems, in the leaderboard it increases to 5. I have the feeling that submissions made at the same time get the same score. Conclusion The most important conclusion from this iteration is that I need to do a full evaluation (580 level 5 MATH problems with 25 repetitions each) if I want to evaluate the model correctly. I need to evaluate 580 problems to lower the uncertainty of the evaluation, and despite of that it is still high in the order of 4% accuracy. I need to do 25 repetitions on each problem because doing a single repetition is not enough. Maybe I could do just 4-5 repetitions but I don't know yet. This has been a very hard iteration because despite all my work I haven't been able to improve over the baseline. But now I feel I have a solid foundation and a sound evaluation. Next steps RAG to select the best shots for the prompt Prompt tuning to help the model use the correct output format Could I reduce the errors of the model? F.e. validating the answers Maybe I could use a model that given two possible answers chooses which one seems to be correct. On a first step I would gather as much possible answers as possible and on a second step I will filter them out. Analyze errors to find ways of fixing them Maybe code prompts and non-code prompts are complementary when using self-consistency? Why I don't see the same variance of LB scores with my submissions, public notebook ranges from 15 to 23? Idea: a code only approach. The LLM is only allowed to generate one snippet of code. This might speedup generation. What would be my score if simulating that condition? This will work if the model is not able to recover from coding errors What if I reduce the number of tokens to 512 or 640 and increase the number of repetitions? What is the weight of None answers? Can I reduce them? TODO How to evaluate the MATH dataset What if I use the MATH dataset to create few-shot prompts for the train dataset? Analyze evaluation results based on difficulty level, how do they correlate with LB score? Use kaggle notebooks for evaluation, I have 30 hours per week. Evaluate on Veridas cluster? Measure effect of MathInstruct Is the number of shots relevant when using MathInstruct? Currently evaluating What is the effect of temperature? Does quantization affect to speed and accuracy? Currently measuring on Kaggle. How much time is saved by saving the kv values? Prompt that forces to use python code, AIMO train prompts use more code than MathInstruct and the paper says that few-shot prompt is not worth it. The prompt should finish with the start of python code. Implement better parsing for non boxed answers, by looking at how the model answers. Then revisit the different styles of prompting. Reanalyze the results with the new parsing Evaluate AIMO train with new configuration on Kaggle notebook. DONE Compare all the prompts -> How good are forum's prompts on my evaluation? What if I use markdown section in the prompt?. Does not have sense once I have seen that few-shot is not helpful. Can I improve my LB score by using more repetitions with P100 gpu? I can indeed do more repetitions but still haven't improved the LB score yet Study the effect of confidence level and repetitions on runtime and accuracy. The problem is that I'm currently using 25 repetitions for the submission. That evaluation could take more than one day on my machine. But that data could be useful for later analysis. Document youtube video about reasoning, there was some interesting paper. I need actionable insights What if I do a full evaluation with the public notebook prompts? Can I reproduce locally the variability of LB score? By selecting 50 problems and running multiple evaluations. Update Kaggle notebook with gpu memory check Need a notebook to do pairwise comparison of inference Future work on Google Cloud T4 -> 0.3 \\(/hour V100 -> 1.92 L4 -> 0.5 A100 40GB -> 2.5\\) /hour Imagine that the evaluation takes 20 hours on A100, that would be 50$ per evaluation. https://www.xcelerit.com/computing-benchmarks/insights/benchmarks-deep-learning-nvidia-p100-vs-v100-gpu/ https://technical.city/en/video/Tesla-P100-PCIe-16-GB-vs-Tesla-A100","title":"Iteration 2. MATH dataset"},{"location":"modeling/Iteration_02_MATH_dataset/#iteration-2-math-dataset","text":"06/05/2024","title":"Iteration 2. MATH dataset"},{"location":"modeling/Iteration_02_MATH_dataset/#goal","text":"Can we improve LB score by focusing on MATH dataset?","title":"Goal"},{"location":"modeling/Iteration_02_MATH_dataset/#motivation","text":"There is a conversation on the forum that suggests that the correlation between train score and leaderboard score is very weak. The host said: 10 public problems were intentionally chosen to be at or above average difficulty. 50 test problems have a range of difficulties from relatively simple problems to those approaching national Olympiad level This implies that claiming the overall prize winner is going to be very hard, because progress on the leaderboard is not expected to be linear because the difficulty of the problems is increasingly harder. Futhermore the train set is very small, just 10 problems. Using a bigger dataset for validation will allow to measure small improvements.","title":"Motivation"},{"location":"modeling/Iteration_02_MATH_dataset/#development","text":"","title":"Development"},{"location":"modeling/Iteration_02_MATH_dataset/#about-the-math-dataset","text":"MATH is a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. The problems belong to these categories: algebra, counting and probability, geometry, intermediate algebra, number theory, prealgebra and precalculus. They also have levels of difficulty from 1 to 5. The problems are written in latex and they have also a text response in latex. The proposed division has 7500 train problems and 5000 test problems. If I were to evaluate all the test problems once using Kaggle's hardware it would take 90 hours (current submission takes 9 hours to solve 50 problems with 10 repetitions each). So this is a problem. Evaluation is slow. But it is a problem that all the teams are going to have. And I could use my resources from consultant job to pay for computation. The prize is 130k$, it might be worth spending a few thousand dollars in evaluation to get the prize. But I have to first find something that works and is scalable. Links: Original repo of the MATH dataset Kaggle dataset with MATH and GSM8K datasets","title":"About the MATH dataset"},{"location":"modeling/Iteration_02_MATH_dataset/#token-len-distribution","text":"As expected the more difficult problems have longer answers and descriptions. Very beautiful graph.","title":"Token len distribution"},{"location":"modeling/Iteration_02_MATH_dataset/#removing-problems-with-non-positive-integer-answers","text":"I'm left with 4345 train problems and 2828 test problems. The distribution of problems is more or less balanced except for level 1 that has half the problems than the other categories.","title":"Removing problems with non positive integer answers"},{"location":"modeling/Iteration_02_MATH_dataset/#evaluating-the-math-dataset","text":"If making a single prediction per problem evaluating 566 test problems in 85 minutes. That is around 6 problems per minute. GPU usage is high, so I don't think there is much room for improvement. I can run two evaluations in parallel with good GPU usage. Uncertainty in the results is big, f.e. for an accuracy of 54% the uncertainty is around 4%. Using the whole test set will reduce the uncertainty below 2%, but evaluation would take 7 hours on my PC. However if I only evaluate the problems of difficulty level 5 then evaluating 580 problems takes around 131 minutes.","title":"Evaluating the MATH dataset"},{"location":"modeling/Iteration_02_MATH_dataset/#about-inference-loop-repetitions","text":"Sometimes I have seen repetitions on inference, f.e. the following text was repeated multiple times: We can then try each factor to see if it gives us a valid solution for $x$ and $y$.\\n\\nIf we try $10x + 15 = 1$, we get $10x = -14$, which is not possible since $x$ must be a positive integer.\\n\\nIf we try $10x + 15 = 2$, we get $10x = -13$, which is not possible.\\n\\nIf we try $10x + 15 = 7$, we get $10x = -8$, which is not possible.\\n\\nIf we try $10x + 15 = 14$, we get $10x = -1$, which is not possible.\\n\\nIf we try $10x + 15 = 49$, we get $10x = 34$, so $x = 3.4$, which is not an integer.\\n\\nIf we try $10x + 15 = 98$, we get $10x = 83$, so $x = 8.3$, which is not an integer.\\n\\nSo, we must have made a mistake somewhere. Let's go back and check our work.\\n\\nWe factored the left side of the equation as $(10x + 15)(y - 4) = 98$.\\n\\n I have seen that Huggingface has tools such as repetition_penalty or no_repeat_ngram_size . However using those options can be dangerous. In the response some tokens will be repeated very often, using a penalty on them will hurt the accuracy of the model. Documentation is not clear and I believe it is better to play with the temperature of the model than touching those parameters.","title":"About inference loop repetitions"},{"location":"modeling/Iteration_02_MATH_dataset/#optimal-stop-criteria-for-self-consistency","text":"On the public notebook that won the early prize the stopping criteria was count > sqrt(n_tries . When I saw it I didn't liked and replaced it with a simpler majority check, f.e. if I was running 8 repetitions I required the most voted answer to be 4 to stop sampling more answers. But I felt that it was not a good allocation of resources. I have implemented a better stop criteria based on statistics. My intuition says that I should stop sampling more answers when the frequency of the most voted answer is statistically higher than the second most voted answer. The beauty of this definition is that I can choose how much statistical confidence I want to use: 80%, 90%, 95%... I found this paper about the topic, but didn't have the time to read it because I believe my approach is the correct one.","title":"Optimal stop criteria for self-consistency"},{"location":"modeling/Iteration_02_MATH_dataset/#speedup-inference-with-p100-instead-of-2xt4","text":"Although using 2xT4 in parallel is likely to be faster than a a single P100, I could try using the P100 now because I know it is faster. I have verified that when using 1024 output tokens on P100 I can use 1 or 2 shots, but using 3 or more results on Out of Memory errors. The inference speed is twice as 2xT4 as the information below shows. P100 [19:05<00:00, 21.62s/it] Correct: 18/53 (0.34 \u00b1 0.13) Unanswered: 0/53 (0.00 \u00b1 0.00) Wrong: 35/53 (0.66 \u00b1 0.13) T4x2 34/53 53/53 [42:16<00:00, 47.87s/it] Correct: 21/53 (0.40 \u00b1 0.13) Unanswered: 0/53 (0.00 \u00b1 0.00) Wrong: 32/53 (0.60 \u00b1 0.13) It is a little bit weird because apparently the token speed generation was not so different ( 18.7 vs 15.2 )","title":"Speedup inference with P100 instead of 2xT4"},{"location":"modeling/Iteration_02_MATH_dataset/#oom-issues-with-p100","text":"2 shots 63/580 out of memory 10% https://www.kaggle.com/code/ironbar/deepseekmath-with-code-interpreter?scriptVersionId=177616865 1 shot 0/580 https://www.kaggle.com/code/ironbar/deepseekmath-with-code-interpreter?scriptVersionId=177756625 Despite not having any OOM issues, the metrics are almost the same. That could point to the fact that OOM errors might be happening similarly to achieving the max output tokens, thus not affecting the accuracy of the model.","title":"OOM issues with P100"},{"location":"modeling/Iteration_02_MATH_dataset/#using-the-right-evaluation","text":"If I just want to evaluate the goodness of the base model is better to not aggregate the predictions. For example if I make a single prediction on the 580 MATH level 5 problems I will get an uncertainty of around 4% on the accuracy metrics. If I make 4 predictions on each problem that uncertainty will drop to 2%. In the other hand to evaluate the whole system I need to aggregate the predictions on each problem. However this evaluation is much slower, because f.e. making 10 prediction on 50 problems will take a similar amount of time to the proposed evaluation in the previous paragraph, and the uncertainty will be huge: 13% I need to use the whole dataset for evaluation, otherwise the uncertainty is too big. And that could take around 16 hours. A model that has greater accuracy and less errors on individual problems evaluation will also have higher accuracy when the inferences are aggregated per problem.","title":"Using the right evaluation"},{"location":"modeling/Iteration_02_MATH_dataset/#results","text":"","title":"Results"},{"location":"modeling/Iteration_02_MATH_dataset/#first-evaluations-on-math-dataset","text":"I have evaluated 1/5 of the test dataset in around 85 minutes. The accuracy of the model decreases with the level of difficulty. Maybe I should focus on problems of level 3 and above. The averaged accuracy of those categories is 42%, which equals to 21 problems solved on the public leaderboard. How an ideal validation set should be? As similar as possible to the test set. Correlation should be as high as possible. Improvements on the validation set should translate to improvements on the test set. The validation process should be as similar to the submission as possible. This implies that I should do many repetitions on each problem. Otherwise I won't be optimizing the same thing as in submission. As precise as possible. A small dataset with high uncertainty does not allow to measure small improvements. This implies that I should select the levels with higher correlation to leaderboard, and use a lot of compute to evaluate all the problems with a few repetitions.","title":"First evaluations on MATH dataset"},{"location":"modeling/Iteration_02_MATH_dataset/#this-challenge-is-a-race-in-disguise","text":"In the event of a tie, the Submission that was entered first to the Competition will be the winner. The private test set is tiny and thus uncertainty on the scores is very high. I have created a notebook to estimate the effect of uncertainty. A team with a score of just 22 will have a 30% chance of equaling or winning a team with a score of 25. Luck will play a big role in the competition. And reaching a good score first will be very valuable. However this is calculated with the assumption that all the problems have the same difficulty, which is likely untrue.","title":"This challenge is a race in disguise"},{"location":"modeling/Iteration_02_MATH_dataset/#few-shot-prompt-sources","text":"few-shot source MATH5 accuracy LB score MATH 30% 7 AIMO train 51% 20 MathInstruct 49% 21 Using prompts with code clearly beats the text approach. On a following iteration I should try using RAG to find similar problems.","title":"Few-shot prompt sources"},{"location":"modeling/Iteration_02_MATH_dataset/#number-of-shots","text":"The table below shows the evaluation on Math level 5 problems with a single inference on each. few-shot runtime (min) correct unanswered wrong boxed_answers mean code interpreter calls 1 131 34% 28% 38% 65% 0.9 2 131 33% 27% 40% 70% 0.9 3 148 33% 28% 39% 69% 0.9 5 164 36% 27% 37% 71% 0.9 The changes are not significative (at least without a pairwise comparison), uncertainty is around 4% The only significative change is that boxed answers are more frequent","title":"Number of shots"},{"location":"modeling/Iteration_02_MATH_dataset/#effect-of-saving-kv-values","text":"I have run the exact same evaluation with and without saving KV values. The first run was 131 minutes, and without saving KV values the time grew to 139 minutes. Thus it is a small difference (at least for 1 shot). This opens the door to using an LLM server, however memory usage was almost the same.","title":"Effect of saving KV values"},{"location":"modeling/Iteration_02_MATH_dataset/#effect-of-quantization","text":"I have run the same evaluation with and without quantization. It was an experiment with 2 shots from AIMO train dataset and 4 repetitions for each problem. precision runtime (min) correct unanswered wrong 16 bit 469 39% 23% 38% 4 bit 783 40% 22% 39% It seems that the quantization does not have any effect on precision, considering that the uncertainty in the results is around 2%. However it does affect inference speed, the model is 67% slower when quantized.","title":"Effect of quantization"},{"location":"modeling/Iteration_02_MATH_dataset/#effect-of-temperature","text":"I have made experiments with 2 shots from AIMO train dataset and 4 repetitions for each problem. The only variation between experiments was the temperature. temperature runtime (min) correct unanswered wrong 1 506 35% 24% 40% 0.5 508 39% 24% 37% 0.25 483 40% 23% 37% 0.12 492 40% 23% 37% 0.06 484 41% 23% 35% 0 469 39% 23% 38% The uncertainty of this metrics is around 2%, so temperature 1 is worse but the differences between the other experiments might not be significative.","title":"Effect of temperature"},{"location":"modeling/Iteration_02_MATH_dataset/#using-different-prompts","text":"prompts correct unanswered wrong original code prompt 31% 29% 40% original code prompt forced python 36% 28% 36% original cot prompt 34% 23% 43% original cot prompt forced python 40% 26% 34% MATHCodeInstruct 2 shots 35% 22% 44% AIMO_train 2 shots 40% 22% 38% custom prompt 38% 25% 38% Forcing python clearly improves the results, thus I should be able to improve results with my prompts. This is achieved by starting the response with the markdown format of python code. No evidence that using few shot prompt gives best results, as the paper already says. AIMO train prompts are better than MATHCodeInstruct, my hypothesis is because they favour to use code because the response starts with python code. On MATHCodeInstruct the response can start with some text. The uncertainty of this metrics is around 4%, and this are individual problem scores (no self-consistency results) \u00bfCould I craft better prompts that achieve more correct results and less wrong predictions?","title":"Using different prompts"},{"location":"modeling/Iteration_02_MATH_dataset/#full-evaluation","text":"I have prepared a full evaluation of the 580 MATH level 5 problems using the same configuration as the submission: 25 repetitions and a confidence level of 95% for stopping doing inference and returning an answer. I have done two evaluations: Using the prompts from the early prize public notebook. The evaluation has taken 42 hours (21 hours on 2x3090 GPUs) Using those prompts with forced python code and one custom prompt. The evaluation has taken 34 hours (17 hours on 2x3090 GPUs) prompts MATH level 5 accuracy LB score early prize 54% 16, 21 forced python 57% 15, 17, 18, 17 The uncertainty on MATH is around \u00b14%, despite evaluating on 580 problems. On leaderboard there is an uncertainty of \u00b17 problems. Notice that uncertainty is not equal to variability. We could have a deterministic system that always scores the same on the public test set, but the expected score on the private test set is related to the uncertainty. I have run a pairwise comparison and there is no significative difference between the results. This is very interesting, because in the previous section we saw significative differences when using a single repetition for evaluation. Thus we see differences on a single repetition but no difference when using multiple repetitions. This implies that we need to do the costly full evaluation to optimize our system .","title":"Full evaluation"},{"location":"modeling/Iteration_02_MATH_dataset/#what-is-the-effect-of-the-number-of-repetitions","text":"In both cases using more repetitions improves the results as expected. Also there are diminishing returns as expected as well. We can see significative differences up to 3 repetitions, but after that the difference is not significative. So maybe we can do evaluations with 4-5 repetitions and the results will hold up to 25 repetitions?","title":"What is the effect of the number of repetitions?"},{"location":"modeling/Iteration_02_MATH_dataset/#what-is-the-effect-of-the-number-of-output-tokens","text":"These plots suggests that we might not need 1024 output tokens, let's have a deeper look at the distribution of output tokens. Maybe an approach that uses more repetitions but shorter answers could be better. My estimate is that if we reduce the number of output tokens from 1024 to 512 we could increase the repetitions from 25 to 35 while maintaining the execution time.","title":"What is the effect of the number of output tokens?"},{"location":"modeling/Iteration_02_MATH_dataset/#variability-in-evaluations-vs-lb-variability","text":"prompts val scores LB scores python_prompts 25, 25, 28, 28 15, 17, 17, 18 public notebook prompts 23, 23, 24, 25, 26 16, 21, 21, 21 On my validation scores I see a variability of 3 problems, in the leaderboard it increases to 5. I have the feeling that submissions made at the same time get the same score.","title":"Variability in evaluations vs LB variability"},{"location":"modeling/Iteration_02_MATH_dataset/#conclusion","text":"The most important conclusion from this iteration is that I need to do a full evaluation (580 level 5 MATH problems with 25 repetitions each) if I want to evaluate the model correctly. I need to evaluate 580 problems to lower the uncertainty of the evaluation, and despite of that it is still high in the order of 4% accuracy. I need to do 25 repetitions on each problem because doing a single repetition is not enough. Maybe I could do just 4-5 repetitions but I don't know yet. This has been a very hard iteration because despite all my work I haven't been able to improve over the baseline. But now I feel I have a solid foundation and a sound evaluation.","title":"Conclusion"},{"location":"modeling/Iteration_02_MATH_dataset/#next-steps","text":"RAG to select the best shots for the prompt Prompt tuning to help the model use the correct output format Could I reduce the errors of the model? F.e. validating the answers Maybe I could use a model that given two possible answers chooses which one seems to be correct. On a first step I would gather as much possible answers as possible and on a second step I will filter them out. Analyze errors to find ways of fixing them Maybe code prompts and non-code prompts are complementary when using self-consistency? Why I don't see the same variance of LB scores with my submissions, public notebook ranges from 15 to 23? Idea: a code only approach. The LLM is only allowed to generate one snippet of code. This might speedup generation. What would be my score if simulating that condition? This will work if the model is not able to recover from coding errors What if I reduce the number of tokens to 512 or 640 and increase the number of repetitions? What is the weight of None answers? Can I reduce them?","title":"Next steps"},{"location":"modeling/Iteration_02_MATH_dataset/#todo","text":"How to evaluate the MATH dataset What if I use the MATH dataset to create few-shot prompts for the train dataset? Analyze evaluation results based on difficulty level, how do they correlate with LB score? Use kaggle notebooks for evaluation, I have 30 hours per week. Evaluate on Veridas cluster? Measure effect of MathInstruct Is the number of shots relevant when using MathInstruct? Currently evaluating What is the effect of temperature? Does quantization affect to speed and accuracy? Currently measuring on Kaggle. How much time is saved by saving the kv values? Prompt that forces to use python code, AIMO train prompts use more code than MathInstruct and the paper says that few-shot prompt is not worth it. The prompt should finish with the start of python code. Implement better parsing for non boxed answers, by looking at how the model answers. Then revisit the different styles of prompting. Reanalyze the results with the new parsing Evaluate AIMO train with new configuration on Kaggle notebook. DONE Compare all the prompts -> How good are forum's prompts on my evaluation? What if I use markdown section in the prompt?. Does not have sense once I have seen that few-shot is not helpful. Can I improve my LB score by using more repetitions with P100 gpu? I can indeed do more repetitions but still haven't improved the LB score yet Study the effect of confidence level and repetitions on runtime and accuracy. The problem is that I'm currently using 25 repetitions for the submission. That evaluation could take more than one day on my machine. But that data could be useful for later analysis. Document youtube video about reasoning, there was some interesting paper. I need actionable insights What if I do a full evaluation with the public notebook prompts? Can I reproduce locally the variability of LB score? By selecting 50 problems and running multiple evaluations. Update Kaggle notebook with gpu memory check Need a notebook to do pairwise comparison of inference","title":"TODO"},{"location":"modeling/Iteration_02_MATH_dataset/#future-work-on-google-cloud","text":"T4 -> 0.3 \\(/hour V100 -> 1.92 L4 -> 0.5 A100 40GB -> 2.5\\) /hour Imagine that the evaluation takes 20 hours on A100, that would be 50$ per evaluation. https://www.xcelerit.com/computing-benchmarks/insights/benchmarks-deep-learning-nvidia-p100-vs-v100-gpu/ https://technical.city/en/video/Tesla-P100-PCIe-16-GB-vs-Tesla-A100","title":"Future work on Google Cloud"},{"location":"modeling/Iteration_03_prompt_engineering/","text":"Iteration 3. Prompt engineering 24-05-2024 Goal Can I improve the LB score with prompt engineering? Motivation I'm going to leave the evaluation fixed: I'm going to use MATH level 5 problems (580) and I will be using 5 repetitions for each problem (confidence level 100%). I want to try different prompt strategies: No code prompt Bad prompt Few-shot prompt Few-shot prompt with RAG Carefully crafted prompts If the model is steerable by prompts I will be able to improve the LB score. If not I will have to find another strategy. I need to improve from my current 21 on LB score to 27, that is a +12% in accuracy that I need to get. Development Results A total of 29 experiments have been run and the results can be seen on this Google Sheet . All the experiments used 580 Math level 5 problems, 5 repetitions with the hope that the accuracy when using 5 repetitions will correlate with the accuracy of using 25 or 30 repetitions. experiment correct unanswered wrong public notebook prompts 45% 5% 50% forced python public prompts 48% 9% 44% custom prompt v1 48% 8% 44% custom prompt v2 below 47% 5% 48% custom prompt v3 list 48% 5% 47% custom prompt v4 47% 6% 47% cot no code 29% 3% 68% minimal prompt 28% 4% 68% AIMO train 2 shots 52% 5% 43% AIMO train 4 shots 52% 6% 42% MathInstruct 2 shots 46% 6% 48% custom prompt v5 assistant 50% 9% 41% custom prompt v6 easy 49% 6% 45% custom prompt v7 48% 7% 46% AIMO train 2 shots assistant 52% 4% 45% AIMO train 2 shots assistant, forced python 51% 6% 43% MATHInstruct lv5 2 shots 47% 5% 48% MATHInstruct lv5 2 shots RAG 48% 5% 47% AIMO train 2 shots RAG 48% 6% 46% AIMO train 2 shots assistant, T=0.2 48% 5% 47% AIMO train 2 shots assistant, T=0.7 51% 4% 45% AIMO train 2 shots assistant, T=0.9 50% 5% 45% AIMO train 2 shots assistant, T=20 52% 4% 44% AIMO train 2 shots assistant, T=0.7 top_p=0.5 51% 6% 43% AIMO train 2 shots assistant, T=0.9, top_p=0.5 51% 6% 43% custom prompt v8 program 51% 8% 42% custom prompt v9 28% 4% 68% 2 prompts 52% 7% 41% 5 prompts 50% 6% 45% The best results are obtained when using two prompts with the format proposed on DeepSeekMath repo User: PROBLEM_PLACEHOLDER Please reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer. Assistant: Sure, we can solve the problem by writing a Python program. ```python User: PROBLEM_PLACEHOLDER Please integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}. The answer is a non negative integer. Assistant: Sure, we can solve the problem by writing a Python program. ```python We observer drops in accuracy when code is not forced or used. So using code is crucial for this task Number of shots does not seem relevant Retrieval Augmented Generation (RAG) does not give significative improvements. Remember that in the DeepSeekMath paper they mention that the model is too small to benefit from few-shot prompting. The differences between the prompts that use code are very small, not significative The effect of temperature and top_p is uncertain in this evaluation Full evaluation I have made a full evaluation with 31 repetitions using the two prompts shown in the previous section. Unfortunately there is no significative improvement. It has an accuracy of 57%, just like using the public prompts with python forcing. The new candidate improves the accuracy faster but leads to the same end result. It seems again that we have to do full evaluation, we cannot do just 5 repetitions but we have to do 25. Multi-prompt full evaluation I run a random search to combine multiple prompts from previous experiments. That search found a combination that achieved an accuracy of 61% when mixing different prompts. However the result was optimistic because I was selecting already done evaluations, thus I decided to run a realistic evaluation. confidence runtime (h) accuracy 90% 27.7 59% 95% 29.1 57% Reducing the confidence to 90% results on a minimal speedup, the accuracy is slightly higher but it is not statistically significative. In theory using a higher confidence will lead to more stable and more accurate results. The results from this evaluation do not show significative improvements over previous experiments. However it might have sense to use many different prompts to induce diversity in the responses since we have seen that there are no big differences between good enough prompts. Conclusion After a week and more than 20 experiments before I have not been able to improve LB score with prompt engineering. How could I improve? Using a better base model to generate answers. Fine-tuning DeepSeekMath may allow to do that. However people in the forum have said that they got worse results after fine-tuning. Remember that the model has already being trained with RL. It is uncertain if fine-tuning can improve the reasoning skills of a model. Change the generation process. Maybe giving as input already generated answers in a dialog between LLMs. Or we might give the possible answers to the model to choose between them, like in a test exam. MMLU example Validate or verify the answers. I might discard wrong answers by validating them. Some problems might be easier to validate than others. Answer selection. Instead of relying on votes, I might use a model to select the best answer. Maybe rewriting the problem in a more clear way could help sometimes. I could create a dataset with rewritten problems and test the accuracy on it. Score of 22 when increasing temperature to 0.9, it was luck because submitting the same version of the notebook returned the following score distribution: [21, 22, 18, 17, 15] Next steps Reread the literature and better understand how DeepSeekMath model was trained Measure the effect of temperature with a higher number of repetitions TODO How long does it take the evaluation with 5 repetitions? around 10 hours. Prompts to evaluate CoT no code prompt Minimal prompt ~~Bad prompt~~ Few-shot prompt Few-shot prompt with RAG Temperature and top_p Prompt that uses code from the repo Ask the model to verify the answer. It ignores the request to verify the answer. If we want to do a verification we should do it manually. Document results Full evaluation with the best configuration Analysis merging the results of all the evaluations Can I find a better combination of prompts? I might drop the problems that do not receive any correct answer. This will allow to speedup evaluation. Role of confidence, what if I decrease confidence from 0.95 to 0.9","title":"Iteration 3. Prompt engineering"},{"location":"modeling/Iteration_03_prompt_engineering/#iteration-3-prompt-engineering","text":"24-05-2024","title":"Iteration 3. Prompt engineering"},{"location":"modeling/Iteration_03_prompt_engineering/#goal","text":"Can I improve the LB score with prompt engineering?","title":"Goal"},{"location":"modeling/Iteration_03_prompt_engineering/#motivation","text":"I'm going to leave the evaluation fixed: I'm going to use MATH level 5 problems (580) and I will be using 5 repetitions for each problem (confidence level 100%). I want to try different prompt strategies: No code prompt Bad prompt Few-shot prompt Few-shot prompt with RAG Carefully crafted prompts If the model is steerable by prompts I will be able to improve the LB score. If not I will have to find another strategy. I need to improve from my current 21 on LB score to 27, that is a +12% in accuracy that I need to get.","title":"Motivation"},{"location":"modeling/Iteration_03_prompt_engineering/#development","text":"","title":"Development"},{"location":"modeling/Iteration_03_prompt_engineering/#results","text":"A total of 29 experiments have been run and the results can be seen on this Google Sheet . All the experiments used 580 Math level 5 problems, 5 repetitions with the hope that the accuracy when using 5 repetitions will correlate with the accuracy of using 25 or 30 repetitions. experiment correct unanswered wrong public notebook prompts 45% 5% 50% forced python public prompts 48% 9% 44% custom prompt v1 48% 8% 44% custom prompt v2 below 47% 5% 48% custom prompt v3 list 48% 5% 47% custom prompt v4 47% 6% 47% cot no code 29% 3% 68% minimal prompt 28% 4% 68% AIMO train 2 shots 52% 5% 43% AIMO train 4 shots 52% 6% 42% MathInstruct 2 shots 46% 6% 48% custom prompt v5 assistant 50% 9% 41% custom prompt v6 easy 49% 6% 45% custom prompt v7 48% 7% 46% AIMO train 2 shots assistant 52% 4% 45% AIMO train 2 shots assistant, forced python 51% 6% 43% MATHInstruct lv5 2 shots 47% 5% 48% MATHInstruct lv5 2 shots RAG 48% 5% 47% AIMO train 2 shots RAG 48% 6% 46% AIMO train 2 shots assistant, T=0.2 48% 5% 47% AIMO train 2 shots assistant, T=0.7 51% 4% 45% AIMO train 2 shots assistant, T=0.9 50% 5% 45% AIMO train 2 shots assistant, T=20 52% 4% 44% AIMO train 2 shots assistant, T=0.7 top_p=0.5 51% 6% 43% AIMO train 2 shots assistant, T=0.9, top_p=0.5 51% 6% 43% custom prompt v8 program 51% 8% 42% custom prompt v9 28% 4% 68% 2 prompts 52% 7% 41% 5 prompts 50% 6% 45% The best results are obtained when using two prompts with the format proposed on DeepSeekMath repo User: PROBLEM_PLACEHOLDER Please reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer. Assistant: Sure, we can solve the problem by writing a Python program. ```python User: PROBLEM_PLACEHOLDER Please integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}. The answer is a non negative integer. Assistant: Sure, we can solve the problem by writing a Python program. ```python We observer drops in accuracy when code is not forced or used. So using code is crucial for this task Number of shots does not seem relevant Retrieval Augmented Generation (RAG) does not give significative improvements. Remember that in the DeepSeekMath paper they mention that the model is too small to benefit from few-shot prompting. The differences between the prompts that use code are very small, not significative The effect of temperature and top_p is uncertain in this evaluation","title":"Results"},{"location":"modeling/Iteration_03_prompt_engineering/#full-evaluation","text":"I have made a full evaluation with 31 repetitions using the two prompts shown in the previous section. Unfortunately there is no significative improvement. It has an accuracy of 57%, just like using the public prompts with python forcing. The new candidate improves the accuracy faster but leads to the same end result. It seems again that we have to do full evaluation, we cannot do just 5 repetitions but we have to do 25.","title":"Full evaluation"},{"location":"modeling/Iteration_03_prompt_engineering/#multi-prompt-full-evaluation","text":"I run a random search to combine multiple prompts from previous experiments. That search found a combination that achieved an accuracy of 61% when mixing different prompts. However the result was optimistic because I was selecting already done evaluations, thus I decided to run a realistic evaluation. confidence runtime (h) accuracy 90% 27.7 59% 95% 29.1 57% Reducing the confidence to 90% results on a minimal speedup, the accuracy is slightly higher but it is not statistically significative. In theory using a higher confidence will lead to more stable and more accurate results. The results from this evaluation do not show significative improvements over previous experiments. However it might have sense to use many different prompts to induce diversity in the responses since we have seen that there are no big differences between good enough prompts.","title":"Multi-prompt full evaluation"},{"location":"modeling/Iteration_03_prompt_engineering/#conclusion","text":"After a week and more than 20 experiments before I have not been able to improve LB score with prompt engineering. How could I improve? Using a better base model to generate answers. Fine-tuning DeepSeekMath may allow to do that. However people in the forum have said that they got worse results after fine-tuning. Remember that the model has already being trained with RL. It is uncertain if fine-tuning can improve the reasoning skills of a model. Change the generation process. Maybe giving as input already generated answers in a dialog between LLMs. Or we might give the possible answers to the model to choose between them, like in a test exam. MMLU example Validate or verify the answers. I might discard wrong answers by validating them. Some problems might be easier to validate than others. Answer selection. Instead of relying on votes, I might use a model to select the best answer. Maybe rewriting the problem in a more clear way could help sometimes. I could create a dataset with rewritten problems and test the accuracy on it. Score of 22 when increasing temperature to 0.9, it was luck because submitting the same version of the notebook returned the following score distribution: [21, 22, 18, 17, 15]","title":"Conclusion"},{"location":"modeling/Iteration_03_prompt_engineering/#next-steps","text":"Reread the literature and better understand how DeepSeekMath model was trained Measure the effect of temperature with a higher number of repetitions","title":"Next steps"},{"location":"modeling/Iteration_03_prompt_engineering/#todo","text":"How long does it take the evaluation with 5 repetitions? around 10 hours. Prompts to evaluate CoT no code prompt Minimal prompt ~~Bad prompt~~ Few-shot prompt Few-shot prompt with RAG Temperature and top_p Prompt that uses code from the repo Ask the model to verify the answer. It ignores the request to verify the answer. If we want to do a verification we should do it manually. Document results Full evaluation with the best configuration Analysis merging the results of all the evaluations Can I find a better combination of prompts? I might drop the problems that do not receive any correct answer. This will allow to speedup evaluation. Role of confidence, what if I decrease confidence from 0.95 to 0.9","title":"TODO"},{"location":"modeling/Iteration_04_answer_validation/","text":"Iteration 4. Answer validation 06-06-2024 Goal Can we improve the accuracy of the model by validating the answers? Motivation The current solution is able to achieve a 75% pass25 accuracy and 57% maj25 accuracy when being evaluated on 580 level 5 MATH problems. This implies that the majority has an accuracy of 76% for selecting the correct answer. Can I use an LLM to validate or discard answers? That might help to improve the accuracy of the solution. Another option would be to give the numerical answers as options and request to solve the problem again. However there is very little information provided in that scenario, I don't believe it has sense. We might give complete solutions to the model and ask the model to select the correct one. That way the model has a lot of context and this might work. The downside is that when using the P100 the available context window is small. We might have to use another model such as phi-2. Development Play with answer validation The idea is to take the responses from an evaluation and request the model to validate them. Using the chat format could be a good way to try that. Results DeepSeekMath is not able to validate responses or select the correct response I have made multiple experiments with different problems and responses and almost always DeepSeekMath says that the answer is correct. Even more surprising is that GPT4-o makes obvious mistakes also. I have also tested giving multiple response to the model and asking to select the correct and the responses seemed to be random. This DeepSeekMath model is not very intelligent. I might try with other models. Conclusion DeepSeekMath is a model that can generate responses to mathematical problems, but it is not able of validating the answers or select the correct response among a few responses. It is not an intelligent model. Next steps Try different models instead of Kaggle's DeepSeekMath-RL Try VLLM to speedup model inference Experiments with different temperatures or increasing temperatures I might improve the results by rewriting the problems to make them more clear, or divide the problem in steps. TODO [ ]","title":"Iteration 4. Answer validation"},{"location":"modeling/Iteration_04_answer_validation/#iteration-4-answer-validation","text":"06-06-2024","title":"Iteration 4. Answer validation"},{"location":"modeling/Iteration_04_answer_validation/#goal","text":"Can we improve the accuracy of the model by validating the answers?","title":"Goal"},{"location":"modeling/Iteration_04_answer_validation/#motivation","text":"The current solution is able to achieve a 75% pass25 accuracy and 57% maj25 accuracy when being evaluated on 580 level 5 MATH problems. This implies that the majority has an accuracy of 76% for selecting the correct answer. Can I use an LLM to validate or discard answers? That might help to improve the accuracy of the solution. Another option would be to give the numerical answers as options and request to solve the problem again. However there is very little information provided in that scenario, I don't believe it has sense. We might give complete solutions to the model and ask the model to select the correct one. That way the model has a lot of context and this might work. The downside is that when using the P100 the available context window is small. We might have to use another model such as phi-2.","title":"Motivation"},{"location":"modeling/Iteration_04_answer_validation/#development","text":"","title":"Development"},{"location":"modeling/Iteration_04_answer_validation/#play-with-answer-validation","text":"The idea is to take the responses from an evaluation and request the model to validate them. Using the chat format could be a good way to try that.","title":"Play with answer validation"},{"location":"modeling/Iteration_04_answer_validation/#results","text":"","title":"Results"},{"location":"modeling/Iteration_04_answer_validation/#deepseekmath-is-not-able-to-validate-responses-or-select-the-correct-response","text":"I have made multiple experiments with different problems and responses and almost always DeepSeekMath says that the answer is correct. Even more surprising is that GPT4-o makes obvious mistakes also. I have also tested giving multiple response to the model and asking to select the correct and the responses seemed to be random. This DeepSeekMath model is not very intelligent. I might try with other models.","title":"DeepSeekMath is not able to validate responses or select the correct response"},{"location":"modeling/Iteration_04_answer_validation/#conclusion","text":"DeepSeekMath is a model that can generate responses to mathematical problems, but it is not able of validating the answers or select the correct response among a few responses. It is not an intelligent model.","title":"Conclusion"},{"location":"modeling/Iteration_04_answer_validation/#next-steps","text":"Try different models instead of Kaggle's DeepSeekMath-RL Try VLLM to speedup model inference Experiments with different temperatures or increasing temperatures I might improve the results by rewriting the problems to make them more clear, or divide the problem in steps.","title":"Next steps"},{"location":"modeling/Iteration_04_answer_validation/#todo","text":"[ ]","title":"TODO"},{"location":"modeling/Iteration_05_vllm/","text":"Iteration 5. VLLM 07-06-2024 Goal Can I speedup inference using VLLM ? Motivation If I can make a more efficient inference maximizing hardware utilization I might increase the number of repetitions. If I can double the number of repetitions from 30 to 60 I may increase the accuracy by 5%. Development What is VLLM ? A high-throughput and memory-efficient inference and serving engine for LLMs It is open-source and it has an Apache-2.0 license. GPU: compute capability 7.0 or higher (e.g., V100, T4, RTX20xx, A100, L4, H100, etc.) My intuition is that the highest throughput will be achieved using 2xT4, which is on that list. One option would be to create a server and make http requests to it. This will decouple the use of GPU and the inference logic completely, allowing to parallelize the inference completely. Another option might be to use this class: https://docs.vllm.ai/en/stable/dev/engine/async_llm_engine.html https://github.com/vllm-project/vllm/issues/1200 Using VLLM on Kaggle on T4 GPUs T4 GPUs have 15360MiB according to Nvidia-smi, 3090s have 24576MiB. So that is 0.625 of the memory of my home PC. I could use that value to simulate and experiment faster at home. Loading model weights took 12.8725 GB Quantization fp8 , althought is documented it does not find that option on Kaggle, maybe there is a missmatch between versions awq -> ValueError: Cannot find the config file for awq gptq -> ValueError: Cannot find the config file for gptq squeezellm -> ValueError: Cannot find the config file for squeezellm # when not using quantization and just a single worker I get this error # maybe there is a problem with the installation of vllm top_logprobs = prompt_logprobs + output.logprobs TypeError: unsupported operand type(s) for +: 'NoneType' and 'NoneType' If I install vllm directly from pip I have been able to make one succesfull inference. So maybe the wheels are not correct. Local runs I'm able to run inference with just 62% of 3090 memory with 1 repetition. When reducing gpu memory to 60% I get this error. [rank0]: ValueError: The model's max seq len (1536) is larger than the maximum number of tokens that can be stored in KV cache (1104). Try increasing gpu_memory_utilization or decreasing max_model_len when initializing the engine. I need to make the processes robust to exceptions, and log them. Also remove tokenizer from code This local runs with lower memory will show me the speedup I could get on Kaggle. Maybe I will have to use a quantized model, that will allow to make batched inference. Probably using sort prompts will be better. Results Validation results repetitions accuracy runtime (min) 25 57% 219 100 59% 463 200 61% 830 I'm able to do an evaluation with 200 repetitions in 13 hours using the 2 gpus. Without using VLLM I was able to do an evaluation with 25 repetitions in about the same time. So we have achieved an speedup of x8 with the introduction of VLLM. The speedup is even higher because previous evaluation had early stopping for each problem and I haven't implemented that yet with VLLM (probably the speedup is close to x16). This speedup comes from batching inferences and being able to parallelize the CPU and GPU execution. We can observe a small increase in accuracy when using more repetitions. It seems that with the current configuration after 200-300 repetitions there is no improvement. Probably evaluating just by 100 repetitions should be enough to measure improvements very reliably. Leaderboard results repetitions LB score 30 16, 17, 18, 18, 18, 19, 20 200 18, 18, 18, 19, 20, 21 On submission I have been able to increase the repetitions from 30 to 200. Thus we see a similar speedup. It is unclear if we have improved. The min score is higher Optimal number of workers For Kaggle 30 workers will be fine, maybe 20 even better if I implement early stopping. My PC is more powerful and 80 workers is a good choice. Conclusion I have been able to speedup inference with a factor higher than x8, but it is unclear if the LB score has improved. With 21 problems solved in the luckiest submission we are still far from the top team which is is able to solve 27 problems which is much better (42% vs 54%). Next steps Maybe some advanced inference like speculative decoding might improve the results? I'm not sure what it is but VLLM has some parameters for that. I have done a quick search and it is to use a small model in combination with a bigger model to be more efficient on inference. In other article it seems to be to predict multiple tokens at once. Can I reduce inference time using early stopping confidence in the decision? Probably, but that is not expected to bring improvements. Maybe playing with the temperature, increasing the temperature on each step? What if I use a different model on each server? The RL and the instruct versions of DeepSeekMath TODO Refactor current code to make room for VLLM What would be the max seq len available at Kaggle? It seems to be capable of handling 1536 evaluation I haven't seen any problem when increasing the max output tokens from 640 to 896. Neither when adding few-shot prompting (with 1 example).","title":"Iteration 5. VLLM"},{"location":"modeling/Iteration_05_vllm/#iteration-5-vllm","text":"07-06-2024","title":"Iteration 5. VLLM"},{"location":"modeling/Iteration_05_vllm/#goal","text":"Can I speedup inference using VLLM ?","title":"Goal"},{"location":"modeling/Iteration_05_vllm/#motivation","text":"If I can make a more efficient inference maximizing hardware utilization I might increase the number of repetitions. If I can double the number of repetitions from 30 to 60 I may increase the accuracy by 5%.","title":"Motivation"},{"location":"modeling/Iteration_05_vllm/#development","text":"","title":"Development"},{"location":"modeling/Iteration_05_vllm/#what-is-vllm","text":"A high-throughput and memory-efficient inference and serving engine for LLMs It is open-source and it has an Apache-2.0 license. GPU: compute capability 7.0 or higher (e.g., V100, T4, RTX20xx, A100, L4, H100, etc.) My intuition is that the highest throughput will be achieved using 2xT4, which is on that list. One option would be to create a server and make http requests to it. This will decouple the use of GPU and the inference logic completely, allowing to parallelize the inference completely. Another option might be to use this class: https://docs.vllm.ai/en/stable/dev/engine/async_llm_engine.html https://github.com/vllm-project/vllm/issues/1200","title":"What is VLLM?"},{"location":"modeling/Iteration_05_vllm/#using-vllm-on-kaggle-on-t4-gpus","text":"T4 GPUs have 15360MiB according to Nvidia-smi, 3090s have 24576MiB. So that is 0.625 of the memory of my home PC. I could use that value to simulate and experiment faster at home. Loading model weights took 12.8725 GB","title":"Using VLLM on Kaggle on T4 GPUs"},{"location":"modeling/Iteration_05_vllm/#quantization","text":"fp8 , althought is documented it does not find that option on Kaggle, maybe there is a missmatch between versions awq -> ValueError: Cannot find the config file for awq gptq -> ValueError: Cannot find the config file for gptq squeezellm -> ValueError: Cannot find the config file for squeezellm # when not using quantization and just a single worker I get this error # maybe there is a problem with the installation of vllm top_logprobs = prompt_logprobs + output.logprobs TypeError: unsupported operand type(s) for +: 'NoneType' and 'NoneType' If I install vllm directly from pip I have been able to make one succesfull inference. So maybe the wheels are not correct.","title":"Quantization"},{"location":"modeling/Iteration_05_vllm/#local-runs","text":"I'm able to run inference with just 62% of 3090 memory with 1 repetition. When reducing gpu memory to 60% I get this error. [rank0]: ValueError: The model's max seq len (1536) is larger than the maximum number of tokens that can be stored in KV cache (1104). Try increasing gpu_memory_utilization or decreasing max_model_len when initializing the engine. I need to make the processes robust to exceptions, and log them. Also remove tokenizer from code This local runs with lower memory will show me the speedup I could get on Kaggle. Maybe I will have to use a quantized model, that will allow to make batched inference. Probably using sort prompts will be better.","title":"Local runs"},{"location":"modeling/Iteration_05_vllm/#results","text":"","title":"Results"},{"location":"modeling/Iteration_05_vllm/#validation-results","text":"repetitions accuracy runtime (min) 25 57% 219 100 59% 463 200 61% 830 I'm able to do an evaluation with 200 repetitions in 13 hours using the 2 gpus. Without using VLLM I was able to do an evaluation with 25 repetitions in about the same time. So we have achieved an speedup of x8 with the introduction of VLLM. The speedup is even higher because previous evaluation had early stopping for each problem and I haven't implemented that yet with VLLM (probably the speedup is close to x16). This speedup comes from batching inferences and being able to parallelize the CPU and GPU execution. We can observe a small increase in accuracy when using more repetitions. It seems that with the current configuration after 200-300 repetitions there is no improvement. Probably evaluating just by 100 repetitions should be enough to measure improvements very reliably.","title":"Validation results"},{"location":"modeling/Iteration_05_vllm/#leaderboard-results","text":"repetitions LB score 30 16, 17, 18, 18, 18, 19, 20 200 18, 18, 18, 19, 20, 21 On submission I have been able to increase the repetitions from 30 to 200. Thus we see a similar speedup. It is unclear if we have improved. The min score is higher","title":"Leaderboard results"},{"location":"modeling/Iteration_05_vllm/#optimal-number-of-workers","text":"For Kaggle 30 workers will be fine, maybe 20 even better if I implement early stopping. My PC is more powerful and 80 workers is a good choice.","title":"Optimal number of workers"},{"location":"modeling/Iteration_05_vllm/#conclusion","text":"I have been able to speedup inference with a factor higher than x8, but it is unclear if the LB score has improved. With 21 problems solved in the luckiest submission we are still far from the top team which is is able to solve 27 problems which is much better (42% vs 54%).","title":"Conclusion"},{"location":"modeling/Iteration_05_vllm/#next-steps","text":"Maybe some advanced inference like speculative decoding might improve the results? I'm not sure what it is but VLLM has some parameters for that. I have done a quick search and it is to use a small model in combination with a bigger model to be more efficient on inference. In other article it seems to be to predict multiple tokens at once. Can I reduce inference time using early stopping confidence in the decision? Probably, but that is not expected to bring improvements. Maybe playing with the temperature, increasing the temperature on each step? What if I use a different model on each server? The RL and the instruct versions of DeepSeekMath","title":"Next steps"},{"location":"modeling/Iteration_05_vllm/#todo","text":"Refactor current code to make room for VLLM What would be the max seq len available at Kaggle? It seems to be capable of handling 1536 evaluation I haven't seen any problem when increasing the max output tokens from 640 to 896. Neither when adding few-shot prompting (with 1 example).","title":"TODO"},{"location":"modeling/Iteration_06_different_models/","text":"Iteration n. Iteration_title 12-06-2024 Goal Can I improve the score using different models? Motivation I have noticed that the DeepSeekMath-RL model that is on Kaggle has different files than the one on Huggingface. Maybe it is worse. Development Results Evaluation without VLLM The evaluation is done without VLLM because it was done previously to the development. So it only has 25 repetitions. Moreover it was done just on half of the data to speedup the evaluation. model Accuracy Kaggle DeepSeekMath-RL 61.50% DeepSeekMath-Instruct 51.50% DeepSeekMath-RL 59.00% There are no significative differences between Kaggle's and Huggingface's models. The Instruct model is clearly worse. Ensembling the models Now let's do an evaluation with VLLM. What if I use a different model on each GPU? That might be beneficial. The following experiment uses 100 repetitions, and only changes the model. model Accuracy RL + instruct 58% RL 59% We don't see any improvement when using two different models. This is probably happening because as seen on the previous section the instruct model is clearly worse. Conclusion We did not observe improvements in validation score when trying other models from the DeepSeekMath family. Next steps I could do experiments with the output tokens and temperature. This might bring small improvements, I don't expect big changes. I might fine-tune DeepSeekMath with DPO on the MATH test dataset. I will destroy my ability to measure improvements but maybe it improves on LB. I haven't explored this path yet and I don't see too many alternatives. TODO [ ]","title":"Iteration n. Iteration_title"},{"location":"modeling/Iteration_06_different_models/#iteration-n-iteration_title","text":"12-06-2024","title":"Iteration n. Iteration_title"},{"location":"modeling/Iteration_06_different_models/#goal","text":"Can I improve the score using different models?","title":"Goal"},{"location":"modeling/Iteration_06_different_models/#motivation","text":"I have noticed that the DeepSeekMath-RL model that is on Kaggle has different files than the one on Huggingface. Maybe it is worse.","title":"Motivation"},{"location":"modeling/Iteration_06_different_models/#development","text":"","title":"Development"},{"location":"modeling/Iteration_06_different_models/#results","text":"","title":"Results"},{"location":"modeling/Iteration_06_different_models/#evaluation-without-vllm","text":"The evaluation is done without VLLM because it was done previously to the development. So it only has 25 repetitions. Moreover it was done just on half of the data to speedup the evaluation. model Accuracy Kaggle DeepSeekMath-RL 61.50% DeepSeekMath-Instruct 51.50% DeepSeekMath-RL 59.00% There are no significative differences between Kaggle's and Huggingface's models. The Instruct model is clearly worse.","title":"Evaluation without VLLM"},{"location":"modeling/Iteration_06_different_models/#ensembling-the-models","text":"Now let's do an evaluation with VLLM. What if I use a different model on each GPU? That might be beneficial. The following experiment uses 100 repetitions, and only changes the model. model Accuracy RL + instruct 58% RL 59% We don't see any improvement when using two different models. This is probably happening because as seen on the previous section the instruct model is clearly worse.","title":"Ensembling the models"},{"location":"modeling/Iteration_06_different_models/#conclusion","text":"We did not observe improvements in validation score when trying other models from the DeepSeekMath family.","title":"Conclusion"},{"location":"modeling/Iteration_06_different_models/#next-steps","text":"I could do experiments with the output tokens and temperature. This might bring small improvements, I don't expect big changes. I might fine-tune DeepSeekMath with DPO on the MATH test dataset. I will destroy my ability to measure improvements but maybe it improves on LB. I haven't explored this path yet and I don't see too many alternatives.","title":"Next steps"},{"location":"modeling/Iteration_06_different_models/#todo","text":"[ ]","title":"TODO"},{"location":"modeling/Iteration_07_tweak_inference_parameters/","text":"Iteration n. Iteration_title 13/06/2024 Goal Can I improve the LB score by changing parameters such as temperature, top_p or output tokens? Motivation There might be room for improvement by changing the inference parameters. Development Results Output tokens On some previous experiments it seemed that using a maximum number of output tokens of 640 could be enough to solve the problems. Now that using VLLM I can do more repetitions I want to revisit that. Output tokens runtime (min) Accuracy maj Accuracy pass 640 463 59% 87% 896 604 62% 90% 1024 650 61% 91% Temperature and top_p How does temperature affect to validation accuracy? Conclusion Next steps TODO [ ]","title":"Iteration n. Iteration_title"},{"location":"modeling/Iteration_07_tweak_inference_parameters/#iteration-n-iteration_title","text":"13/06/2024","title":"Iteration n. Iteration_title"},{"location":"modeling/Iteration_07_tweak_inference_parameters/#goal","text":"Can I improve the LB score by changing parameters such as temperature, top_p or output tokens?","title":"Goal"},{"location":"modeling/Iteration_07_tweak_inference_parameters/#motivation","text":"There might be room for improvement by changing the inference parameters.","title":"Motivation"},{"location":"modeling/Iteration_07_tweak_inference_parameters/#development","text":"","title":"Development"},{"location":"modeling/Iteration_07_tweak_inference_parameters/#results","text":"","title":"Results"},{"location":"modeling/Iteration_07_tweak_inference_parameters/#output-tokens","text":"On some previous experiments it seemed that using a maximum number of output tokens of 640 could be enough to solve the problems. Now that using VLLM I can do more repetitions I want to revisit that. Output tokens runtime (min) Accuracy maj Accuracy pass 640 463 59% 87% 896 604 62% 90% 1024 650 61% 91%","title":"Output tokens"},{"location":"modeling/Iteration_07_tweak_inference_parameters/#temperature-and-top_p","text":"How does temperature affect to validation accuracy?","title":"Temperature and top_p"},{"location":"modeling/Iteration_07_tweak_inference_parameters/#conclusion","text":"","title":"Conclusion"},{"location":"modeling/Iteration_07_tweak_inference_parameters/#next-steps","text":"","title":"Next steps"},{"location":"modeling/Iteration_07_tweak_inference_parameters/#todo","text":"[ ]","title":"TODO"},{"location":"modeling/Iteration_08_fine-tuning/","text":"Iteration 8. Fine-tuning 13/06/2024 Goal Can I improve the LB score by fine-tuning DeepSeekMath on MATH problems? Motivation The best team is getting a score of 27 on leaderboard, while my luckiest submission only gets 22. I have done already many experiments with DeepSeekMath-RL model, so maybe their advantage is that they have fine-tuned the model to high school problems. Development Direct Preference Optimization My idea is to use DPO to teach the model to better solve the math problems. I have evaluated the MATH dataset many times and thus for each problem I have a lot of good and bad answers. I can use that data to fine-tune the model. Then I will evaluate on the test set and I should get a huge improvement since I will be training and evaluating on the same set. But that will validate that the training has worked. The real test will be the leaderboard. If I see improvements in the leaderboard then the next step would be to gather new data to evaluate and later fine-tune the model. How to train with DPO? https://huggingface.co/docs/trl/main/en/dpo_trainer https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py I fine-tuned a model with LoRA for the LLM prompt recovery challenge. Example notebook https://www.philschmid.de/dpo-align-llms-in-2024-with-trl https://medium.com/@anchen.li/fine-tune-llama-2-with-sft-and-dpo-8b57cf3ec69 ? I have to create a dataset with the fields: prompt, chosen and rejected. The prompt does not need to be in the answers. Source code Dataset for DPO 509 MATH test level 5 problems 10661 pairs of good and bad responses Max prompt length: 296 Max length: 937 Results Validation results I haven't been able to improve the validation accuracy consistently despite training and evaluating on the same dataset. I have seen improvements but too small considering I was training on that dataset. experiment runtime(min) Accuracy maj Accuracy pass baseline 134 46% 58% 01_first_steps 172 48% 54% 02_shuffle_train_set 163 53% 59% 03_4_epochs 190 47% 50% 04_4_epochs_constant_lr 183 52% 56% 06_v1_dataset 189 55% 59% 07_v2_dataset 182 49% 53% On v1 version of the dataset I moved the python code block start to the prompt On v2 version I used the same number of train pairs for each problem and increased the dataset size to 25k pairs. To verify that the model was learning I added a silly comment in the python code but only was generated on inference on 18% of the responses, despite being so easy to learn. Conclusion I haven't been able to successfully fine-tune DeepSeekMath model. I haven't made a submission since I did not get good results on validation. Next steps TODO Notebook to create train dataset, that will make the training notebook shorter. How to train the model using DPO and LoRA? How to make inference with VLLM and LoRA?","title":"Iteration 8. Fine-tuning"},{"location":"modeling/Iteration_08_fine-tuning/#iteration-8-fine-tuning","text":"13/06/2024","title":"Iteration 8. Fine-tuning"},{"location":"modeling/Iteration_08_fine-tuning/#goal","text":"Can I improve the LB score by fine-tuning DeepSeekMath on MATH problems?","title":"Goal"},{"location":"modeling/Iteration_08_fine-tuning/#motivation","text":"The best team is getting a score of 27 on leaderboard, while my luckiest submission only gets 22. I have done already many experiments with DeepSeekMath-RL model, so maybe their advantage is that they have fine-tuned the model to high school problems.","title":"Motivation"},{"location":"modeling/Iteration_08_fine-tuning/#development","text":"","title":"Development"},{"location":"modeling/Iteration_08_fine-tuning/#direct-preference-optimization","text":"My idea is to use DPO to teach the model to better solve the math problems. I have evaluated the MATH dataset many times and thus for each problem I have a lot of good and bad answers. I can use that data to fine-tune the model. Then I will evaluate on the test set and I should get a huge improvement since I will be training and evaluating on the same set. But that will validate that the training has worked. The real test will be the leaderboard. If I see improvements in the leaderboard then the next step would be to gather new data to evaluate and later fine-tune the model.","title":"Direct Preference Optimization"},{"location":"modeling/Iteration_08_fine-tuning/#how-to-train-with-dpo","text":"https://huggingface.co/docs/trl/main/en/dpo_trainer https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py I fine-tuned a model with LoRA for the LLM prompt recovery challenge. Example notebook https://www.philschmid.de/dpo-align-llms-in-2024-with-trl https://medium.com/@anchen.li/fine-tune-llama-2-with-sft-and-dpo-8b57cf3ec69 ? I have to create a dataset with the fields: prompt, chosen and rejected. The prompt does not need to be in the answers. Source code","title":"How to train with DPO?"},{"location":"modeling/Iteration_08_fine-tuning/#dataset-for-dpo","text":"509 MATH test level 5 problems 10661 pairs of good and bad responses Max prompt length: 296 Max length: 937","title":"Dataset for DPO"},{"location":"modeling/Iteration_08_fine-tuning/#results","text":"","title":"Results"},{"location":"modeling/Iteration_08_fine-tuning/#validation-results","text":"I haven't been able to improve the validation accuracy consistently despite training and evaluating on the same dataset. I have seen improvements but too small considering I was training on that dataset. experiment runtime(min) Accuracy maj Accuracy pass baseline 134 46% 58% 01_first_steps 172 48% 54% 02_shuffle_train_set 163 53% 59% 03_4_epochs 190 47% 50% 04_4_epochs_constant_lr 183 52% 56% 06_v1_dataset 189 55% 59% 07_v2_dataset 182 49% 53% On v1 version of the dataset I moved the python code block start to the prompt On v2 version I used the same number of train pairs for each problem and increased the dataset size to 25k pairs. To verify that the model was learning I added a silly comment in the python code but only was generated on inference on 18% of the responses, despite being so easy to learn.","title":"Validation results"},{"location":"modeling/Iteration_08_fine-tuning/#conclusion","text":"I haven't been able to successfully fine-tune DeepSeekMath model. I haven't made a submission since I did not get good results on validation.","title":"Conclusion"},{"location":"modeling/Iteration_08_fine-tuning/#next-steps","text":"","title":"Next steps"},{"location":"modeling/Iteration_08_fine-tuning/#todo","text":"Notebook to create train dataset, that will make the training notebook shorter. How to train the model using DPO and LoRA? How to make inference with VLLM and LoRA?","title":"TODO"},{"location":"modeling/Iteration_09_hinting_the_model/","text":"Iteration 9. Giving hints to the model 17/06/2024 Goal Can we improve the accuracy of the model by giving hints of how to solve the problem? Does the model improve if being given hints? Can I automate the process of giving hints? Motivation I have explored almost all ideas: Using VLLM to speedup inference. Inference was much faster but results didn't improve. Could not fine-tune the model to be more accurate Could not make the model to validate or select the correct answer Using the instruct model in an ensemble with the RL model did not improve So today I was re-reading the forum and read some comments about how brittle the model is. Changing small parts of a problem results in very different inferences. So maybe we can use that in our favour. Development On a first step I'm going to take problems where the model is struggling to solve them. Since the MATH dataset has solutions I could use them for inspiration to give hints to the model. I will measure how much the model improves and if it is promising I will have to find a way to automate the process of giving hints. Results Some problems are better solved without using code. Adding that the answer is a non negative answer might be harmful Sometimes reason step by step works worse than simply giving the problem as input Hints do not seem to work too well Following this tips I have run a new validation with simpler prompts, some of them without python forcing and I get the same accuracy of 59% of previous configurations. However when making a submission it seems to be slightly better than previous prompts. prompts LB score mean LB score multiple prompts 18, 18, 18, 19, 20, 21 19.0 simpler prompts 19, 19, 20, 20, 20, 20, 21 19.9 Conclusion I have found that some problems are better solved without code and even without chain of thought. Next steps Select the final submissions TODO Prompts without python code What if I remove references to the answer being non negative? I could later discard negative values. If the model is brittle that might help.","title":"Iteration 9. Giving hints to the model"},{"location":"modeling/Iteration_09_hinting_the_model/#iteration-9-giving-hints-to-the-model","text":"17/06/2024","title":"Iteration 9. Giving hints to the model"},{"location":"modeling/Iteration_09_hinting_the_model/#goal","text":"Can we improve the accuracy of the model by giving hints of how to solve the problem? Does the model improve if being given hints? Can I automate the process of giving hints?","title":"Goal"},{"location":"modeling/Iteration_09_hinting_the_model/#motivation","text":"I have explored almost all ideas: Using VLLM to speedup inference. Inference was much faster but results didn't improve. Could not fine-tune the model to be more accurate Could not make the model to validate or select the correct answer Using the instruct model in an ensemble with the RL model did not improve So today I was re-reading the forum and read some comments about how brittle the model is. Changing small parts of a problem results in very different inferences. So maybe we can use that in our favour.","title":"Motivation"},{"location":"modeling/Iteration_09_hinting_the_model/#development","text":"On a first step I'm going to take problems where the model is struggling to solve them. Since the MATH dataset has solutions I could use them for inspiration to give hints to the model. I will measure how much the model improves and if it is promising I will have to find a way to automate the process of giving hints.","title":"Development"},{"location":"modeling/Iteration_09_hinting_the_model/#results","text":"Some problems are better solved without using code. Adding that the answer is a non negative answer might be harmful Sometimes reason step by step works worse than simply giving the problem as input Hints do not seem to work too well Following this tips I have run a new validation with simpler prompts, some of them without python forcing and I get the same accuracy of 59% of previous configurations. However when making a submission it seems to be slightly better than previous prompts. prompts LB score mean LB score multiple prompts 18, 18, 18, 19, 20, 21 19.0 simpler prompts 19, 19, 20, 20, 20, 20, 21 19.9","title":"Results"},{"location":"modeling/Iteration_09_hinting_the_model/#conclusion","text":"I have found that some problems are better solved without code and even without chain of thought.","title":"Conclusion"},{"location":"modeling/Iteration_09_hinting_the_model/#next-steps","text":"Select the final submissions","title":"Next steps"},{"location":"modeling/Iteration_09_hinting_the_model/#todo","text":"Prompts without python code What if I remove references to the answer being non negative? I could later discard negative values. If the model is brittle that might help.","title":"TODO"},{"location":"modeling/Iteration_10_select_submission/","text":"Iteration 10. Select submissions 25/06/2024 Goal Select a pair of submissions that maximize the chance of ending in a good position. Motivation I have made very little or none progression on the leaderboard throughout this challenge. However the private test set is very small and the date of submission could be very relevant. Results version repetitions date LB score mean LB score 155 public prompts 22 22/05/2024 16, 21, 21, 21 19.8 179 multiple prompts 30 05/06/2024 16, 17, 18, 18, 18, 19, 20 18.0 VLLM 16 multiple prompts 200 10/06/2024 18, 18, 18, 19, 20, 21 19.0 VLLM 28 original prompts 175 20/06/2024 19, 19, 20, 20, 20, 20, 21 19.9 VLLM 31 public prompts 130 22/06/2024 19, 20, 22, 22, 22 21.0 I'm going to submit version 155 and VLLM 28: Version 155 scored high one month ago, if I'm lucky that early submission could pay off. Version VLLM28 uses a great number of repetitions and uses the prompts defined on the official repo. On validation the score is the same as the other approaches, but I hope they will generalize better to the private test set. I'm using the official prompts after all! Conclusion I'm going to need luck in this challenge, currently I'm on position 137 with a score of 22, while the first team has a score of 28. Hopefully I will learn something from the best teams. Next steps Review solutions from other teams","title":"Iteration 10. Select submissions"},{"location":"modeling/Iteration_10_select_submission/#iteration-10-select-submissions","text":"25/06/2024","title":"Iteration 10. Select submissions"},{"location":"modeling/Iteration_10_select_submission/#goal","text":"Select a pair of submissions that maximize the chance of ending in a good position.","title":"Goal"},{"location":"modeling/Iteration_10_select_submission/#motivation","text":"I have made very little or none progression on the leaderboard throughout this challenge. However the private test set is very small and the date of submission could be very relevant.","title":"Motivation"},{"location":"modeling/Iteration_10_select_submission/#results","text":"version repetitions date LB score mean LB score 155 public prompts 22 22/05/2024 16, 21, 21, 21 19.8 179 multiple prompts 30 05/06/2024 16, 17, 18, 18, 18, 19, 20 18.0 VLLM 16 multiple prompts 200 10/06/2024 18, 18, 18, 19, 20, 21 19.0 VLLM 28 original prompts 175 20/06/2024 19, 19, 20, 20, 20, 20, 21 19.9 VLLM 31 public prompts 130 22/06/2024 19, 20, 22, 22, 22 21.0 I'm going to submit version 155 and VLLM 28: Version 155 scored high one month ago, if I'm lucky that early submission could pay off. Version VLLM28 uses a great number of repetitions and uses the prompts defined on the official repo. On validation the score is the same as the other approaches, but I hope they will generalize better to the private test set. I'm using the official prompts after all!","title":"Results"},{"location":"modeling/Iteration_10_select_submission/#conclusion","text":"I'm going to need luck in this challenge, currently I'm on position 137 with a score of 22, while the first team has a score of 28. Hopefully I will learn something from the best teams.","title":"Conclusion"},{"location":"modeling/Iteration_10_select_submission/#next-steps","text":"Review solutions from other teams","title":"Next steps"},{"location":"utils/00_Challenge_Workflow/","text":"Challenge workflow Start of the challenge Create a repository for the code using cookiecutter Add dates to the calendar Download rules of the challenge Bookmark challenge folder on file explorer Create a Google keep label for tasks and ideas of the challenge Download the challenge data Create a conda environment for the challenge and add it to jupyter conda create -n aimo pytest rope pylint tqdm numpy pandas scikit-learn ipython ipykernel coverage ipywidgets matplotlib python=3.10 -y conda activate aimo python -m ipykernel install --user --name $CONDA_DEFAULT_ENV --display-name \"Python ($CONDA_DEFAULT_ENV)\" make env-export Create a github repo to have a backup of the data. Vscode allows to do it directly without having to go to the website, choose a private repo. At the end of the challenge it will be made public. Use TDD methodology whenever possible, this will save time because errors won't be propagated along the challenge. Have an apprentice attitude, collaborate on the forum, I have a lot to learn from Kaggle. Add a nice picture to README End of the challenge Prepare a report with a summary of the approach to the challenge Download the Google keep tasks to the repository in pdf format Delete the tasks on google keep and the label Delete unnecessary data Update the environment yml","title":"Challenge workflow"},{"location":"utils/00_Challenge_Workflow/#challenge-workflow","text":"","title":"Challenge workflow"},{"location":"utils/00_Challenge_Workflow/#start-of-the-challenge","text":"Create a repository for the code using cookiecutter Add dates to the calendar Download rules of the challenge Bookmark challenge folder on file explorer Create a Google keep label for tasks and ideas of the challenge Download the challenge data Create a conda environment for the challenge and add it to jupyter conda create -n aimo pytest rope pylint tqdm numpy pandas scikit-learn ipython ipykernel coverage ipywidgets matplotlib python=3.10 -y conda activate aimo python -m ipykernel install --user --name $CONDA_DEFAULT_ENV --display-name \"Python ($CONDA_DEFAULT_ENV)\" make env-export Create a github repo to have a backup of the data. Vscode allows to do it directly without having to go to the website, choose a private repo. At the end of the challenge it will be made public. Use TDD methodology whenever possible, this will save time because errors won't be propagated along the challenge. Have an apprentice attitude, collaborate on the forum, I have a lot to learn from Kaggle. Add a nice picture to README","title":"Start of the challenge"},{"location":"utils/00_Challenge_Workflow/#end-of-the-challenge","text":"Prepare a report with a summary of the approach to the challenge Download the Google keep tasks to the repository in pdf format Delete the tasks on google keep and the label Delete unnecessary data Update the environment yml","title":"End of the challenge"},{"location":"utils/markdown_cheatsheet/","text":"Markdown cheatsheet Examples of attaching images First an image with markdown syntax Next an image with html syntax that allows to control the size Examples of equations Equation on a different line: \\[\\epsilon = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(log(\\frac{p_i}{a_i}))^2} \\] Examples of inline equations. Let's consider \\(e^{q_i} = p_i + 1\\) and \\(e^{b_i} = a_i + 1\\) . Easy way to create tables http://www.tablesgenerator.com/markdown_tables# representation_size fmeasure val_fmeasure 1024 0.893 0.573 512 0.819 0.476 256 0.676 0.365 128 0.45 0.33 64 0.31 0.29 32 0.26 0.26 16 0.214 0.216","title":"Markdown cheatsheet"},{"location":"utils/markdown_cheatsheet/#markdown-cheatsheet","text":"","title":"Markdown cheatsheet"},{"location":"utils/markdown_cheatsheet/#examples-of-attaching-images","text":"First an image with markdown syntax Next an image with html syntax that allows to control the size","title":"Examples of attaching images"},{"location":"utils/markdown_cheatsheet/#examples-of-equations","text":"Equation on a different line: \\[\\epsilon = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(log(\\frac{p_i}{a_i}))^2} \\] Examples of inline equations. Let's consider \\(e^{q_i} = p_i + 1\\) and \\(e^{b_i} = a_i + 1\\) .","title":"Examples of equations"},{"location":"utils/markdown_cheatsheet/#easy-way-to-create-tables","text":"http://www.tablesgenerator.com/markdown_tables# representation_size fmeasure val_fmeasure 1024 0.893 0.573 512 0.819 0.476 256 0.676 0.365 128 0.45 0.33 64 0.31 0.29 32 0.26 0.26 16 0.214 0.216","title":"Easy way to create tables"},{"location":"utils/methodology/","text":"Methodology I'm following CRISP-DM 1.0 methodology for the reports. I have skipped Evaluation and Deployment steps because they are not usually done on Kaggle. Business understanding Data understanding Data preparation Modeling Solution summary","title":"Methodology"},{"location":"utils/methodology/#methodology","text":"I'm following CRISP-DM 1.0 methodology for the reports. I have skipped Evaluation and Deployment steps because they are not usually done on Kaggle. Business understanding Data understanding Data preparation Modeling Solution summary","title":"Methodology"}]}