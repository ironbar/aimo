{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset for DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset with the required format for DPO fine-tuning:\n",
    "\n",
    "- https://huggingface.co/docs/trl/main/en/dpo_trainer#expected-dataset-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe should have the following fields: prompt, chosen and rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of the MATH test dataset provides me with correct and incorrect answers. I could change the prompt at my will."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json as json\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import random\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/mnt/hdd0/Kaggle/aimo/external_data/filtered_MATH_test_5.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/Kaggle/aimo/experiments/17_vllm/400_repetitions.json', 'r') as file:\n",
    "    responses = json.load(file)\n",
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses['0'][0]['prompt'][-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the following definition of good and bad responses:\n",
    "\n",
    "- A good response is one where the text and code answer is the same and equal to the ground truth.\n",
    "- A bad response is one where there is at least text answer and it is different to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_responses = dict()\n",
    "bad_responses = dict()\n",
    "\n",
    "for idx, ground_truth in tqdm(enumerate(dataset['answer']), total=len(dataset)):\n",
    "    good_responses[idx] = []\n",
    "    bad_responses[idx] = []\n",
    "    for result in responses[str(idx)]:\n",
    "        if ground_truth == result['text_answer'] and ground_truth == result['code_answer']:\n",
    "            good_responses[idx].append(result['response'])\n",
    "        if ground_truth != result['text_answer'] and result['text_answer'] is not None:\n",
    "            bad_responses[idx].append(result['response'])\n",
    "    # remove repetitions\n",
    "    good_responses[idx] = list(set(good_responses[idx]))\n",
    "    bad_responses[idx] = list(set(bad_responses[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's study the distribution of good and bad responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(580), [len(good_responses[idx]) for idx in range(580)], label='Good responses')\n",
    "plt.bar(np.arange(580), [len(bad_responses[idx]) for idx in range(580)], label='Bad responses', bottom=[len(good_responses[idx]) for idx in range(580)])\n",
    "plt.xlim(-1, 580)\n",
    "plt.legend()\n",
    "plt.xlabel('Question index')\n",
    "plt.ylabel('Number of responses');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_responses(idx):\n",
    "    print('Ground truth:', dataset['answer'][idx])\n",
    "    print('Good response:')\n",
    "    print(random.choice(good_responses[idx]))\n",
    "    print('\\n\\nBad responses:')\n",
    "    print(random.choice(bad_responses[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_responses(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also gather the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prompt_length = 300\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/gbarbadillo/data/deepseekmath')\n",
    "unique_prompts = dict()\n",
    "for idx in range(580):\n",
    "    results = responses[str(idx)]\n",
    "    unique_prompts[idx] = list(set([result['prompt'] for result in results]))\n",
    "    unique_prompts[idx] = [prompt for prompt in unique_prompts[idx] if len(tokenizer.tokenize(prompt)) < max_prompt_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset for DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I have good and bad responses. I have to create pairs of them.\n",
    "\n",
    "For this first version of the dataset I'm going to avoid repetitions. But on future version I could create many more pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen, rejected, prompt, problem_idx = [], [], [], []\n",
    "max_pairs_per_problem = 30\n",
    "for idx in range(580):\n",
    "    n_pairs = min(len(good_responses[idx]), len(bad_responses[idx]), max_pairs_per_problem)\n",
    "    if n_pairs > 0 and unique_prompts[idx]:\n",
    "        chosen.extend(np.random.choice(good_responses[idx], n_pairs, replace=False).tolist())\n",
    "        rejected.extend(np.random.choice(bad_responses[idx], n_pairs, replace=False).tolist())\n",
    "        prompt.extend(np.random.choice(unique_prompts[idx], n_pairs, replace=True).tolist())\n",
    "        problem_idx.extend([idx] * n_pairs)\n",
    "assert len(chosen) == len(rejected) == len(problem_idx)\n",
    "len(chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's gather prompts for the problems, let's reuse the results for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.unique(problem_idx, return_counts=True)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many problems have a lot of good and bad responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'prompt': prompt, 'chosen': chosen, 'rejected': rejected, 'problem_idx': problem_idx})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the max lenght and max prompt length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_prompt_length'] = df['prompt'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "df['chosen_length'] = df['chosen'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "df['rejected_length'] = df['rejected'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "print(f'Max prompt length: {df[\"max_prompt_length\"].max()}')\n",
    "print(f'Max length: {df[\"max_prompt_length\"].max() + max(df[\"chosen_length\"].max(), df[\"rejected_length\"].max())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.problem_idx.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/mnt/hdd0/Kaggle/aimo/external_data/dpo/v0.csv', index=False)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
