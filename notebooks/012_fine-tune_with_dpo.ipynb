{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune model with DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if using DPO can create a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    ")\n",
    "\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cecd1cd6ef04ace98dfa5bfdf23a5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = '/home/gbarbadillo/data/deepseekmath'\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "config.gradient_checkpointing = True\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map='auto',\n",
    "    torch_dtype=\"auto\", #torch.bfloat16 does not show speed differences\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=None,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# TODO: check pad token on prompt recovery notebook\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "      <th>problem_idx</th>\n",
       "      <th>max_prompt_length</th>\n",
       "      <th>chosen_length</th>\n",
       "      <th>rejected_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert mathematical programmer. Sol...</td>\n",
       "      <td>```python\\nfrom itertools import combinations\\...</td>\n",
       "      <td>```python\\nfrom sympy import binomial, symbols...</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>123</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User: John computes the sum of the elements of...</td>\n",
       "      <td>```python\\nfrom sympy import *\\nfrom itertools...</td>\n",
       "      <td>```python\\nfrom sympy import binomial, symbols...</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>188</td>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Below is a math problem you are to solve (non ...</td>\n",
       "      <td>```python\\nfrom sympy import *\\nfrom itertools...</td>\n",
       "      <td>```python\\nfrom sympy import binomial\\n\\ndef s...</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>230</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nUser: John computes the sum of the elements ...</td>\n",
       "      <td>```python\\nfrom itertools import combinations\\...</td>\n",
       "      <td>```python\\nfrom sympy import binomial, Rationa...</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>177</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User: John computes the sum of the elements of...</td>\n",
       "      <td>```python\\nfrom itertools import combinations\\...</td>\n",
       "      <td>```python\\nfrom sympy import *\\n\\ndef sum_of_s...</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>173</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  You are an expert mathematical programmer. Sol...   \n",
       "1  User: John computes the sum of the elements of...   \n",
       "2  Below is a math problem you are to solve (non ...   \n",
       "3  \\nUser: John computes the sum of the elements ...   \n",
       "4  User: John computes the sum of the elements of...   \n",
       "\n",
       "                                              chosen  \\\n",
       "0  ```python\\nfrom itertools import combinations\\...   \n",
       "1  ```python\\nfrom sympy import *\\nfrom itertools...   \n",
       "2  ```python\\nfrom sympy import *\\nfrom itertools...   \n",
       "3  ```python\\nfrom itertools import combinations\\...   \n",
       "4  ```python\\nfrom itertools import combinations\\...   \n",
       "\n",
       "                                            rejected  problem_idx  \\\n",
       "0  ```python\\nfrom sympy import binomial, symbols...            0   \n",
       "1  ```python\\nfrom sympy import binomial, symbols...            0   \n",
       "2  ```python\\nfrom sympy import binomial\\n\\ndef s...            0   \n",
       "3  ```python\\nfrom sympy import binomial, Rationa...            0   \n",
       "4  ```python\\nfrom sympy import *\\n\\ndef sum_of_s...            0   \n",
       "\n",
       "   max_prompt_length  chosen_length  rejected_length  \n",
       "0                144            123              212  \n",
       "1                 96            188              620  \n",
       "2                169            230              198  \n",
       "3                153            177              256  \n",
       "4                 89            173              488  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/mnt/hdd0/Kaggle/aimo/external_data/dpo/v0.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_problem_ids = df['problem_idx'].unique()\n",
    "len(unique_problem_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10154 507\n"
     ]
    }
   ],
   "source": [
    "train_problem_ids = np.random.choice(unique_problem_ids, int(0.95 * len(unique_problem_ids)), replace=False)\n",
    "train_df = df[df['problem_idx'].isin(train_problem_ids)]\n",
    "test_df = df[~df['problem_idx'].isin(train_problem_ids)]\n",
    "assert len(train_df) + len(test_df) == len(df)\n",
    "assert set(train_df['problem_idx'].unique()).intersection(set(test_df['problem_idx'].unique())) == set()\n",
    "print(len(train_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df[['prompt', 'chosen', 'rejected']])\n",
    "eval_dataset = Dataset.from_pandas(test_df[['prompt', 'chosen', 'rejected']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# LoRA config based on https://github.com/ironbar/prompt_recovery/blob/main/notebooks/014_fine-tune_mistral_v2.ipynb\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.05, # 0.1, althought Vaca suggested to use 0.05 for big models\n",
    "        # r: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters\n",
    "        r=16,\n",
    "        bias=\"none\",\n",
    "        # target_modules: The modules (for example, attention blocks) to apply the LoRA update matrices.\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:411: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5700d3c0b5d04606a62aa037ef6419af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63aaabaf636c490796f8e60b41e7c257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://www.philschmid.de/dpo-align-llms-in-2024-with-trl\n",
    "# https://huggingface.co/docs/trl/main/en/dpo_trainer\n",
    "args = DPOConfig(\n",
    "    output_dir=\"/mnt/hdd0/Kaggle/aimo/experiments/18_dpo/01_dpo-first-steps\",               # directory to save and repository id\n",
    "    num_train_epochs=1,                     # number of training epochs\n",
    "    per_device_train_batch_size=2,         # batch size per device during training\n",
    "    gradient_accumulation_steps=8,          # number of steps before performing a backward/update pass\n",
    "    per_device_eval_batch_size=4,           # batch size for evaluation\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    learning_rate=2e-5,                     # 10x higher LR than QLoRA paper\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.1,                       # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"cosine\",             # use cosine learning rate scheduler\n",
    "    logging_steps=10,\n",
    "    save_steps=100,                         # when to save checkpoint\n",
    "    # save_total_limit=2,                     # limit the total amount of checkpoints\n",
    "    eval_strategy=\"steps\",                  # evaluate every n steps\n",
    "    eval_steps=100,                         # when to evaluate\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    push_to_hub=False,                      # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    "    model_init_kwargs=None,\n",
    "    max_length=1024,\n",
    "    max_prompt_length=300,\n",
    "    beta=0.1,                               # same as huggingface documentation, default value\n",
    "    loss_type=\"sigmoid\",                    # default value in huggingface documentation\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None, # set to none since we use peft\n",
    "    peft_config=peft_config,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129ff34533964c60ac0a2cd97aeaa8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6944, 'grad_norm': 3.203125, 'learning_rate': 7.8125e-06, 'rewards/chosen': -0.0004793582484126091, 'rewards/rejected': 4.7817306040087715e-05, 'rewards/accuracies': 0.4424999952316284, 'rewards/margins': -0.0005271749687381089, 'logps/rejected': -91.810302734375, 'logps/chosen': -74.58899688720703, 'logits/rejected': 12.51860237121582, 'logits/chosen': 10.896106719970703, 'epoch': 0.04}\n",
      "{'loss': 0.688, 'grad_norm': 4.15625, 'learning_rate': 1.5625e-05, 'rewards/chosen': -0.022095371037721634, 'rewards/rejected': -0.0355038158595562, 'rewards/accuracies': 0.5475000143051147, 'rewards/margins': 0.013408441096544266, 'logps/rejected': -92.0755844116211, 'logps/chosen': -77.1809310913086, 'logits/rejected': 12.603387832641602, 'logits/chosen': 11.228765487670898, 'epoch': 0.08}\n",
      "{'loss': 0.6682, 'grad_norm': 3.84375, 'learning_rate': 1.9981627325622104e-05, 'rewards/chosen': -0.1730453372001648, 'rewards/rejected': -0.24043823778629303, 'rewards/accuracies': 0.5975000262260437, 'rewards/margins': 0.06739292293787003, 'logps/rejected': -94.7824478149414, 'logps/chosen': -78.8284683227539, 'logits/rejected': 12.853318214416504, 'logits/chosen': 11.422307968139648, 'epoch': 0.12}\n",
      "{'loss': 0.6168, 'grad_norm': 3.90625, 'learning_rate': 1.9803799658748096e-05, 'rewards/chosen': -0.03247715160250664, 'rewards/rejected': -0.22783918678760529, 'rewards/accuracies': 0.6974999904632568, 'rewards/margins': 0.19536204636096954, 'logps/rejected': -90.88417053222656, 'logps/chosen': -74.755859375, 'logits/rejected': 11.853673934936523, 'logits/chosen': 10.706335067749023, 'epoch': 0.16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad788a46908458b8d568779a16d32d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5959993004798889, 'eval_runtime': 388.3928, 'eval_samples_per_second': 1.305, 'eval_steps_per_second': 0.327, 'eval_rewards/chosen': -0.11285519599914551, 'eval_rewards/rejected': -0.39046603441238403, 'eval_rewards/accuracies': 0.7467191815376282, 'eval_rewards/margins': 0.2776108384132385, 'eval_logps/rejected': -103.41634368896484, 'eval_logps/chosen': -81.8982162475586, 'eval_logits/rejected': 10.685792922973633, 'eval_logits/chosen': 9.107816696166992, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/gbarbadillo/data/deepseekmath - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5544, 'grad_norm': 3.875, 'learning_rate': 1.9440132825081107e-05, 'rewards/chosen': 0.09427224844694138, 'rewards/rejected': -0.3348385691642761, 'rewards/accuracies': 0.7749999761581421, 'rewards/margins': 0.4291108250617981, 'logps/rejected': -94.19165802001953, 'logps/chosen': -73.97393798828125, 'logits/rejected': 11.793439865112305, 'logits/chosen': 10.369733810424805, 'epoch': 0.2}\n",
      "{'loss': 0.5003, 'grad_norm': 6.34375, 'learning_rate': 1.8897520431560435e-05, 'rewards/chosen': 0.1144249439239502, 'rewards/rejected': -0.5853338837623596, 'rewards/accuracies': 0.7900000214576721, 'rewards/margins': 0.6997588276863098, 'logps/rejected': -95.39093017578125, 'logps/chosen': -73.02861785888672, 'logits/rejected': 10.666786193847656, 'logits/chosen': 9.123523712158203, 'epoch': 0.24}\n",
      "{'loss': 0.4456, 'grad_norm': 5.5, 'learning_rate': 1.8186248146866928e-05, 'rewards/chosen': 0.109018474817276, 'rewards/rejected': -0.9023540019989014, 'rewards/accuracies': 0.7950000166893005, 'rewards/margins': 1.011372447013855, 'logps/rejected': -99.04525756835938, 'logps/chosen': -75.68180847167969, 'logits/rejected': 9.715717315673828, 'logits/chosen': 8.382851600646973, 'epoch': 0.28}\n",
      "{'loss': 0.4153, 'grad_norm': 4.5, 'learning_rate': 1.7319798728028617e-05, 'rewards/chosen': 0.12796910107135773, 'rewards/rejected': -1.0130966901779175, 'rewards/accuracies': 0.824999988079071, 'rewards/margins': 1.1410657167434692, 'logps/rejected': -96.5269546508789, 'logps/chosen': -73.75686645507812, 'logits/rejected': 9.421883583068848, 'logits/chosen': 7.960771560668945, 'epoch': 0.32}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5dfe363bd3448a808708523ce4ebfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41919684410095215, 'eval_runtime': 386.4056, 'eval_samples_per_second': 1.312, 'eval_steps_per_second': 0.329, 'eval_rewards/chosen': -0.16413454711437225, 'eval_rewards/rejected': -1.4692895412445068, 'eval_rewards/accuracies': 0.8372703790664673, 'eval_rewards/margins': 1.3051550388336182, 'eval_logps/rejected': -114.20457458496094, 'eval_logps/chosen': -82.4110107421875, 'eval_logits/rejected': 8.588448524475098, 'eval_logits/chosen': 6.895272731781006, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/gbarbadillo/data/deepseekmath - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4166, 'grad_norm': 3.046875, 'learning_rate': 1.6314596443560777e-05, 'rewards/chosen': 0.07803937047719955, 'rewards/rejected': -1.2395579814910889, 'rewards/accuracies': 0.8349999785423279, 'rewards/margins': 1.3175971508026123, 'logps/rejected': -104.66545104980469, 'logps/chosen': -75.92522430419922, 'logits/rejected': 9.41341495513916, 'logits/chosen': 7.779532432556152, 'epoch': 0.35}\n",
      "{'loss': 0.4082, 'grad_norm': 5.5625, 'learning_rate': 1.5189695737812153e-05, 'rewards/chosen': 0.1666778177022934, 'rewards/rejected': -1.1636799573898315, 'rewards/accuracies': 0.8149999976158142, 'rewards/margins': 1.3303577899932861, 'logps/rejected': -100.0866470336914, 'logps/chosen': -74.34703063964844, 'logits/rejected': 9.636592864990234, 'logits/chosen': 8.3733491897583, 'epoch': 0.39}\n",
      "{'loss': 0.3531, 'grad_norm': 5.03125, 'learning_rate': 1.3966420038143342e-05, 'rewards/chosen': 0.3202120065689087, 'rewards/rejected': -1.1516671180725098, 'rewards/accuracies': 0.8774999976158142, 'rewards/margins': 1.471879243850708, 'logps/rejected': -97.96879577636719, 'logps/chosen': -70.57746124267578, 'logits/rejected': 10.007938385009766, 'logits/chosen': 8.494813919067383, 'epoch': 0.43}\n",
      "{'loss': 0.3156, 'grad_norm': 5.03125, 'learning_rate': 1.2667957551647263e-05, 'rewards/chosen': 0.1998811662197113, 'rewards/rejected': -1.517208456993103, 'rewards/accuracies': 0.8899999856948853, 'rewards/margins': 1.7170895338058472, 'logps/rejected': -106.20459747314453, 'logps/chosen': -73.92239379882812, 'logits/rejected': 9.942481994628906, 'logits/chosen': 8.43991756439209, 'epoch': 0.47}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1a9f851a0245148342e6a15442c8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4208602011203766, 'eval_runtime': 387.3897, 'eval_samples_per_second': 1.309, 'eval_steps_per_second': 0.328, 'eval_rewards/chosen': -0.5477638840675354, 'eval_rewards/rejected': -1.8813204765319824, 'eval_rewards/accuracies': 0.8175853490829468, 'eval_rewards/margins': 1.3335566520690918, 'eval_logps/rejected': -118.32488250732422, 'eval_logps/chosen': -86.24730682373047, 'eval_logits/rejected': 8.237112998962402, 'eval_logits/chosen': 6.639263153076172, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/gbarbadillo/data/deepseekmath - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2871, 'grad_norm': 6.90625, 'learning_rate': 1.1318921713420691e-05, 'rewards/chosen': 0.061236314475536346, 'rewards/rejected': -1.8166781663894653, 'rewards/accuracies': 0.8999999761581421, 'rewards/margins': 1.8779144287109375, 'logps/rejected': -106.7466049194336, 'logps/chosen': -72.7172622680664, 'logits/rejected': 8.892027854919434, 'logits/chosen': 7.227189540863037, 'epoch': 0.51}\n",
      "{'loss': 0.2721, 'grad_norm': 2.28125, 'learning_rate': 9.944884618454996e-06, 'rewards/chosen': 0.08680646866559982, 'rewards/rejected': -2.102017402648926, 'rewards/accuracies': 0.9150000214576721, 'rewards/margins': 2.188823699951172, 'logps/rejected': -109.77825927734375, 'logps/chosen': -73.68531036376953, 'logits/rejected': 8.373934745788574, 'logits/chosen': 6.708476543426514, 'epoch': 0.55}\n",
      "{'loss': 0.2434, 'grad_norm': 3.8125, 'learning_rate': 8.571892281332213e-06, 'rewards/chosen': 0.002565858419984579, 'rewards/rejected': -2.312694787979126, 'rewards/accuracies': 0.9150000214576721, 'rewards/margins': 2.315260648727417, 'logps/rejected': -115.22782897949219, 'logps/chosen': -73.98109436035156, 'logits/rejected': 8.491092681884766, 'logits/chosen': 6.593437671661377, 'epoch': 0.59}\n",
      "{'loss': 0.2983, 'grad_norm': 6.5, 'learning_rate': 7.225970912381557e-06, 'rewards/chosen': -0.3146754801273346, 'rewards/rejected': -2.5563158988952637, 'rewards/accuracies': 0.8525000214576721, 'rewards/margins': 2.241640567779541, 'logps/rejected': -115.30354309082031, 'logps/chosen': -79.06669616699219, 'logits/rejected': 7.2255778312683105, 'logits/chosen': 5.837727069854736, 'epoch': 0.63}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a77cb4f27094ca7ad4728454390d43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4043298661708832, 'eval_runtime': 394.5809, 'eval_samples_per_second': 1.285, 'eval_steps_per_second': 0.322, 'eval_rewards/chosen': -1.1567224264144897, 'eval_rewards/rejected': -2.899566411972046, 'eval_rewards/accuracies': 0.8175853490829468, 'eval_rewards/margins': 1.7428438663482666, 'eval_logps/rejected': -128.50735473632812, 'eval_logps/chosen': -92.3368911743164, 'eval_logits/rejected': 6.802318572998047, 'eval_logits/chosen': 5.2856645584106445, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/gbarbadillo/data/deepseekmath - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2285, 'grad_norm': 6.875, 'learning_rate': 5.932633569242e-06, 'rewards/chosen': -0.3080903887748718, 'rewards/rejected': -2.7032525539398193, 'rewards/accuracies': 0.9150000214576721, 'rewards/margins': 2.395162343978882, 'logps/rejected': -117.1850814819336, 'logps/chosen': -77.2620620727539, 'logits/rejected': 7.743144989013672, 'logits/chosen': 6.120285511016846, 'epoch': 0.67}\n",
      "{'loss': 0.2886, 'grad_norm': 5.625, 'learning_rate': 4.716396535660412e-06, 'rewards/chosen': -0.5579458475112915, 'rewards/rejected': -2.9327666759490967, 'rewards/accuracies': 0.8974999785423279, 'rewards/margins': 2.3748204708099365, 'logps/rejected': -123.11552429199219, 'logps/chosen': -80.95430755615234, 'logits/rejected': 7.356427192687988, 'logits/chosen': 5.726658821105957, 'epoch': 0.71}\n",
      "{'loss': 0.2415, 'grad_norm': 4.15625, 'learning_rate': 3.6003145949668338e-06, 'rewards/chosen': -0.3022012412548065, 'rewards/rejected': -2.789123773574829, 'rewards/accuracies': 0.9100000262260437, 'rewards/margins': 2.4869225025177, 'logps/rejected': -115.51618194580078, 'logps/chosen': -76.51903533935547, 'logits/rejected': 7.588232517242432, 'logits/chosen': 5.654693603515625, 'epoch': 0.75}\n",
      "{'loss': 0.2792, 'grad_norm': 6.59375, 'learning_rate': 2.6055440075042793e-06, 'rewards/chosen': -0.43373245000839233, 'rewards/rejected': -2.7296664714813232, 'rewards/accuracies': 0.9049999713897705, 'rewards/margins': 2.295933723449707, 'logps/rejected': -119.78642272949219, 'logps/chosen': -80.16027069091797, 'logits/rejected': 7.554419994354248, 'logits/chosen': 6.052490234375, 'epoch': 0.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277ad2d6dca2498aaa0d1a4fbb2db0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39460909366607666, 'eval_runtime': 374.1204, 'eval_samples_per_second': 1.355, 'eval_steps_per_second': 0.339, 'eval_rewards/chosen': -1.1515700817108154, 'eval_rewards/rejected': -2.9324898719787598, 'eval_rewards/accuracies': 0.8097112774848938, 'eval_rewards/margins': 1.7809196710586548, 'eval_logps/rejected': -128.83657836914062, 'eval_logps/chosen': -92.28536224365234, 'eval_logits/rejected': 6.485034465789795, 'eval_logits/chosen': 5.000820636749268, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/gbarbadillo/data/deepseekmath - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.291, 'grad_norm': 7.5625, 'learning_rate': 1.7509414761388854e-06, 'rewards/chosen': -0.33489105105400085, 'rewards/rejected': -2.559382677078247, 'rewards/accuracies': 0.887499988079071, 'rewards/margins': 2.224491834640503, 'logps/rejected': -117.00155639648438, 'logps/chosen': -78.22147369384766, 'logits/rejected': 7.056517124176025, 'logits/chosen': 5.574831962585449, 'epoch': 0.83}\n",
      "{'loss': 0.24, 'grad_norm': 4.78125, 'learning_rate': 1.0527067017923654e-06, 'rewards/chosen': -0.25393348932266235, 'rewards/rejected': -2.6048943996429443, 'rewards/accuracies': 0.9075000286102295, 'rewards/margins': 2.3509607315063477, 'logps/rejected': -112.43512725830078, 'logps/chosen': -76.33489990234375, 'logits/rejected': 7.386599063873291, 'logits/chosen': 5.990445613861084, 'epoch': 0.87}\n",
      "{'loss': 0.2424, 'grad_norm': 3.09375, 'learning_rate': 5.240753046535396e-07, 'rewards/chosen': -0.3071029782295227, 'rewards/rejected': -2.753527879714966, 'rewards/accuracies': 0.8999999761581421, 'rewards/margins': 2.446424722671509, 'logps/rejected': -113.71314239501953, 'logps/chosen': -77.83509826660156, 'logits/rejected': 7.556097507476807, 'logits/chosen': 6.069302558898926, 'epoch': 0.91}\n",
      "{'loss': 0.2731, 'grad_norm': 5.25, 'learning_rate': 1.7506793200248507e-07, 'rewards/chosen': -0.2851887047290802, 'rewards/rejected': -2.776096820831299, 'rewards/accuracies': 0.9125000238418579, 'rewards/margins': 2.490908145904541, 'logps/rejected': -118.82655334472656, 'logps/chosen': -79.3986587524414, 'logits/rejected': 7.4509077072143555, 'logits/chosen': 5.734309673309326, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0d14e8870f4aa2b8c736d536be4931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3919623792171478, 'eval_runtime': 374.6222, 'eval_samples_per_second': 1.353, 'eval_steps_per_second': 0.339, 'eval_rewards/chosen': -1.1435599327087402, 'eval_rewards/rejected': -2.9309442043304443, 'eval_rewards/accuracies': 0.8156168460845947, 'eval_rewards/margins': 1.787384033203125, 'eval_logps/rejected': -128.8211212158203, 'eval_logps/chosen': -92.20526123046875, 'eval_logits/rejected': 6.475827217102051, 'eval_logits/chosen': 4.985001087188721, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /home/gbarbadillo/data/deepseekmath - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gbarbadillo/miniconda3/envs/aimo/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2465, 'grad_norm': 2.546875, 'learning_rate': 1.230030851695263e-08, 'rewards/chosen': -0.30363237857818604, 'rewards/rejected': -2.697334051132202, 'rewards/accuracies': 0.9225000143051147, 'rewards/margins': 2.3937015533447266, 'logps/rejected': -119.35763549804688, 'logps/chosen': -77.18180847167969, 'logits/rejected': 7.547821521759033, 'logits/chosen': 5.730653285980225, 'epoch': 0.98}\n",
      "{'train_runtime': 17792.9492, 'train_samples_per_second': 0.571, 'train_steps_per_second': 0.036, 'train_loss': 0.3783860379589093, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=634, training_loss=0.3783860379589093, metrics={'train_runtime': 17792.9492, 'train_samples_per_second': 0.571, 'train_steps_per_second': 0.036, 'total_flos': 0.0, 'train_loss': 0.3783860379589093, 'epoch': 0.9990151664368722})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Count the tokens in the train set, max length and max prompt length\n",
    "- [x] Do the training parameters from the example have sense?\n",
    "- [ ] Shuffle the data?\n",
    "- [ ] Verify that the model generates text correctly, on the prompt recovery challenge after fine-tuning the model forgot to use the eos token."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
