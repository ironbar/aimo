{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Zero-shot MMOS-DeepSeekMath-7B with self-consistency and generated code reasoning evaluation\n","\n","Self-consistency is a modification of the standard greedy decoding in reasoning pipelines via sampling several diverse answers followed by aggregation, e.g., most common answer ([SC-CoT paper](https://arxiv.org/pdf/2203.11171.pdf)).\n","\n","In this kernel, we will consider MMOS-DeepSeekMath-7B RL-tuned backbone; in my experiments, this model produces more consistent code reasoning and the code block execution will allow us to decrease arithmetic hallucinations."]},{"cell_type":"markdown","metadata":{},"source":["## References"]},{"cell_type":"markdown","metadata":{},"source":["- https://www.kaggle.com/code/ironbar/autobots-roll-out/notebook\n","- https://www.kaggle.com/code/abdurrafae/improved-code-interpretation\n","- https://kaggle.com/code/xiaoz259/pure-rng/notebook\n","- https://www.kaggle.com/code/olyatsimboy/aimo-openmath-mistral-baseline\n","- https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama\n","- https://www.kaggle.com/code/thedrcat/aimo-mixtral-baseline"]},{"cell_type":"markdown","metadata":{},"source":["## Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:19.753064Z","iopub.status.busy":"2024-06-03T15:36:19.752261Z","iopub.status.idle":"2024-06-03T15:36:19.768025Z","shell.execute_reply":"2024-06-03T15:36:19.766868Z","shell.execute_reply.started":"2024-06-03T15:36:19.753031Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    # Data parameters\n","    quick_save = False # If true it will set the time limit to 1 so the saving of the notebook is really quick\n","    submission_mode = False # If True it will use aimo.env otherwise a mock environment\n","    ## Data parameters only used when submission_mode is False\n","    # dataset = '/kaggle/input/ai-mathematical-olympiad-prize/train.csv'\n","    # dataset = '/kaggle/input/filtered-math/filtered_MATH_test_5.csv' # filepath, ignored if submission_mode is True\n","    dataset = '/mnt/hdd0/Kaggle/aimo/external_data/filtered_MATH_test_5.csv'\n","    problem_indices = None # list(range(580))[::11] # If not None will restrict the evaluation to the given problem idx of the dataset\n","    # Model parameters\n","    # model_path = \"/kaggle/input/deepseek-math\"\n","    model_path = \"/home/gbarbadillo/data/deepseekmath\"\n","    use_4bit_quantization = False\n","    balanced_device_map = True\n","    cuda_visible_devices = 0 # If not None could be used to limit the use of GPUS\n","    context_window_size = 4096\n","    # Run parameters\n","    time_limit = 31500 # seconds, 31500 by default which is 8.75 hours\n","    verbose = True\n","    save_results = True\n","    result_priority = ['code_answer', 'text_answer'] #['code_answer', 'boxed_answer', 'text_answer'] # Select which answers will be used as result\n","    # few-shot parameters\n","    # few_shot_dataset = '/kaggle/input/filtered-math/AIMO_train_with_solutions.csv'\n","    few_shot_dataset = '/mnt/hdd0/Kaggle/aimo/data/AIMO_train_with_solutions.csv'\n","    few_shot_samples = 2\n","    max_sample_tokens = 512 # problems with more than this tokens will be filtered\n","    max_prompt_tokens = 1024 # 3072 # only prompts with less than this tokens will be used\n","    difficulty_levels = None # levels outside this range won't be used\n","    # Inference parameters\n","    confidence_level = 0.95 # this will be used to stop sampling solutions if the difference between the first and second most voted options is significative\n","    n_repetitions = 1\n","    random_seed = None # None or int\n","    max_new_tokens = 640 #2048\n","    max_coding_errors = 2\n","    code_output_truncate_length = 125 # max number of output parameters\n","    default_answer = 0 # this will be the response when the system does not have a valid answer\n","    stop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\", 'Problem:', 'User:']\n","    # https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api/172683\n","    # temperature for text generation\n","    temperature_text = 0.9\n","    top_p_text = 0.9\n","    # temperature for coding generation\n","    temperature_code = 0.9\n","    top_p_code = 0.9"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:19.770370Z","iopub.status.busy":"2024-06-03T15:36:19.769996Z","iopub.status.idle":"2024-06-03T15:36:19.781951Z","shell.execute_reply":"2024-06-03T15:36:19.781248Z","shell.execute_reply.started":"2024-06-03T15:36:19.770338Z"},"trusted":true},"outputs":[],"source":["import logging\n","\n","for handler in logging.root.handlers[:]:\n","    logging.root.removeHandler(handler)\n","logging.basicConfig(level=logging.INFO,\n","                    format='%(asctime)s - %(levelname)s - %(message)s')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:19.783361Z","iopub.status.busy":"2024-06-03T15:36:19.783013Z","iopub.status.idle":"2024-06-03T15:36:27.340792Z","shell.execute_reply":"2024-06-03T15:36:27.339799Z","shell.execute_reply.started":"2024-06-03T15:36:19.783319Z"},"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import time\n","NOTEBOOK_START_TIME = time.time()\n","\n","if CFG.use_4bit_quantization:\n","    !pip install -U /kaggle/input/accelerate-wheelwhl/accelerate-0.29.1-py3-none-any.whl -qq\n","    !pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq\n","\n","import os\n","if CFG.cuda_visible_devices is not None:\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(CFG.cuda_visible_devices)\n","    \n","import sys\n","import subprocess\n","from IPython.display import display, Markdown\n","import pandas as pd\n","from tqdm.auto import tqdm\n","import torch\n","import gc\n","import re\n","import math\n","import random\n","import json\n","from collections import Counter\n","import numpy as np\n","import tempfile\n","from pydantic import BaseModel\n","from typing import Optional\n","import datetime\n","from scipy.stats import norm\n","\n","# https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.enable_mem_efficient_sdp\n","# Enables or disables memory efficient scaled dot product attention.\n","# If set to True I get this error: RuntimeError: cutlassF: no kernel found to launch!\n","if any(gpu in torch.cuda.get_device_properties(0).name for gpu in ['P100', 'T4']): # 'NVIDIA GeForce RTX 3090', 'Tesla P100-PCIE-16GB', 'Tesla T4'\n","    logging.info('Disabling torch cuda mem efficient sdp')\n","    torch.backends.cuda.enable_mem_efficient_sdp(False)\n","\n","from transformers import (\n","    AutoModelForCausalLM, \n","    AutoTokenizer, \n","    AutoConfig,\n","    StoppingCriteria,\n","    StoppingCriteriaList,\n","    set_seed\n",")\n","\n","import transformers\n","print(f\"Transformers Version: {transformers.__version__}\")\n","if CFG.random_seed is not None:\n","    set_seed(CFG.random_seed)\n","\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","from openai import OpenAI\n","import threading\n","import subprocess\n","import requests\n","\n","plt.plot()\n","plt.close('all')\n","plt.rcParams[\"figure.figsize\"] = (20, 5)\n","mpl.rcParams['lines.linewidth'] = 3\n","mpl.rcParams['font.size'] = 16\n","logging.info('Imported all libraries.')"]},{"cell_type":"markdown","metadata":{},"source":["## Code"]},{"cell_type":"markdown","metadata":{},"source":["### Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:27.352218Z","iopub.status.busy":"2024-06-03T15:36:27.351942Z","iopub.status.idle":"2024-06-03T15:36:27.361233Z","shell.execute_reply":"2024-06-03T15:36:27.360422Z","shell.execute_reply.started":"2024-06-03T15:36:27.352196Z"},"trusted":true},"outputs":[],"source":["class MockEnvWithDataframe:\n","    \"\"\"\n","    This class has the same interface as aimo.env, thus you can reuse the same code\n","    for making submissions or evaluating other datasets\n","    \"\"\"\n","    def __init__(self, df):\n","        \"\"\"\n","        Initializes the mock environment with a dataframe containing problems.\n","        \"\"\"\n","        self.df = df\n","        self.submissions = []\n","\n","    def iter_test(self):\n","        \"\"\"\n","        Simulates the iter_test function by yielding each problem with an accompanying sample_submission.\n","        \"\"\"\n","        for _, row in self.df.iterrows():\n","            problem = pd.DataFrame([row])\n","            sample_submission = pd.DataFrame({'id': problem.id, 'answer': [None]})\n","            yield problem, sample_submission\n","\n","    def predict(self, sample_submission):\n","        self.submissions.append(sample_submission)\n","        \n","    def get_all_submissions(self):\n","        return pd.concat(self.submissions)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:27.363023Z","iopub.status.busy":"2024-06-03T15:36:27.362392Z","iopub.status.idle":"2024-06-03T15:36:27.397112Z","shell.execute_reply":"2024-06-03T15:36:27.396399Z","shell.execute_reply.started":"2024-06-03T15:36:27.362999Z"},"trusted":true},"outputs":[],"source":["if CFG.submission_mode:\n","    import aimo\n","    env = aimo.make_env()\n","    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n","        N_PROBLEMS = 50\n","    else:\n","        N_PROBLEMS = 3\n","else:\n","    df = pd.read_csv(CFG.dataset)\n","    if CFG.problem_indices is not None:\n","        df = df.iloc[CFG.problem_indices].reset_index(drop=True)\n","    if 'answer' in df.columns:\n","        df['ground_truth'] = df['answer']\n","    elif CFG.dataset == '/kaggle/input/ai-mathematical-olympiad-prize/test.csv': \n","        df['ground_truth'] = 0\n","    N_PROBLEMS = len(df)\n","    display(df)\n","    env = MockEnvWithDataframe(df)\n","iter_test = env.iter_test()"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:27.406228Z","iopub.status.busy":"2024-06-03T15:36:27.405924Z","iopub.status.idle":"2024-06-03T15:36:27.419355Z","shell.execute_reply":"2024-06-03T15:36:27.418485Z","shell.execute_reply.started":"2024-06-03T15:36:27.406206Z"},"trusted":true},"outputs":[],"source":["def get_tokenizer(model_path):\n","    tokenizer = AutoTokenizer.from_pretrained(model_path)\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","    return tokenizer\n","\n","tokenizer = get_tokenizer(CFG.model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:27.763482Z","iopub.status.busy":"2024-06-03T15:36:27.763198Z","iopub.status.idle":"2024-06-03T15:36:27.780526Z","shell.execute_reply":"2024-06-03T15:36:27.779617Z","shell.execute_reply.started":"2024-06-03T15:36:27.763457Z"},"trusted":true},"outputs":[],"source":["class TextGenerator():\n","    \"\"\"\n","    Abstraction that allows to generate text and code in different steps efficiently\n","    \"\"\"\n","    def __init__(self, cfg):\n","        self.cfg = cfg\n","        self.reset()\n","\n","    def reset(self):\n","        self.prompt_tokens = 0\n","        self.generated_tokens = 0\n","        self.past_key_values = None\n","        self.set_generation_mode('text')\n","        self.max_new_tokens = self.cfg.max_new_tokens\n","\n","    def set_generation_mode(self, mode):\n","        if mode == 'text':\n","            self.set_sampling_parameters(self.cfg.temperature_text, self.cfg.top_p_text)\n","        elif mode == 'code':\n","            self.set_sampling_parameters(self.cfg.temperature_code, self.cfg.top_p_code)\n","        else:\n","            raise KeyError(mode)\n","\n","    def set_sampling_parameters(self, temperature, top_p):\n","        if temperature == 0:\n","            self.sampling_parameters = dict(do_sample=False)\n","        else:\n","            self.sampling_parameters = dict(do_sample=True, temperature=temperature, top_p=top_p)\n","\n","    def are_generation_tokens_available(self):\n","        return self.generated_tokens < self.max_new_tokens\n","\n","    def verify_max_new_tokens(self):\n","        if self.max_new_tokens > self.cfg.context_window_size - self.prompt_tokens:\n","            self.max_new_tokens = self.cfg.context_window_size - self.prompt_tokens\n","            logging.warning(f'Reducing max_new_tokens to {self.max_new_tokens} to avoid exceeding the context window of {self.cfg.context_window_size}')\n","\n","    def generate(self, prompt, mode='text'):\n","        self.set_generation_mode(mode)\n","        model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","        if self.prompt_tokens == 0:\n","            self.prompt_tokens = len(model_inputs['input_ids'][0])\n","            logging.info(f'Prompt has {self.prompt_tokens} tokens.')\n","            self.verify_max_new_tokens()\n","        self.generated_tokens = len(model_inputs['input_ids'][0]) - self.prompt_tokens\n","        if not self.are_generation_tokens_available():\n","            logging.warning(f'Input text exceeded the available generation tokens. This is likely happening because a big code output.')\n","            return prompt\n","\n","        t0 = time.time()\n","        clear_memory()\n","        optional_past_key_values = dict(past_key_values=self.past_key_values) if self.past_key_values is not None else {}\n","        generation_output = model.generate(\n","            **model_inputs,\n","            max_new_tokens=self.max_new_tokens - self.generated_tokens,\n","            return_dict_in_generate=True,\n","            num_return_sequences=1,\n","            stopping_criteria=stopping_criteria,\n","            pad_token_id=tokenizer.eos_token_id,\n","            **self.sampling_parameters,\n","            **optional_past_key_values,\n","            )\n","        output_ids = generation_output.sequences[0]\n","        newly_generated_tokens = len(output_ids) - len(model_inputs['input_ids'][0])\n","        self.generated_tokens = len(output_ids) - self.prompt_tokens\n","        logging.info(f'Generating {mode} speed: {newly_generated_tokens/(time.time() - t0):.1f} tokens/s ({newly_generated_tokens}) ({self.generated_tokens}/{self.max_new_tokens})')\n","        self.past_key_values = generation_output.past_key_values\n","        decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","        return decoded_output\n","\n","    def __call__(self, prompt, mode):\n","        return self.generate(prompt, mode)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TextGeneratorVLLM(TextGenerator):\n","    def __init__(self, cfg):\n","        super().__init__(cfg)\n","        self.reset()\n","        self.client = OpenAI(api_key='EMPTY', base_url=\"http://localhost:8000/v1\")\n","\n","    def set_sampling_parameters(self, temperature, top_p):\n","        self.sampling_parameters = dict(temperature=temperature, top_p=top_p)\n","\n","    def generate(self, prompt, mode='text'):\n","        self.set_generation_mode(mode)\n","        if not self.are_generation_tokens_available():\n","            logging.warning(f'Input text exceeded the available generation tokens. This is likely happening because a big code output.')\n","            return prompt\n","\n","        t0 = time.time()\n","        clear_memory()\n","        completion = self.client.completions.create(\n","            model=self.cfg.model_path,\n","            prompt=prompt,\n","            max_tokens=self.max_new_tokens - self.generated_tokens,\n","            **self.sampling_parameters,\n","            echo=True,\n","            stop=self.cfg.stop_words,\n","        )\n","        decoded_output = completion.choices[0].text\n","        if completion.choices[0].finish_reason == 'stop' and completion.choices[0].stop_reason is not None:\n","            decoded_output += completion.choices[0].stop_reason\n","\n","        if self.prompt_tokens == 0:\n","            logging.info(f'Prompt has {self.prompt_tokens} tokens.')\n","            self.prompt_tokens = completion.usage.prompt_tokens\n","            self.verify_max_new_tokens()\n","        newly_generated_tokens = completion.usage.completion_tokens\n","        self.generated_tokens = completion.usage.total_tokens - self.prompt_tokens\n","        logging.info(f'Generating {mode} speed: {newly_generated_tokens/(time.time() - t0):.1f} tokens/s ({newly_generated_tokens}) ({self.generated_tokens}/{self.max_new_tokens})')\n","        return decoded_output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:27.783666Z","iopub.status.busy":"2024-06-03T15:36:27.783382Z","iopub.status.idle":"2024-06-03T15:36:27.796802Z","shell.execute_reply":"2024-06-03T15:36:27.796001Z","shell.execute_reply.started":"2024-06-03T15:36:27.783644Z"},"trusted":true},"outputs":[],"source":["def log_gpu_memory():\n","    for device in range(torch.cuda.device_count()):\n","        logging.info(f'GPU {device} memory allocated: {torch.cuda.memory_allocated(device)/1024**3:.1f} GB, max memory allocated: {torch.cuda.max_memory_allocated(device)/1024**3:.1f} GB')\n","\n","def empty_gpu_vram():\n","    logging.info('Emptying GPU VRAM...')\n","    global tokenizer\n","    variables_to_delete = ['tokenizer']\n","    for variable_name in variables_to_delete:\n","        if variable_name in globals():\n","            del globals()[variable_name]\n","    gc.collect()\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    log_gpu_memory()\n","\n","log_gpu_memory()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:27.798671Z","iopub.status.busy":"2024-06-03T15:36:27.797853Z","iopub.status.idle":"2024-06-03T15:36:27.808932Z","shell.execute_reply":"2024-06-03T15:36:27.808052Z","shell.execute_reply.started":"2024-06-03T15:36:27.798640Z"},"trusted":true},"outputs":[],"source":["def run_vllm_server():\n","    # https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n","    subprocess.run(['python', '-m', 'vllm.entrypoints.openai.api_server',\n","                    '--model', CFG.model_path,\n","                    '--uvicorn-log-level', 'error', # this is not working as intended\n","                    ])\n","\n","\n","def is_server_running(server_url):\n","    try:\n","        response = requests.get(server_url)\n","        return response.status_code == 200\n","    except requests.exceptions.ConnectionError:\n","        return False\n","\n","\n","def wait_for_server(server_url):\n","    while not is_server_running(server_url):\n","        time.sleep(1)\n","    logging.info(f\"Server is running at {server_url}!\")\n","\n","\n","def create_model_and_inference_artifacts():\n","    global server_thread\n","    if 'server_thread' in globals():\n","        return\n","\n","    server_thread = threading.Thread(target=run_vllm_server)\n","    logging.info('Starting VLLM server...')\n","    server_thread.start()\n","    wait_for_server(\"http://localhost:8000/v1/models\")\n","    log_gpu_memory()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:27.810765Z","iopub.status.busy":"2024-06-03T15:36:27.810042Z","iopub.status.idle":"2024-06-03T15:36:27.935385Z","shell.execute_reply":"2024-06-03T15:36:27.934448Z","shell.execute_reply.started":"2024-06-03T15:36:27.810741Z"},"trusted":true},"outputs":[],"source":["def is_gpu_memory_available(required_memory=14):\n","    for device in range(torch.cuda.device_count()):\n","        available_memory = torch.cuda.mem_get_info(device)[0]/1024**3\n","        logging.info(f'Available memory on GPU {device} is {available_memory:.1f} GB')\n","        if available_memory < required_memory:\n","            return False\n","    return True\n","\n","def wait_for_gpu_memory(wait_time=60, required_memory=14):\n","    while not is_gpu_memory_available(required_memory):\n","        logging.info(f'Waiting for GPU memory to be available...')\n","        time.sleep(wait_time)\n","    logging.info(f'GPU memory is available. Let\\'s go training!')\n","\n","is_gpu_memory_available()"]},{"cell_type":"markdown","metadata":{},"source":["### Prompts"]},{"cell_type":"markdown","metadata":{},"source":["#### Define problems"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:27.937161Z","iopub.status.busy":"2024-06-03T15:36:27.936782Z","iopub.status.idle":"2024-06-03T15:36:27.977023Z","shell.execute_reply":"2024-06-03T15:36:27.976156Z","shell.execute_reply.started":"2024-06-03T15:36:27.937128Z"},"trusted":true},"outputs":[],"source":["prompts_df = pd.read_csv(CFG.few_shot_dataset)\n","prompts_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:27.978476Z","iopub.status.busy":"2024-06-03T15:36:27.978170Z","iopub.status.idle":"2024-06-03T15:36:27.988525Z","shell.execute_reply":"2024-06-03T15:36:27.987414Z","shell.execute_reply.started":"2024-06-03T15:36:27.978451Z"},"trusted":true},"outputs":[],"source":["logging.info(f'The number of problems for few-shot prompting is {len(prompts_df)} previous to filtering')\n","logging.info(f'Filtering problems longer than {CFG.max_sample_tokens} tokens and outside levels {CFG.difficulty_levels}')\n","prompts_df = prompts_df[prompts_df.total_tokens < CFG.max_sample_tokens]\n","if CFG.difficulty_levels is not None:\n","    prompts_df = prompts_df[prompts_df.level.isin([f'Level {i}' for i in CFG.difficulty_levels])]\n","prompts_df.reset_index(drop=True, inplace=True)\n","logging.info(f'The number of problems for few-shot prompting is {len(prompts_df)} after filtering')"]},{"cell_type":"markdown","metadata":{},"source":["#### Create few shot prompts"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:27.990043Z","iopub.status.busy":"2024-06-03T15:36:27.989741Z","iopub.status.idle":"2024-06-03T15:36:28.012070Z","shell.execute_reply":"2024-06-03T15:36:28.011252Z","shell.execute_reply.started":"2024-06-03T15:36:27.990017Z"},"trusted":true},"outputs":[],"source":["# https://github.com/deepseek-ai/DeepSeek-Math/tree/main\n","few_shot_prompt_templates = [\n","\"\"\"\n","User: QUESTION_PLACEHOLDER\n","Please integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\n","\n","Assistant: Sure, we can solve the problem by writing a Python script.\n","\n","ANSWER_PLACEHOLDER\n","\"\"\",\n","\"\"\"\n","User: QUESTION_PLACEHOLDER\n","Please reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\n","\n","Assistant: Sure, we can solve the problem by writing a Python program.\n","\n","ANSWER_PLACEHOLDER\n","\"\"\",\n","\"\"\"\n","Problem:\n","\n","QUESTION_PLACEHOLDER\n","\n","You are an expert mathematical programmer. Solve the above mathematical problem by writing a Python program.\n","Express your answer as a numeric type or a SymPy object. The answer must be an integer greater or equal to zero.\n","Please reason step by step, and always end with \"The answer is $\\\\boxed{}$\".\n","\n","ANSWER_PLACEHOLDER\n","\"\"\"\n","]\n","\n","\n","def create_random_few_shot_prompt(n=CFG.few_shot_samples, template_idx=0):\n","    prompt_template = few_shot_prompt_templates[template_idx % len(few_shot_prompt_templates)]\n","    prompt = ''\n","    problem_indices = np.random.choice(np.arange(len(prompts_df)), n, replace=False)\n","    for problem_idx in problem_indices:\n","        row = prompts_df.loc[problem_idx]\n","        prompt += prompt_template.replace('QUESTION_PLACEHOLDER', row['problem']).replace('ANSWER_PLACEHOLDER', row['solution'])\n","        #prompt += f'\\nThe final answer is $\\\\boxed{{{row[\"answer\"]}}}$\\n'\n","    prompt += prompt_template.replace('QUESTION_PLACEHOLDER', 'PROBLEM_PLACEHOLDER').replace('ANSWER_PLACEHOLDER', '```python')\n","    return prompt.strip()\n","\n","def create_random_few_shot_prompt_with_token_limit(token_limit=CFG.max_prompt_tokens, template_idx=0):\n","    while 1:\n","        prompt = create_random_few_shot_prompt(template_idx=template_idx)\n","        if len(tokenizer.tokenize(prompt)) < token_limit:\n","            return prompt\n","\n","\n","prompt = create_random_few_shot_prompt_with_token_limit()\n","print(f'Number of tokens in prompt: {len(tokenizer.tokenize(prompt))}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:28.013324Z","iopub.status.busy":"2024-06-03T15:36:28.013045Z","iopub.status.idle":"2024-06-03T15:36:28.083965Z","shell.execute_reply":"2024-06-03T15:36:28.083146Z","shell.execute_reply.started":"2024-06-03T15:36:28.013289Z"},"trusted":true},"outputs":[],"source":["%%time\n","print('Create some random prompts to see token length distribution')\n","[len(tokenizer.tokenize(create_random_few_shot_prompt_with_token_limit())) for _ in range(10)]"]},{"cell_type":"markdown","metadata":{},"source":["#### Simple prompts"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:28.094392Z","iopub.status.busy":"2024-06-03T15:36:28.094058Z","iopub.status.idle":"2024-06-03T15:36:28.102990Z","shell.execute_reply":"2024-06-03T15:36:28.102237Z","shell.execute_reply.started":"2024-06-03T15:36:28.094359Z"},"trusted":true},"outputs":[],"source":["prompt_options = [\n","\"\"\"Below is a math problem you are to solve (non negative integer answer):\n","\n","\\\"PROBLEM_PLACEHOLDER\\\"\n","\n","To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be a non negative integer, not an algebraic expression!\n","Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n","\n","Approach:\n","\n","```python\"\"\",\n","\"\"\"Below is a math problem you are to solve (non negative integer answer):\n","\n","\\\"PROBLEM_PLACEHOLDER\\\"\n","\n","Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\n","\n","```python\"\"\",\n","\"\"\"User: PROBLEM_PLACEHOLDER\n","Please reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\n","\n","Assistant: Sure, we can solve the problem by writing a Python script.\n","\n","```python\"\"\",\n","\"\"\"User: PROBLEM_PLACEHOLDER\n","Please integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\n","\n","Assistant: Sure, we can solve the problem by writing a Python program.\n","\n","```python\"\"\",\n","\"\"\"You are an expert mathematical programmer. Solve the mathematical problem below by writing a Python program.\n","\n","- Express your answer as a numeric type or a sympy object. The answer must be an integer greater or equal to zero.\n","- Please reason step by step, and write clean and readable code.\n","- You can use python libraries such as sympy, math or numpy to solve the problem.\n","- Finally always end with \"The answer is $\\\\boxed{}$\".\n","\n","PROBLEM_PLACEHOLDER\n","\n","```python\"\"\",\n","\"\"\"\n","User: PROBLEM_PLACEHOLDER\n","Please reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\n","Use all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\n","You might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\n","Use code even for the simpler calculations to avoid mistakes.\n","\n","Assistant: Sure, we can solve the problem by writing a Python program.\n","\n","```python\"\"\",\n","]\n","\n","print(len(prompt_options))"]},{"cell_type":"markdown","metadata":{},"source":["#### Prompt examples"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_formatted_prompt(problem, repetition_idx):\n","    prompt_idx = (repetition_idx + len(problem))% (len(prompt_options) + len(few_shot_prompt_templates))\n","    if prompt_idx < len(prompt_options):\n","        prompt = prompt_options[prompt_idx]\n","    else:\n","        prompt = create_random_few_shot_prompt_with_token_limit(template_idx=repetition_idx)\n","    prompt = prompt.replace('PROBLEM_PLACEHOLDER', problem)\n","    return prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:28.104614Z","iopub.status.busy":"2024-06-03T15:36:28.104037Z","iopub.status.idle":"2024-06-03T15:36:28.130895Z","shell.execute_reply":"2024-06-03T15:36:28.130070Z","shell.execute_reply.started":"2024-06-03T15:36:28.104584Z"},"trusted":true},"outputs":[],"source":["for idx in range(len(prompt_options) + len(few_shot_prompt_templates)):\n","    display(Markdown(get_formatted_prompt('What is $1 + 10$?', idx)))\n","    display(Markdown('---'))"]},{"cell_type":"markdown","metadata":{},"source":["### Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:28.132286Z","iopub.status.busy":"2024-06-03T15:36:28.131952Z","iopub.status.idle":"2024-06-03T15:36:28.137069Z","shell.execute_reply":"2024-06-03T15:36:28.136154Z","shell.execute_reply.started":"2024-06-03T15:36:28.132256Z"},"trusted":true},"outputs":[],"source":["def clear_memory():\n","    for _ in range(2):\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","        time.sleep(0.01)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:28.138378Z","iopub.status.busy":"2024-06-03T15:36:28.138093Z","iopub.status.idle":"2024-06-03T15:36:28.146845Z","shell.execute_reply":"2024-06-03T15:36:28.146157Z","shell.execute_reply.started":"2024-06-03T15:36:28.138356Z"},"trusted":true},"outputs":[],"source":["def is_ending_time(max_time=CFG.time_limit):\n","    is_ending_time = get_time_spent() > max_time\n","    if is_ending_time:\n","        logging.warning('Reached limit time, inference will be skipped.')\n","    return is_ending_time\n","\n","def get_time_spent():\n","    return time.time() - NOTEBOOK_START_TIME\n","\n","assert not is_ending_time(100)\n","assert is_ending_time(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:28.148239Z","iopub.status.busy":"2024-06-03T15:36:28.147872Z","iopub.status.idle":"2024-06-03T15:36:28.155708Z","shell.execute_reply":"2024-06-03T15:36:28.154860Z","shell.execute_reply.started":"2024-06-03T15:36:28.148216Z"},"trusted":true},"outputs":[],"source":["def is_quick_save_condition(idx, test):\n","    if CFG.quick_save and idx == 0 and CFG.submission_mode:\n","        if test['id'].values[0] == '000aaa':\n","            if test['problem'].values[0] == 'What is $1-1$?':\n","                logging.info('Quick save condition reached. Skipping inference')\n","                return True\n","    return False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:28.161535Z","iopub.status.busy":"2024-06-03T15:36:28.161253Z","iopub.status.idle":"2024-06-03T15:36:28.166577Z","shell.execute_reply":"2024-06-03T15:36:28.165635Z","shell.execute_reply.started":"2024-06-03T15:36:28.161513Z"},"trusted":true},"outputs":[],"source":["def get_timestamp():\n","    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n","\n","print(get_timestamp())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:28.168292Z","iopub.status.busy":"2024-06-03T15:36:28.167736Z","iopub.status.idle":"2024-06-03T15:36:28.176114Z","shell.execute_reply":"2024-06-03T15:36:28.175424Z","shell.execute_reply.started":"2024-06-03T15:36:28.168261Z"},"trusted":true},"outputs":[],"source":["N_REPETITIONS = CFG.n_repetitions\n","PROBLEM_REPETITIONS = []\n","\n","def adjust_repetitions_to_meet_ending_time(answered_problems,\n","                                           max_time=CFG.time_limit,\n","                                           min_problem_threshold=20,\n","                                           hysteresis=0.975):\n","    global N_REPETITIONS, PROBLEM_REPETITIONS\n","    PROBLEM_REPETITIONS.append(N_REPETITIONS)\n","    if answered_problems < min_problem_threshold:\n","        return\n","    spent_time = get_time_spent()\n","    mean_problem_time = spent_time/sum(PROBLEM_REPETITIONS)\n","    estimated_ending_time = (N_PROBLEMS - answered_problems)*mean_problem_time*N_REPETITIONS + spent_time\n","    logging.info(f'Mean problem time: {mean_problem_time:.1f} seconds, estimated ending time {estimated_ending_time/3600:.1f} hours')\n","    if estimated_ending_time > max_time and N_REPETITIONS > 1:\n","        N_REPETITIONS -= 1\n","        logging.warning(f'Decreasing the number of repetitions to {N_REPETITIONS} to try to meet ending time')\n","    elif estimated_ending_time < max_time*hysteresis and N_REPETITIONS < CFG.n_repetitions:\n","        N_REPETITIONS += 1\n","        logging.warning(f'Increasing the number of repetitions to {N_REPETITIONS} because it seems to be enough time to meet the ending time')"]},{"cell_type":"markdown","metadata":{},"source":["### Response parsing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:28.177464Z","iopub.status.busy":"2024-06-03T15:36:28.177196Z","iopub.status.idle":"2024-06-03T15:36:28.187765Z","shell.execute_reply":"2024-06-03T15:36:28.187050Z","shell.execute_reply.started":"2024-06-03T15:36:28.177443Z"},"trusted":true},"outputs":[],"source":["def text_to_int_answer(text):\n","    try:\n","        answer = float(text)\n","        if answer < 0 or not answer.is_integer():\n","            return None\n","        return int(answer)\n","    except (ValueError, OverflowError):\n","        return None\n","\n","assert 5 == text_to_int_answer('5')\n","assert 5 == text_to_int_answer('5.0')\n","assert text_to_int_answer('-1') is None\n","assert text_to_int_answer('0.5') is None\n","assert text_to_int_answer('pi') is None"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:28.189162Z","iopub.status.busy":"2024-06-03T15:36:28.188827Z","iopub.status.idle":"2024-06-03T15:36:28.197324Z","shell.execute_reply":"2024-06-03T15:36:28.196429Z","shell.execute_reply.started":"2024-06-03T15:36:28.189135Z"},"trusted":true},"outputs":[],"source":["def parse_boxed_answer(text):\n","    matches = re.findall(r'\\\\boxed\\{(\\d+)\\}', text)\n","    if matches:\n","        return text_to_int_answer(matches[-1])\n","    return None\n","\n","test_text = \"\"\"\n","\n","blah blah \\\\boxed{5} 7\n","\"\"\"\n","assert parse_boxed_answer(test_text) == 5\n","\n","test_text = \"\"\"\n","\n","blah blah {5} 7\n","\"\"\"\n","assert parse_boxed_answer(test_text) == None"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:28.198467Z","iopub.status.busy":"2024-06-03T15:36:28.198208Z","iopub.status.idle":"2024-06-03T15:36:28.211565Z","shell.execute_reply":"2024-06-03T15:36:28.210739Z","shell.execute_reply.started":"2024-06-03T15:36:28.198436Z"},"trusted":true},"outputs":[],"source":["def parse_response_in_text(text):\n","    response = parse_boxed_answer(text)\n","    if response is not None:\n","        return response\n","    return parse_last_answer(text)\n","\n","def parse_last_answer(text):\n","    pattern = r'(?:the answer is|the final answer is)\\s*:?\\s*\\$?(\\d+(\\.\\d+)?)\\$?'\n","    matches = re.findall(pattern, text, re.IGNORECASE)\n","    if matches:\n","        return text_to_int_answer(matches[-1][0])\n","    return None\n","\n","test_cases = [\n","    ('The answer is: $651$', 651),\n","    ('The answer is: $5$.', 5),\n","    ('The answer is: 6.', 6),\n","    ('The final answer is 0.', 0),\n","    ('The final answer is 126.', 126),\n","    ('The final answer is: $2$.', 2),\n","    ('The answer is $\\\\boxed{3}$', 3),\n","    ('The answer is $\\\\boxed{-1}$', None),\n","    ('The answer is $\\\\boxed{1.5}$', None),\n","    ('The answer is: $-1$.', None),\n","    ('The answer is: $4.5$.', None),\n","    ('The final answer is 0.6', None),\n","]\n","for text, answer in test_cases:\n","    assert parse_response_in_text(text) == answer\n","    assert parse_response_in_text(text.lower()) == answer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:28.212797Z","iopub.status.busy":"2024-06-03T15:36:28.212538Z","iopub.status.idle":"2024-06-03T15:36:28.221593Z","shell.execute_reply":"2024-06-03T15:36:28.220838Z","shell.execute_reply.started":"2024-06-03T15:36:28.212776Z"},"trusted":true},"outputs":[],"source":["def parse_response_in_code(code_output):\n","    if code_output is None:\n","        return None\n","    try:\n","        code_output = code_output.strip()\n","        if code_output.startswith('[') and code_output.endswith(']'):\n","            return text_to_int_answer(code_output[1:-1])\n","        return text_to_int_answer(code_output)\n","    except Exception as e:\n","        print(f'Exception when trying to get a response from code: {e}')\n","        return None\n","    \n","assert parse_response_in_code('0') == 0\n","assert parse_response_in_code('[0]') == 0"]},{"cell_type":"markdown","metadata":{},"source":["### Code interpreter"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:28.223135Z","iopub.status.busy":"2024-06-03T15:36:28.222816Z","iopub.status.idle":"2024-06-03T15:36:29.110852Z","shell.execute_reply":"2024-06-03T15:36:29.109889Z","shell.execute_reply.started":"2024-06-03T15:36:28.223089Z"},"trusted":true},"outputs":[],"source":["def code_interpreter(code):\n","    code = preprocess_code(code)\n","    output, run_success = execute_code(code)\n","    return output, run_success\n","\n","def preprocess_code(code):\n","    code = ensure_symbols_are_real(code)\n","    code = add_simplify_to_print(code)\n","    code = f'from sympy import *\\n{code}'\n","    return code\n","\n","def add_simplify_to_print(code):\n","    code = code.replace('print(', 'simplify_print(')\n","    new_code = \"\"\"\n","def simplify_print(x):\n","    print(recursive_simplify(x))\n","        \n","def recursive_simplify(x):\n","    if isinstance(x, list):\n","        return [recursive_simplify(y) for y in x]\n","    return simplify(x)\n","\"\"\"\n","    code = new_code + '\\n' + code\n","    return code\n","\n","def ensure_symbols_are_real(code):\n","    def replace_symbols_call(match):\n","        matched_text = match.group()\n","        if \"real\" not in matched_text:\n","            return f\"{matched_text[:-1]}, real=True)\"\n","        else:\n","            return matched_text\n","    code = re.sub(r\"symbols\\([^)]+\\)\", replace_symbols_call, code)\n","    return code\n","\n","assert ensure_symbols_are_real(\"x, y, z = symbols('x y z')\") == \"x, y, z = symbols('x y z', real=True)\"\n","assert ensure_symbols_are_real(\"x, y, z = symbols('x y z', real=True)\") == \"x, y, z = symbols('x y z', real=True)\"\n","\n","def execute_code(code, timeout_limit=7):\n","    with tempfile.NamedTemporaryFile(mode='w+', delete=False) as temp_file:\n","        temp_file.write(code)\n","        temp_filepath = temp_file.name\n","    cmd = f'timeout {timeout_limit} {sys.executable} {temp_filepath}'\n","    ret = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n","    os.remove(temp_filepath)\n","    if ret.returncode == 0:\n","        return truncate_text(get_last_line(ret.stdout)), True\n","    elif ret.returncode == 124:\n","        return f'The execution of the code timeout. The code needs to run in less than {timeout_limit} seconds.', False\n","    else:\n","        #output = remove_references_to_temp_code_file(ret.stderr, temp_filepath)\n","        output = truncate_text(get_last_line(ret.stderr))\n","        return output, False\n","    \n","def remove_references_to_temp_code_file(output, filepath):\n","    return output.replace(f'File \"{filepath}\", ', '')\n","\n","def get_last_line(text):\n","    lines = text.strip().splitlines()\n","    if lines:\n","        return lines[-1]\n","    return text.strip()\n","\n","def truncate_text(text, max_length=CFG.code_output_truncate_length):\n","    \"\"\"Sometimes code output can be very long\"\"\"\n","    if len(text) > max_length:\n","        return text[:max_length] + '...'\n","    return text\n","\n","test_code = \"\"\"\n","print('Hello')\n","\"\"\"\n","print(code_interpreter('print(0)'))\n","print(code_interpreter('foo'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:29.112574Z","iopub.status.busy":"2024-06-03T15:36:29.112093Z","iopub.status.idle":"2024-06-03T15:36:30.129094Z","shell.execute_reply":"2024-06-03T15:36:30.128226Z","shell.execute_reply.started":"2024-06-03T15:36:29.112543Z"},"trusted":true},"outputs":[],"source":["test_code = \"\"\"\n","from sympy import symbols, Eq, solve\n","\n","def solve_equation():\n","    x = symbols('x')\n","    equation = Eq(4 + x, 4)\n","    solution = solve(equation, x)\n","\n","    return solution\n","\n","result = solve_equation()\n","print(result)\n","\"\"\"\n","print(code_interpreter(test_code))\n","\n","test_code = \"\"\"\n","from sympy import symbols, Eq, solve\n","\n","def solve_equation():\n","    x = symbols('x', real=True)\n","    equation = Eq(4 + x, 4)\n","    solution = solve(equation, x)\n","\n","    return solution\n","\n","result = solve_equation()\n","print(result)\n","\"\"\"\n","print(code_interpreter(test_code))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.130942Z","iopub.status.busy":"2024-06-03T15:36:30.130371Z","iopub.status.idle":"2024-06-03T15:36:30.136145Z","shell.execute_reply":"2024-06-03T15:36:30.135343Z","shell.execute_reply.started":"2024-06-03T15:36:30.130907Z"},"trusted":true},"outputs":[],"source":["def parse_last_python_code_block(text):\n","    return text.split('```python')[-1].split(\"```\")[0]\n","\n","test_text = \"\"\"\n","```python\n","hello\n","``````output\n","\"\"\"\n","assert parse_last_python_code_block(test_text) == '\\nhello\\n'\n","\n","test_text = \"\"\"\n","```python\n","hello\n","```\n","\"\"\"\n","assert parse_last_python_code_block(test_text) == '\\nhello\\n'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.137735Z","iopub.status.busy":"2024-06-03T15:36:30.137409Z","iopub.status.idle":"2024-06-03T15:36:30.145151Z","shell.execute_reply":"2024-06-03T15:36:30.144338Z","shell.execute_reply.started":"2024-06-03T15:36:30.137705Z"},"trusted":true},"outputs":[],"source":["def add_code_output_to_prompt(decoded_output, code_output):\n","    if decoded_output.endswith(\")\\n```\"):\n","        prompt = decoded_output+'```output\\n'+str(code_output)+'\\n```\\n'\n","    else:\n","        prompt = decoded_output+'\\n'+str(code_output)+'\\n```\\n'\n","    return prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.146351Z","iopub.status.busy":"2024-06-03T15:36:30.146065Z","iopub.status.idle":"2024-06-03T15:36:30.155503Z","shell.execute_reply":"2024-06-03T15:36:30.154779Z","shell.execute_reply.started":"2024-06-03T15:36:30.146326Z"},"trusted":true},"outputs":[],"source":["class CodeRunner():\n","    \"\"\"\n","    Abstraction to run code that:\n","    \n","    - Accumulates the code if the runs are succesfull\n","    - Measures number of coding errors\n","    \"\"\"\n","    def __init__(self, max_coding_errors=2):\n","        self.accumulated_code = ''\n","        self.n_coding_errors = 0\n","        self.successful_code_output = None\n","        self.max_coding_errors = max_coding_errors\n","        self.code_interpreter_calls = 0\n","    \n","    def run_code(self, code):\n","        self.code_interpreter_calls += 1\n","        new_code = self.accumulated_code + \"\\n\" + code\n","        code_output, run_success = code_interpreter(new_code)\n","        if run_success:\n","            self.accumulated_code = new_code\n","            self.successful_code_output = code_output\n","        else:\n","            self.n_coding_errors += 1\n","            self.successful_code_output = None\n","        return code_output\n","    \n","    def max_coding_errors_reached(self):\n","        max_coding_errors_reached = self.n_coding_errors >= self.max_coding_errors\n","        if max_coding_errors_reached:\n","            logging.warning(f'Stopping solution generation because {self.n_coding_errors} coding errors were done.')\n","        return max_coding_errors_reached"]},{"cell_type":"markdown","metadata":{},"source":["### Results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.156715Z","iopub.status.busy":"2024-06-03T15:36:30.156456Z","iopub.status.idle":"2024-06-03T15:36:30.269646Z","shell.execute_reply":"2024-06-03T15:36:30.268879Z","shell.execute_reply.started":"2024-06-03T15:36:30.156686Z"},"trusted":true},"outputs":[],"source":["class InferenceResult(BaseModel):\n","    # text\n","    prompt: str\n","    response: Optional[str] = None\n","    # answers\n","    boxed_answer: Optional[int] = None\n","    text_answer: Optional[int] = None\n","    code_answer: Optional[int] = None\n","    # output\n","    output_tokens: int = 0\n","    reached_max_tokens: bool = False\n","    # code\n","    coding_errors: int = 0\n","    code_interpreter_calls: int = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.271152Z","iopub.status.busy":"2024-06-03T15:36:30.270802Z","iopub.status.idle":"2024-06-03T15:36:30.285550Z","shell.execute_reply":"2024-06-03T15:36:30.284491Z","shell.execute_reply.started":"2024-06-03T15:36:30.271107Z"},"trusted":true},"outputs":[],"source":["def is_difference_significative(n_first, n_second, n_tries, confidence_level=CFG.confidence_level):\n","    if n_second == 0:\n","        if n_first == n_tries:\n","            return is_difference_significative(n_first, 1, n_tries + 1, confidence_level)\n","        elif n_first < n_tries:\n","            return is_difference_significative(n_first, 1, n_tries, confidence_level)\n","        else:\n","            raise ValueError()\n","    p_first = n_first/n_tries\n","    p_second = n_second/n_tries\n","    uncertainty = (p_first*(1-p_first)/n_tries + p_second*(1-p_second)/n_tries)**0.5\n","    z = (p_first - p_second)/uncertainty\n","    logging.info(f'p_first: {p_first*100:.1f}% p_second: {p_second*100:.1f}% Confidence level for the difference: {2*(norm.cdf(z) - 0.5)*100:.1f}%')\n","    return z > norm.interval(confidence_level)[1]\n","\n","is_difference_significative(3, 0, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.287627Z","iopub.status.busy":"2024-06-03T15:36:30.287022Z","iopub.status.idle":"2024-06-03T15:36:30.308156Z","shell.execute_reply":"2024-06-03T15:36:30.307162Z","shell.execute_reply.started":"2024-06-03T15:36:30.287597Z"},"trusted":true},"outputs":[],"source":["def log_ground_truth(idx):\n","    if isinstance(env, MockEnvWithDataframe) and 'ground_truth' in df.columns:\n","        logging.info(f'Ground truth: {df[\"ground_truth\"].loc[idx]}')\n","\n","class Results():\n","    def __init__(self):\n","        self.results = dict()\n","\n","    def initialize(self, idx):\n","        self.results[idx] = []\n","\n","    def add_result(self, idx, result: InferenceResult):\n","        self.results[idx].append(result)\n","    \n","    def log_results_distribution(self, idx):\n","        log_ground_truth(idx)\n","        keys = ['boxed_answer', 'text_answer', 'code_answer']\n","        for key in keys:\n","            values = self.get_result_distribution(idx, key)\n","            logging.info(f'{key} distribution: {values}')\n","\n","    def get_valid_results(self, idx, result_priority):\n","        results = []\n","        for result in self.results[idx]:\n","            result = result.dict()\n","            for key in result_priority:\n","                if result[key] is not None:\n","                    results.append(result[key])\n","                    break\n","        if results:\n","            return results\n","        raise NoValidResults(idx)\n","\n","    def get_most_frequent_result(self, idx, result_priority=CFG.result_priority):\n","        valid_results = self.get_valid_results(idx, result_priority)\n","        counter_ret = Counter(valid_results).most_common()\n","        logging.info(f'Result counts for {idx}: {counter_ret}')\n","        result, count = get_minimum_most_frequent_value(counter_ret)\n","        return result, count\n","\n","    def is_best_solution_found(self, idx, result_priority=CFG.result_priority):\n","        try:\n","            valid_results = self.get_valid_results(idx, result_priority)\n","            counter_ret = Counter(valid_results).most_common()\n","            logging.info(f'Result counts for {idx}: {counter_ret}')\n","            if len(counter_ret) == 1:\n","                return is_difference_significative(counter_ret[0][1], 0, len(valid_results))\n","            else:\n","                return is_difference_significative(counter_ret[0][1], counter_ret[1][1], len(valid_results))\n","        except NoValidResults:\n","            return False\n","\n","    def get_result_distribution(self, idx, key):\n","        results = self.results[idx]\n","        distribution = np.array([result.dict()[key] for result in results])\n","        return distribution\n","    \n","    def save(self, filepath='results.json'):\n","        logging.info(f'Saving results in {os.path.realpath(filepath)}')\n","        results = {idx: [result.dict() for result in results] for idx, results in self.results.items()}\n","        with open(filepath, 'w') as f:\n","            json.dump(results, f, indent=4)\n","\n","    def load(self, filepath):\n","        logging.info(f'Loading results from {filepath}')\n","        with open(filepath, 'r') as f:\n","            results = json.load(f)\n","        self.results = {int(idx): [InferenceResult(**result) for result in results] for idx, results in results.items()}\n","            \n","    def __repr__(self):\n","        return str(self.results)\n","    \n","def get_minimum_most_frequent_value(counter_ret):\n","    max_count = counter_ret[0][1]\n","    candidates = []\n","    for value, count in counter_ret:\n","        if count == max_count:\n","            candidates.append(value)\n","        else:\n","            break\n","    return min(candidates), max_count\n","\n","class NoValidResults(Exception):\n","    pass\n","\n","assert get_minimum_most_frequent_value([(2, 1), (3, 1)]) == (2, 1)\n","assert get_minimum_most_frequent_value([(3, 1), (2, 1)]) == (2, 1)\n","assert get_minimum_most_frequent_value([(3, 2), (2, 1)]) == (3, 2)"]},{"cell_type":"markdown","metadata":{},"source":["### Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.310022Z","iopub.status.busy":"2024-06-03T15:36:30.309639Z","iopub.status.idle":"2024-06-03T15:36:30.323002Z","shell.execute_reply":"2024-06-03T15:36:30.322082Z","shell.execute_reply.started":"2024-06-03T15:36:30.309988Z"},"trusted":true},"outputs":[],"source":["def solve_problem_with_code_interpreter(prompt):\n","    text_generator = TextGeneratorVLLM(cfg=CFG)\n","    clear_memory()\n","    code_runner = CodeRunner(CFG.max_coding_errors)\n","    decoded_output = prompt\n","    stop_word_cond = True\n","    generation_mode = 'text'\n","    while stop_word_cond and text_generator.are_generation_tokens_available():\n","        if decoded_output.endswith(\"Problem:\") or decoded_output.endswith(\"User:\"):\n","            break\n","        is_code_block_finished = not decoded_output.endswith(\"```python\") and generation_mode == 'code'\n","        if is_code_block_finished:\n","            code_text = parse_last_python_code_block(decoded_output)\n","            code_output = code_runner.run_code(code_text)\n","            if code_runner.max_coding_errors_reached():\n","                break\n","            decoded_output = add_code_output_to_prompt(decoded_output, code_output)\n","\n","        if decoded_output.endswith(\"```python\"):\n","            decoded_output += '\\n'\n","            generation_mode = 'code'\n","        else:\n","            generation_mode = 'text'\n","\n","        decoded_output = text_generator(decoded_output, mode=generation_mode)\n","        stop_word_cond = any(decoded_output.endswith(stop_word) for stop_word in CFG.stop_words)\n","\n","    log_gpu_memory()\n","    if prompt.endswith(\"```python\"):\n","        decoded_output = decoded_output.replace(prompt, '```python')\n","        prompt = prompt[:-len(\"```python\")]\n","    else:\n","        decoded_output = decoded_output.replace(prompt, '')\n","    result = InferenceResult(\n","        prompt=prompt,\n","        response=decoded_output,\n","        output_tokens=text_generator.generated_tokens,\n","        coding_errors=code_runner.n_coding_errors,\n","        code_interpreter_calls=code_runner.code_interpreter_calls\n","    )\n","    if not text_generator.are_generation_tokens_available():\n","        # Solution was not achieved, it does not have sense to parse responses\n","        logging.warning(f'Max number of new tokens {CFG.max_new_tokens} was reached. Solution not found.')\n","        result.reached_max_tokens = True\n","    else:\n","        logging.info(f'Total generated tokens: {text_generator.generated_tokens}')\n","        if not code_runner.max_coding_errors_reached():\n","            result.boxed_answer = parse_boxed_answer(decoded_output)\n","            result.text_answer = parse_response_in_text(decoded_output)\n","            result.code_answer = parse_response_in_code(code_runner.successful_code_output)\n","    return result"]},{"cell_type":"markdown","metadata":{},"source":["### Show"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.324276Z","iopub.status.busy":"2024-06-03T15:36:30.323999Z","iopub.status.idle":"2024-06-03T15:36:30.334663Z","shell.execute_reply":"2024-06-03T15:36:30.333889Z","shell.execute_reply.started":"2024-06-03T15:36:30.324254Z"},"trusted":true},"outputs":[],"source":["def display_decoded_output(idx, text):\n","    display(Markdown('---'))\n","    display(Markdown(f'### Problem {idx}'))\n","    display(Markdown(text.replace('Assistant: ', 'Assistant: \\n')))\n","    display(Markdown('---'))"]},{"cell_type":"markdown","metadata":{},"source":["### Results analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.336235Z","iopub.status.busy":"2024-06-03T15:36:30.335678Z","iopub.status.idle":"2024-06-03T15:36:30.348133Z","shell.execute_reply":"2024-06-03T15:36:30.347235Z","shell.execute_reply.started":"2024-06-03T15:36:30.336211Z"},"trusted":true},"outputs":[],"source":["def show_inference_insights(results):\n","    keys = ['coding_errors', 'output_tokens', 'code_interpreter_calls']\n","    answers = ['boxed_answer', 'text_answer', 'code_answer']\n","    rows = []\n","    for idx in results.results:\n","        logging.info(f'Logging inference insights for problem {idx}')\n","        row = dict(n_runs=len(results.get_result_distribution(idx, keys[0])))\n","        for key in keys:\n","            values = results.get_result_distribution(idx, key)\n","            logging.info(f'{key} distribution: {values}')\n","            row[f'mean_{key}'] = round(np.mean(values), 1)\n","            row[f'median_{key}'] = round(np.median(values), 1)\n","        values = results.get_result_distribution(idx, 'reached_max_tokens')\n","        logging.info(f'reached_max_tokens distribution: {values}')\n","        row['unfinished_responses'] = np.sum(values)\n","        values = results.get_result_distribution(idx, 'coding_errors')\n","        row['max_coding_errors_reached'] = np.sum(values >= CFG.max_coding_errors)\n","        \n","        for answer in answers:\n","            values = results.get_result_distribution(idx, answer)\n","            logging.info(f'{answer} distribution: {values}')\n","            row[f'{answer}s'] = np.sum(values != None)\n","        rows.append(row)\n","        logging.info('')\n","    insights = pd.DataFrame(rows)\n","    summary = insights.sum()\n","    for column in insights.columns:\n","        if 'mean' in column or 'median' in column:\n","            summary[column] = round(summary[column] / len(insights), 1)\n","    insights.loc['all'] = summary\n","    for column in insights.columns[-5:]:\n","        insights[column] = (insights[column]/insights['n_runs']*100).round(1)\n","    return insights"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.349738Z","iopub.status.busy":"2024-06-03T15:36:30.349448Z","iopub.status.idle":"2024-06-03T15:36:30.367137Z","shell.execute_reply":"2024-06-03T15:36:30.366168Z","shell.execute_reply.started":"2024-06-03T15:36:30.349713Z"},"trusted":true},"outputs":[],"source":["def get_accuracy_report(results, result_priority):\n","    report = df[['answer', 'ground_truth']].copy()\n","    report['answer'] = 0\n","    report['n_runs'] = 0\n","    report['correct_counts'] = 0\n","    report['highest_wrong_counts'] = 0\n","    report['wrong_counts'] = 0\n","    report['highest_correct_tokens'] = None\n","\n","    for idx in results.results:\n","        try:\n","            report.loc[idx, 'n_runs'] = get_n_runs(idx, results)\n","            values = np.array(results.get_valid_results(idx, result_priority))\n","            counter_ret = Counter(values).most_common()\n","            report.loc[idx, 'answer'] = get_minimum_most_frequent_value(counter_ret)[0]\n","            ground_truth = df.loc[idx, 'ground_truth']\n","            for pred, count in counter_ret:\n","                if pred == ground_truth:\n","                    report.loc[idx, 'correct_counts'] = count\n","                    break\n","            report.loc[idx, 'highest_wrong_counts'] = get_highest_wrong_count(counter_ret, ground_truth)\n","            report.loc[idx, 'wrong_counts'] = get_wrong_counts(counter_ret, ground_truth)\n","        except NoValidResults:\n","            report.loc[idx, 'answer'] = None\n","        if report.loc[idx, 'correct_counts'] > 0:\n","            report.loc[idx, 'highest_correct_tokens'] = get_highest_correct_tokens(idx, ground_truth, results)\n","    report['is_correct'] = (report['answer'] == report['ground_truth']).astype(int)\n","    report['pass'] = report['correct_counts'] > 0\n","    report.loc[report['answer'].isna(), 'is_correct'] = np.nan\n","    return add_summary_to_report(report)\n","\n","def add_summary_to_report(report):\n","    summary = report.sum()\n","    for key in report.columns[:2]:\n","        summary[key] = '-'\n","    summary['highest_correct_tokens'] = report['highest_correct_tokens'].max()\n","    report.loc['summary'] = summary\n","    return report\n","\n","def get_highest_wrong_count(counter_ret, ground_truth):\n","    for pred, count in counter_ret:\n","        if pred != ground_truth:\n","            return count\n","    return 0\n","\n","def get_wrong_counts(counter_ret, ground_truth):\n","    wrong_counts = 0\n","    for pred, count in counter_ret:\n","        if pred != ground_truth:\n","            wrong_counts += count\n","    return wrong_counts\n","\n","\n","def get_n_runs(idx, results):\n","    return len(results.results[idx])\n","\n","def get_highest_correct_tokens(idx, ground_truth, results):\n","    highest_correct_tokens = 0\n","    tokens = results.get_result_distribution(idx, 'output_tokens')\n","    for answer in CFG.result_priority:\n","        values = results.get_result_distribution(idx, answer)\n","        correct_answer_tokens = tokens[values == ground_truth]\n","        if len(correct_answer_tokens) > 0:\n","            max_tokens = max(correct_answer_tokens)\n","            highest_correct_tokens = max(highest_correct_tokens, max_tokens)\n","    return highest_correct_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.368884Z","iopub.status.busy":"2024-06-03T15:36:30.368593Z","iopub.status.idle":"2024-06-03T15:36:30.392132Z","shell.execute_reply":"2024-06-03T15:36:30.391057Z","shell.execute_reply.started":"2024-06-03T15:36:30.368860Z"},"trusted":true},"outputs":[],"source":["def analyze_MATH_results(result_priority):\n","    logging.info(f'Analyzing MATH results for {result_priority} priorities')\n","    accuracy_report = get_accuracy_report(results, result_priority)\n","    print_disaggregated_metrics(accuracy_report)\n","    accuracy_report = accuracy_report.loc[accuracy_report.index[:-1]]\n","    print_relevant_metrics(accuracy_report)\n","    for key in ['level', 'type']:\n","        accuracy_report[key] = df[key]\n","        plot_grouped_results(accuracy_report, key)\n","\n","\n","def print_relevant_metrics(accuracy_report):\n","    correct = accuracy_report['is_correct'].value_counts().get(1, 0)\n","    unanswered = accuracy_report['is_correct'].isna().sum()\n","    wrong = accuracy_report['is_correct'].value_counts().get(0, 0)\n","    total = correct + unanswered + wrong\n","    accuracy = correct/total\n","    print('\\tAggregated metrics majority vote')\n","    print(f'Correct: {correct}/{total} ({accuracy:.2f} ± {estimate_uncertainty(accuracy, total):.2f})')\n","    print(f'Unanswered: {unanswered}/{total} ({unanswered/total:.2f} ± {estimate_uncertainty(unanswered/total, total):.2f})')\n","    print(f'Wrong: {wrong}/{total} ({wrong/total:.2f} ± {estimate_uncertainty(wrong/total, total):.2f})')\n","    print('\\tAggregated metrics pass')\n","    correct = accuracy_report['pass'].sum()\n","    accuracy = correct/total\n","    print(f'Correct: {correct}/{total} ({accuracy:.2f} ± {estimate_uncertainty(accuracy, total):.2f})')\n","\n","\n","def estimate_uncertainty(proportion, n):\n","    return 1.96 * np.sqrt(proportion * (1 - proportion) / n)\n","\n","\n","def print_disaggregated_metrics(accuracy_report):\n","    correct = accuracy_report.loc['summary', 'correct_counts']\n","    wrong = accuracy_report.loc['summary', 'wrong_counts']\n","    total = accuracy_report.loc['summary', 'n_runs']\n","    unanswered = total - correct - wrong\n","    print('\\tDisaggregated metrics')\n","    print(f'Correct: {correct}/{total} ({correct/total:.2f} ± {estimate_uncertainty(correct/total, total):.2f})')\n","    print(f'Unanswered: {unanswered}/{total} ({unanswered/total:.2f} ± {estimate_uncertainty(unanswered/total, total):.2f})')\n","    print(f'Wrong: {wrong}/{total} ({wrong/total:.2f} ± {estimate_uncertainty(wrong/total, total):.2f})')\n","\n","\n","def plot_grouped_results(df, group):\n","    categories = sorted(df[group].unique().tolist())\n","    correct = []\n","    unanswered = []\n","    wrong = []\n","    for category in categories:\n","        correct.append(df[df[group] == category].is_correct.value_counts().get(1, 0))\n","        unanswered.append(df[df[group] == category].is_correct.isna().sum())\n","        wrong.append(df[df[group] == category].is_correct.value_counts().get(0, 0))\n","\n","    correct.append(np.sum(correct))\n","    unanswered.append(np.sum(unanswered))\n","    wrong.append(np.sum(wrong))\n","    categories.append('overall')\n","\n","    total = np.array(correct) + np.array(unanswered) + np.array(wrong)\n","    correct = np.array(correct)/total\n","    unanswered = np.array(unanswered)/total\n","    wrong = np.array(wrong)/total\n","    plt.bar(categories, correct, label='Correct', color='tab:green')\n","    plt.bar(categories, unanswered, bottom=correct, label='Unanswered', color='tab:orange')\n","    plt.bar(categories, wrong, bottom=np.array(correct)+np.array(unanswered), label='Wrong', color='tab:red')\n","    for idx, value in enumerate(categories):\n","        plt.text(value, correct[idx]/2, f'{correct[idx]*100:.0f}%', ha='center', va='center')\n","        plt.text(value, correct[idx] + unanswered[idx]/2, f'{unanswered[idx]*100:.0f}%', ha='center', va='center')\n","        plt.text(value, correct[idx] + unanswered[idx] + wrong[idx]/2, f'{wrong[idx]*100:.0f}%', ha='center', va='center')\n","    #plt.legend(loc=0)\n","    plt.ylim(0, 1)\n","    plt.grid(axis='y')\n","    plt.title(f'Results grouped by {group}')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.393835Z","iopub.status.busy":"2024-06-03T15:36:30.393503Z","iopub.status.idle":"2024-06-03T15:36:30.407388Z","shell.execute_reply":"2024-06-03T15:36:30.406398Z","shell.execute_reply.started":"2024-06-03T15:36:30.393807Z"},"trusted":true},"outputs":[],"source":["wait_for_gpu_memory()\n","create_model_and_inference_artifacts() # TODO: remove this line"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-06-03T15:36:30.409015Z","iopub.status.busy":"2024-06-03T15:36:30.408647Z","iopub.status.idle":"2024-06-03T15:36:30.457872Z","shell.execute_reply":"2024-06-03T15:36:30.456858Z","shell.execute_reply.started":"2024-06-03T15:36:30.408982Z"},"papermill":{"duration":34.259365,"end_time":"2024-02-29T09:37:05.548829","exception":false,"start_time":"2024-02-29T09:36:31.289464","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["results = Results()\n","for idx, (test, sample_submission) in tqdm(enumerate(iter_test), total=N_PROBLEMS, smoothing=0):\n","    if is_quick_save_condition(idx, test):\n","        break\n","    create_model_and_inference_artifacts()\n","    results.initialize(idx)\n","    adjust_repetitions_to_meet_ending_time(idx)\n","    for repetition_idx in tqdm(range(N_REPETITIONS), desc='Repetitions', smoothing=0):\n","        if is_ending_time() or results.is_best_solution_found(idx, CFG.result_priority):\n","            break\n","        logging.info(f\"Problem {idx} - repetition {repetition_idx+1}/{CFG.n_repetitions} - time spent : {get_time_spent():.0f} secs\")\n","        try:\n","            prompt = get_formatted_prompt(test['problem'].values[0], repetition_idx)\n","            result = solve_problem_with_code_interpreter(prompt)\n","            results.add_result(idx, result)\n","            results.log_results_distribution(idx)\n","            if CFG.verbose:\n","                display_decoded_output(idx, test['problem'].values[0] + '\\n\\n' + result.response)\n","        except Exception as e:\n","            logging.warning(f'Exception when trying to generate response for {idx}: {e}')\n","            raise e # TODO: remove this line\n","    try:\n","        result, count = results.get_most_frequent_result(idx, CFG.result_priority)\n","        sample_submission['answer'] = result % 1000\n","    except NoValidResults:\n","        logging.warning(f'No valid results for problem {idx}. Using default answer: {CFG.default_answer}')\n","        sample_submission['answer'] = CFG.default_answer\n","    log_ground_truth(idx)\n","    logging.info(f'Predicted answer for problem {idx} is: {sample_submission[\"answer\"].values[0]}')\n","    env.predict(sample_submission)\n","    print('\\n'*4)"]},{"cell_type":"markdown","metadata":{},"source":["## Results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.459407Z","iopub.status.busy":"2024-06-03T15:36:30.459066Z","iopub.status.idle":"2024-06-03T15:36:30.465282Z","shell.execute_reply":"2024-06-03T15:36:30.464304Z","shell.execute_reply.started":"2024-06-03T15:36:30.459380Z"},"trusted":true},"outputs":[],"source":["if CFG.save_results:\n","    timestamp = get_timestamp()\n","    results.save(f'{timestamp}_results.json')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.466803Z","iopub.status.busy":"2024-06-03T15:36:30.466483Z","iopub.status.idle":"2024-06-03T15:36:30.473490Z","shell.execute_reply":"2024-06-03T15:36:30.472507Z","shell.execute_reply.started":"2024-06-03T15:36:30.466773Z"},"trusted":true},"outputs":[],"source":["if not CFG.quick_save:\n","    insights = show_inference_insights(results)\n","    display(insights)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.475010Z","iopub.status.busy":"2024-06-03T15:36:30.474683Z","iopub.status.idle":"2024-06-03T15:36:30.483524Z","shell.execute_reply":"2024-06-03T15:36:30.482693Z","shell.execute_reply.started":"2024-06-03T15:36:30.474952Z"},"trusted":true},"outputs":[],"source":["if not CFG.submission_mode:\n","    accuracy_report = get_accuracy_report(results, CFG.result_priority)\n","    accuracy_report.to_csv(f'{timestamp}_accuracy_report.csv', index=True)\n","    display(accuracy_report)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.484754Z","iopub.status.busy":"2024-06-03T15:36:30.484509Z","iopub.status.idle":"2024-06-03T15:36:30.496900Z","shell.execute_reply":"2024-06-03T15:36:30.496143Z","shell.execute_reply.started":"2024-06-03T15:36:30.484733Z"},"trusted":true},"outputs":[],"source":["if not CFG.submission_mode and 'MATH' in CFG.dataset:\n","    analyze_MATH_results(CFG.result_priority)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.498184Z","iopub.status.busy":"2024-06-03T15:36:30.497906Z","iopub.status.idle":"2024-06-03T15:36:30.507275Z","shell.execute_reply":"2024-06-03T15:36:30.506443Z","shell.execute_reply.started":"2024-06-03T15:36:30.498162Z"},"trusted":true},"outputs":[],"source":["logging.info(f'The notebook run in {get_time_spent()/60:.1f} minutes')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T15:36:30.508413Z","iopub.status.busy":"2024-06-03T15:36:30.508158Z","iopub.status.idle":"2024-06-03T15:36:30.806392Z","shell.execute_reply":"2024-06-03T15:36:30.805568Z","shell.execute_reply.started":"2024-06-03T15:36:30.508392Z"},"trusted":true},"outputs":[],"source":["empty_gpu_vram()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["del server_thread"]},{"cell_type":"markdown","metadata":{},"source":["## TODO:"]},{"cell_type":"markdown","metadata":{},"source":["- [x] Wait until the server is running\n","- [x] Better handling of stop words\n","- [ ] Better handling of tokens using completions object instead of tokenizer\n","- [ ] Remove verbosity from the server\n","- [ ] Allow to stop the server: https://chatgpt.com/share/7f5d5cc3-db8c-42f0-ae07-f7fa00e493c4\n","- [ ] Update the inference loop to allow parallelization\n","- [ ] Use 2 gpus instead of 1\n","- [ ] Clean code and remove old code\n","- [ ] Verify that we get similar results but much faster\n","- [ ] Move the code to Kaggle"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":8365361,"sourceId":73231,"sourceType":"competition"},{"datasetId":4281572,"sourceId":7369493,"sourceType":"datasetVersion"},{"datasetId":4720595,"sourceId":8012825,"sourceType":"datasetVersion"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"datasetId":4748944,"sourceId":8052555,"sourceType":"datasetVersion"},{"datasetId":4950771,"sourceId":8424919,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
