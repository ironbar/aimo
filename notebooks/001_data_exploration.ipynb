{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MATH dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/alejopaullier/aimo-external-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_problem(idx):\n",
    "    row = df.loc[idx]\n",
    "    title = f'## Problem {idx+1}: {row[\"level\"]} - {row[\"type\"]} {row[\"source\"]}/{row[\"stage\"]}'\n",
    "    display(Markdown(title))\n",
    "    display(Markdown(f\"### Problem\\n{row['problem']}\"))\n",
    "    display(Markdown(f\"### Solution\\n{row['solution']}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_int_answer(text):\n",
    "    answer = int(float(text))\n",
    "    if answer < 0:\n",
    "        return None\n",
    "    return answer\n",
    "\n",
    "def parse_boxed_answer(text):\n",
    "    matches = re.findall(r'\\\\boxed\\{(\\d+)\\}', text)\n",
    "    if matches:\n",
    "        return text_to_int_answer(matches[-1])\n",
    "    return None\n",
    "\n",
    "def safe_parse_boxed_answer(text):\n",
    "    matches = re.findall(r'\\\\boxed\\{(\\d+)\\}', text)\n",
    "    if len(matches) == 1:\n",
    "        return text_to_int_answer(matches[0])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('/home/gbarbadillo/data/deepseekmath')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mnt/hdd0/Kaggle/aimo/external_data/MATH_and_GSM8k.csv')\n",
    "print(len(df))\n",
    "df = df[df.source == 'MATH']\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_problem(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with integer answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_boxed'] = df.solution.str.count(r'\\\\boxed')\n",
    "df['n_boxed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.n_boxed > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_problem(65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should avoid problems with more than one solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['parsed_answer'] = df.solution.apply(safe_parse_boxed_answer)\n",
    "len(df), len(df[df.parsed_answer.notnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.parsed_answer.notnull()]\n",
    "df = df[df.n_boxed == 1]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_problem(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input_tokens'] = df.problem.apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "df['output_tokens'] = df.solution.apply(lambda x: len(tokenizer.tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 2000, 500)\n",
    "for key in ['input_tokens', 'output_tokens']:\n",
    "    plt.hist(df[key], bins=bins, alpha=0.5, label=key, density=True, cumulative=1)\n",
    "plt.xlim(0, 1000)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 90% of the solutions have less than 400 tokens.\n",
    "\n",
    "Let's see if there is any relation between the difficulty and token length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0, 2000)\n",
    "for key in ['input_tokens', 'output_tokens']:\n",
    "    for level in sorted(df.level.unique())[:-1]:\n",
    "        plt.hist(df[df.level == level][key], bins=bins, alpha=0.5, label=level, density=True, cumulative=1)\n",
    "    if key == 'input_tokens':\n",
    "        plt.xlim(0, 200)\n",
    "    else:\n",
    "        plt.xlim(0, 1000)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid()\n",
    "    plt.legend(loc=0);\n",
    "    plt.title(f'{key} distribution')\n",
    "    plt.xlabel('Tokens')\n",
    "    plt.ylabel('Cumulative probability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the more difficult problems have longer answers and descriptions. Very beautiful graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.stage == 'test']['level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to save the file for later being able to use it for creating few-shot prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_tokens'] = df.input_tokens + df.output_tokens\n",
    "df.columns = [column.replace('parsed_', '') for column in df.columns]\n",
    "df.sort_values('stage', inplace=True)\n",
    "df.sort_values('level', inplace=True)\n",
    "df['id'] = np.arange(len(df))\n",
    "df['answer'] = df['answer'].astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.answer % 1000).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/mnt/hdd0/Kaggle/aimo/external_data/filtered_MATH.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in ['train', 'test']:\n",
    "    df[df.stage == stage].to_csv(f'/mnt/hdd0/Kaggle/aimo/external_data/filtered_MATH_{stage}.csv', index=False)\n",
    "    print(stage, len(df[df.stage == stage]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Focus on problems that have integer and non-negative answers\n",
    "- [x] What is the distribution of output and input tokens?\n",
    "- [ ] What if I create smaller versions of the test set?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
