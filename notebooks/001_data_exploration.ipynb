{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_int_answer(text):\n",
    "    try:\n",
    "        answer = float(text)\n",
    "        if answer < 0 or not answer.is_integer():\n",
    "            return None\n",
    "        return int(answer)\n",
    "    except (ValueError, OverflowError):\n",
    "        return None\n",
    "\n",
    "assert 5 == text_to_int_answer('5')\n",
    "assert 5 == text_to_int_answer('5.0')\n",
    "assert text_to_int_answer('-1') is None\n",
    "assert text_to_int_answer('0.5') is None\n",
    "assert text_to_int_answer('pi') is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('/home/gbarbadillo/data/deepseekmath')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MATH dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/alejopaullier/aimo-external-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_problem(idx):\n",
    "    row = df.loc[idx]\n",
    "    title = f'## Problem {idx+1}: {row[\"level\"]} - {row[\"type\"]} {row[\"source\"]}/{row[\"stage\"]}'\n",
    "    display(Markdown(title))\n",
    "    display(Markdown(f\"### Problem\\n{row['problem']}\"))\n",
    "    display(Markdown(f\"### Solution\\n{row['solution']}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parse_boxed_answer(text):\n",
    "    matches = re.findall(r'\\\\boxed\\{(\\d+)\\}', text)\n",
    "    if matches:\n",
    "        return text_to_int_answer(matches[-1])\n",
    "    return None\n",
    "\n",
    "def safe_parse_boxed_answer(text):\n",
    "    matches = re.findall(r'\\\\boxed\\{(\\d+)\\}', text)\n",
    "    if len(matches) == 1:\n",
    "        return text_to_int_answer(matches[0])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mnt/hdd0/Kaggle/aimo/external_data/MATH_and_GSM8k.csv')\n",
    "print(len(df))\n",
    "df = df[df.source == 'MATH']\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_problem(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with integer answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_boxed'] = df.solution.str.count(r'\\\\boxed')\n",
    "df['n_boxed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.n_boxed > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_problem(65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should avoid problems with more than one solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['parsed_answer'] = df.solution.apply(safe_parse_boxed_answer)\n",
    "len(df), len(df[df.parsed_answer.notnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.parsed_answer.notnull()]\n",
    "df = df[df.n_boxed == 1]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_problem(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input_tokens'] = df.problem.apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "df['output_tokens'] = df.solution.apply(lambda x: len(tokenizer.tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 2000, 500)\n",
    "for key in ['input_tokens', 'output_tokens']:\n",
    "    plt.hist(df[key], bins=bins, alpha=0.5, label=key, density=True, cumulative=1)\n",
    "plt.xlim(0, 1000)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 90% of the solutions have less than 400 tokens.\n",
    "\n",
    "Let's see if there is any relation between the difficulty and token length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0, 2000)\n",
    "for key in ['input_tokens', 'output_tokens']:\n",
    "    for level in sorted(df.level.unique())[:-1]:\n",
    "        plt.hist(df[df.level == level][key], bins=bins, alpha=0.5, label=level, density=True, cumulative=1)\n",
    "    if key == 'input_tokens':\n",
    "        plt.xlim(0, 200)\n",
    "    else:\n",
    "        plt.xlim(0, 1000)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid()\n",
    "    plt.legend(loc=0);\n",
    "    plt.title(f'{key} distribution')\n",
    "    plt.xlabel('Tokens')\n",
    "    plt.ylabel('Cumulative probability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the more difficult problems have longer answers and descriptions. Very beautiful graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.stage == 'test']['level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to save the file for later being able to use it for creating few-shot prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_tokens'] = df.input_tokens + df.output_tokens\n",
    "df.columns = [column.replace('parsed_', '') for column in df.columns]\n",
    "df.sort_values('stage', inplace=True)\n",
    "df.sort_values('level', inplace=True)\n",
    "df['id'] = np.arange(len(df))\n",
    "df['answer'] = df['answer'].astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.answer % 1000).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/mnt/hdd0/Kaggle/aimo/external_data/filtered_MATH.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in ['train', 'test']:\n",
    "    df[df.stage == stage].to_csv(f'/mnt/hdd0/Kaggle/aimo/external_data/filtered_MATH_{stage}.csv', index=False)\n",
    "    print(stage, len(df[df.stage == stage]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More versions of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df.stage == 'test']\n",
    "test = test[test.level.isin(['Level 4', 'Level 5'])]\n",
    "test = test[test.problem.apply(lambda x: '[asy]' not in x)]\n",
    "test = test[test.total_tokens < 1000]\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1211 samples after filtering, previous test set was 2828 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sort_values(['level', 'type'], inplace=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('/mnt/hdd0/Kaggle/aimo/external_data/filtered_MATH_test_45.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test.level.isin(['Level 5'])]\n",
    "test.to_csv('/mnt/hdd0/Kaggle/aimo/external_data/filtered_MATH_test_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MathCodeInstruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/MathLLM/MathCodeInstruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(filepath):\n",
    "    data = []\n",
    "    # Open the file and read line by line\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Strip out any extra whitespace and parse the JSON object\n",
    "            json_obj = json.loads(line.strip())\n",
    "            # Append the parsed JSON object to the data list\n",
    "            data.append(json_obj['messages'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_answer(sample):\n",
    "    text = ''\n",
    "    for content in sample[2]['content'][:-1]: # the last is the response\n",
    "        if content['type'] == 'text':\n",
    "            text += uniform_latex_format(content['content'])\n",
    "        elif content['type'] == 'code':\n",
    "            text += f'\\n\\n```python\\n{content[\"content\"]}\\n``````output\\n'\n",
    "        elif content['type'] == 'execution':\n",
    "            text += f'{content[\"content\"]}\\n```\\n\\n'\n",
    "        else:\n",
    "            raise ValueError(f'Unknown content type: {content[\"type\"]}')\n",
    "    return text\n",
    "\n",
    "def get_problem(sample):\n",
    "    return sample[1]['content'][0]['content']\n",
    "\n",
    "def get_result(sample):\n",
    "    return sample[2]['content'][-1]['content']\n",
    "\n",
    "def uniform_latex_format(text):\n",
    "    text = text.replace('\\\\[', '$$').replace('\\\\]', '$$')\n",
    "    text = text.replace('\\\\(', '$').replace('\\\\)', '$')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_jsonl('/mnt/hdd0/Kaggle/aimo/external_data/MathCodeInstruct/train_80k.jsonl')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for sample in tqdm(data):\n",
    "    rows.append(dict(problem=get_problem(sample), solution=format_answer(sample), answer=get_result(sample)))\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/mnt/hdd0/Kaggle/aimo/external_data/MathCodeInstruct/train_80k.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.answer = df.answer.apply(text_to_int_answer)\n",
    "df = df[~df.answer.isna()]\n",
    "df.answer = df.answer.astype(int)\n",
    "print(len(df))\n",
    "df.to_csv('/mnt/hdd0/Kaggle/aimo/external_data/MathCodeInstruct/train_80k_int_answers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to associate this with the MATH dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_train = pd.read_csv('/mnt/hdd0/Kaggle/aimo/external_data/filtered_MATH_train.csv')\n",
    "math_train.drop_duplicates('problem', inplace=True)\n",
    "math_test = pd.read_csv('/mnt/hdd0/Kaggle/aimo/external_data/filtered_MATH_test.csv')\n",
    "math_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df[df.problem.isin(set(math_test.problem.values))]\n",
    "len(df_subset), len(df_subset.problem.unique()), len(math_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem of the test set is present in the dataset, that is weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df[df.problem.isin(set(math_train.problem.values))]\n",
    "len(df_subset), len(df_subset.problem.unique()), len(math_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have solutions for 3659/4354 train math problems. On average we have around 3 solutions for each problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math = df[df.problem.isin(set(math_train.problem.values))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_train\n",
    "math_train.set_index('problem', inplace=True)\n",
    "math_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['level', 'type', 'source', 'stage']:\n",
    "    df_math[key] = math_train.loc[df_math.problem.values][key].values\n",
    "df_math.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math['input_tokens'] = df_math.problem.apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "df_math['output_tokens'] = df_math.solution.apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "df_math['total_tokens'] = df_math['input_tokens'] + df_math['output_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math['id'] = df_math.index\n",
    "df_math.to_csv('/mnt/hdd0/Kaggle/aimo/external_data/MathCodeInstruct/MATH.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the duplicates, keeping the one with the shortest answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math.sort_values('total_tokens', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_math.total_tokens.values, bins=np.linspace(0, 5000, 100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math.drop_duplicates('problem', inplace=True)\n",
    "plt.hist(df_math.total_tokens.values, bins=np.linspace(0, 5000, 100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math.to_csv('/mnt/hdd0/Kaggle/aimo/external_data/MathCodeInstruct/MATH_no_duplicates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math[df_math.total_tokens < 1000].to_csv('/mnt/hdd0/Kaggle/aimo/external_data/MathCodeInstruct/MATH_no_duplicates_less1000tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math[df_math.total_tokens < 1000].to_csv('/mnt/hdd0/Kaggle/aimo/external_data/MathCodeInstruct/MATHCodeInstruct_curated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mnt/hdd0/Kaggle/aimo/external_data/MathCodeInstruct/MATH_no_duplicates_less1000tokens.csv')\n",
    "df.sort_values(['level', 'type'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0, 2000)\n",
    "for key in ['input_tokens', 'output_tokens']:\n",
    "    for level in sorted(df.level.unique())[:-1]:\n",
    "        plt.hist(df[df.level == level][key], bins=bins, alpha=0.5, label=level, density=True, cumulative=1)\n",
    "    if key == 'input_tokens':\n",
    "        plt.xlim(0, 200)\n",
    "    else:\n",
    "        plt.xlim(0, 1000)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid()\n",
    "    plt.legend(loc=0);\n",
    "    plt.title(f'{key} distribution')\n",
    "    plt.xlabel('Tokens')\n",
    "    plt.ylabel('Cumulative probability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_problem(3220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are many responses for each problem\n",
    "- Some problems do not have answer (maybe the difficult ones?)\n",
    "\n",
    "- [ ] Are there problems from the test set?\n",
    "- [ ] Is my method missing some problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Focus on problems that have integer and non-negative answers\n",
    "- [x] What is the distribution of output and input tokens?\n",
    "- [x] What if I create smaller versions of the test set?\n",
    "- [x] I might also filter long problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
