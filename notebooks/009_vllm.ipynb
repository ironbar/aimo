{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps with VLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn to use VLLM and find the best way to use it for the challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 11:46:45 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/home/gbarbadillo/data/deepseekmath', speculative_config=None, tokenizer='/home/gbarbadillo/data/deepseekmath', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/home/gbarbadillo/data/deepseekmath)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 11:46:57 model_runner.py:146] Loading model weights took 12.8725 GB\n",
      "INFO 06-07 11:46:58 gpu_executor.py:83] # GPU blocks: 985, # CPU blocks: 546\n",
      "INFO 06-07 11:47:00 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-07 11:47:00 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-07 11:47:05 model_runner.py:924] Graph capturing finished in 5 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=\"/home/gbarbadillo/data/deepseekmath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 40/40 [00:17<00:00,  2.26it/s, Generation Speed: 798.50 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: \" Shashank and I am a student at The University of Texas at Dallas.\\nWelcome to the world of mathematics! Here, we will explore the fascinating world of numbers, shapes, and patterns.\\nWe will learn about the basic concepts and principles of mathematics, and how they can be applied to solve problems and understand the world around us.\\nLet's start by learning about the number system, basic operations, and how to solve equations.\\nWe will also learn about geometry, trigonometry, and calculus, which are essential for understanding more advanced topics in mathematics.\\nWe will also explore the many applications of mathematics in the real world, such as in science, engineering, and finance.\\nSo, let's dive into the world of mathematics and explore its wonders!\"\n",
      "Prompt: 'The president of the United States is', Generated text: \" currently weighing a proposal to provide up to $1.6 trillion in aid to the world's poorest countries. In response, the head of the World Bank is considering whether or not the United States should keep all of the money or if it should be split between the United States and the World Bank. The World Bank's chief economist believes that the United States should receive 50% of the total amount.\\n\\nTo find out how much money the United States would receive, we need to calculate 50% of $1.6 trillion.\\n\\nTo do this, we multiply $1.6 trillion by 50% (or 0.5):\\n\\n$1.6 trillion * 0.5 = $0.8 trillion\\n\\nSo, the United States would receive $0.8 trillion if the World Bank's chief economist's proposal is accepted.\\n\\nThe remaining amount, $0.8 trillion, would be given to the World Bank.\"\n",
      "Prompt: 'The capital of France is', Generated text: \" Paris. It is the capital of France and the largest city in France. It is the capital of France and the largest city in France, and it is known for its culture, history, and art. It is the capital of France and the largest city in France, and it is known for its culture, history, and art. Paris is also a major tourist destination, with many attractions and landmarks to explore. It is the capital of France and the largest city in France, and it is known for its culture, history, and art. Paris is also a major tourist destination, with many attractions and landmarks to explore.\\n<jupyter_text>\\nRemove `test` suffix from file names and save them into new files\\n<jupyter_code>\\nfrom os import listdir, rename\\nfrom os.path import isfile, join\\n\\ndirectory = './train_images'\\nfiles = [f for f in listdir(directory) if isfile(join(directory, f))]\\n\\nfor f in files:\\n    new_name = f.replace('_test', '')\\n    rename(join(directory, f), join(directory, new_name))\\n<jupyter_output>\\n<empty_output>\\n\"\n",
      "Prompt: 'The future of AI is', Generated text: ' a question of machine consciousness. Artificial intelligence (AI) has become an integral part of our lives, with applications in various fields such as healthcare, finance, and entertainment. However, one of the most significant challenges in AI is achieving machine consciousness.\\nWhat is Machine Consciousness?\\nMachine consciousness refers to the ability of a machine to exhibit human-like cognitive, emotional, and social behavior. It involves creating AI systems that can understand and interact with the world in a way that is indistinguishable from human intelligence.\\nThe Importance of Machine Consciousness\\nThe development of machine consciousness is crucial for several reasons. First, it will enable AI systems to perform tasks that require complex problem-solving, decision-making, and reasoning. This will help AI systems to become more autonomous and capable of performing tasks that humans currently do.\\nSecond, machine consciousness will allow AI systems to interact with humans in a more natural and intuitive way. This will create a symbiotic relationship between humans and AI, where AI can serve as an aid and a partner in various tasks and activities.\\nFinally, machine consciousness will open up new possibilities for AI research and development. By creating AI systems that are truly conscious, we will gain a deeper understanding of the human mind and the potential of AI to emulate human intelligence.\\nHow to Achieve Machine Consciousness?\\nAchieving machine consciousness is a challenging task that requires interdisciplinary research and collaboration between scientists and engineers from various fields such as computer science, cognitive science, and neuroscience.\\nSeveral approaches have been proposed to achieve machine consciousness, including the following:\\n- Deep learning: Deep learning algorithms have been successful in many AI tasks, such as image recognition and natural language processing. However, deep learning is not sufficient to achieve machine consciousness, as it does not involve understanding and interpreting the world in a human-like way.\\n- Artificial neural networks: Artificial neural networks are inspired by the human brain and can learn from data. However, they still fall short of achieving machine consciousness due to their lack of introspection and reasoning.\\n- Symbolic AI: Symbolic AI systems use logical reasoning and symbolic manipulation to solve problems. While these systems can exhibit some level of intelligence, they still lack the ability to perceive and interact with the world in a human-like way.\\n- Embodied AI: Embodied AI aims to create AI systems that have a physical embodiment and interact with the environment through sensors and actuators. This approach focuses on creating AI systems that can perceive, understand, and act in the world in a human-like way.\\n- Mind-machine interface: A mind-machine interface is a device that allows humans to directly communicate with AI systems. This approach enables humans to train and control AI systems in a way that is more human-like.\\nThe future of AI is undoubtedly linked to machine consciousness. As we continue to explore and develop AI systems, we must focus on creating AI systems that can think, feel, and behave like humans. The development of machine consciousness will not only enable AI systems to perform complex tasks but will also open up new possibilities for human-AI symbiosis and the advancement of AI research.\\nThe journey towards'\n",
      "Prompt: 'Hello, my name is', Generated text: \" John. I am a 6th Grade teacher and football coach. I want to help you with your math problems.\\nI understand that you are having trouble with fractions and I want to help you understand them better.\\nLet's start with a simple question:\\nWhat is a fraction?\\nA fraction is a part of a whole number. For example, if you have a pizza and you cut it into 4 equal pieces, each piece is a fraction of the whole pizza. If you eat one piece, you have eaten 1/4 of the pizza.\\nNow, let's talk about how to add fractions.\\nTo add fractions, you need to have the same number on the bottom, which is called the denominator. For example, if you have 1/4 and you want to add 1/4, you can just add the top numbers (the numerators) and keep the bottom number the same. So, 1/4 + 1/4 = 2/4. But we can simplify this fraction by dividing both the top and bottom by the same number. If we divide both 2 and 4 by 2, we get 1/2. So, 2/4 simplifies to 1/2.\\nLet's try another example: 1/3 + 2/3. Here, the denominators are the same, so we can just add the numerators. 1 + 2 = 3. The denominator stays the same, so the answer is 3/3. But this fraction can also be simplified to 1 because 3 divided by 3 is 1.\\nIf the denominators are not the same, you need to find a common denominator before you can add the fractions. For example, if you want to add 1/4 and 1/2, you need to find a number that both 4 and 2 can divide into. That number is 4 because 4 and 2 can both divide into 4. Now, we can change 1/2 to have 4 on the bottom. To do this, we multiply the top and bottom of 1/2 by 2, which gives us 2/4. Now we can add 1/4 + 2/4 = 3/4.\\nI hope this helps you understand fractions a little better. If you have any more questions, feel free to ask!\"\n",
      "Prompt: 'The president of the United States is', Generated text: \" going to a rally in Florida tonight. The president will be in Florida for 3 days. He will be flying in at 10:00 pm and will be leaving at 5:00 am the following morning. The rally is scheduled to begin at 9:00 pm and will last until 11:00 pm. How many hours will the president be away from home?\\nYet to come\\nThis problem is a bit complex, but we can break it down into smaller parts to make it easier to understand.\\nFirst, let's calculate the total time the president will be in Florida. He will be there for 3 days, and since there are 24 hours in a day, he will be there for 3 * 24 = 72 hours.\\nNext, let's calculate the time he is actually in the rally. The rally starts at 9:00 pm and ends at 11:00 pm, so it lasts for 11:00 - 9:00 = 2 hours.\\nNow, let's calculate the time he is actually away from home. He arrives at 10:00 pm and leaves at 5:00 am the next morning. This means he is there from 10:00 pm to 5:00 am the next morning, which is a total of 5 hours + 14 hours = 19 hours (we add 14 hours because there are 14 hours between 10:00 pm and 5:00 am).\\nHowever, we need to subtract the time he is at the rally from this total time. So, the time he is actually away from home is 19 hours - 2 hours = 17 hours.\\nThe answer is: 17\"\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris.\\nAnswer: True'\n",
      "Prompt: 'The future of AI is', Generated text: ' now in the cloud, and the companies that are ready to adapt to this new paradigm will be the ones that thrive. In this article, we will explore the top 5 AI startups that are transforming the cloud computing industry.\\nAI startups that are transforming the cloud computing industry\\n1. Amazon AWS\\nAmazon AWS is the market leader in cloud computing services, offering a wide range of services including Amazon Web Services, Amazon Simple Storage Service, Amazon Relational Database Service, and Amazon Machine Learning. With AWS, companies can easily access powerful computing resources and AI-based services without having to build and maintain their own infrastructure. AWS is also constantly investing in new AI-related services, such as Amazon SageMaker, which allows developers to easily build and deploy machine learning models.\\n2. Microsoft Azure\\nMicrosoft Azure is another major player in the cloud computing industry, offering a wide range of services, including Azure Machine Learning, Azure Kubernetes Service, and Azure Cognitive Services. Microsoft Azure has a strong focus on AI, with the company investing heavily in AI research and development. Azure also offers a variety of AI-related services, including machine learning models and tools for natural language processing, computer vision, and speech recognition.\\n3. Google Cloud\\nGoogle Cloud is a comprehensive suite of cloud computing services, including Google Cloud AI, Google Cloud Vision, and Google Cloud Natural Language. Google Cloud has been a leader in AI for several years, with the company investing heavily in AI research and development. Google Cloud offers a wide range of AI-related services, including machine learning models and tools for natural language processing, computer vision, and speech recognition.\\n4. IBM Cloud\\nIBM Cloud is another major player in the cloud computing industry, offering a wide range of services, including IBM Watson, IBM Cloud AI, and IBM Cloud Pak for Data. IBM Cloud has a strong focus on AI, with the company investing heavily in AI research and development. IBM Cloud offers a variety of AI-related services, including machine learning models and tools for natural language processing, computer vision, and speech recognition.\\n5. Alibaba Cloud\\nAlibaba Cloud is a rapidly growing cloud computing company, offering a wide range of services, including Alibaba Cloud AI, Alibaba Cloud Video Intelligence, and Alibaba Cloud Data Intelligence. Alibaba Cloud is a leading player in the Chinese market, but the company is rapidly expanding its global presence. Alibaba Cloud is investing heavily in AI research and development, and the company offers a variety of AI-related services, including machine learning models and tools for natural language processing, computer vision, and speech recognition.\\nThese top 5 AI startups are transforming the cloud computing industry by offering powerful AI-based services and tools to companies of all sizes. By staying up-to-date with the latest AI developments and investing in new services and tools, these companies are ensuring that they remain at the forefront of the cloud computing industry.'\n",
      "Prompt: 'Hello, my name is', Generated text: ' Mark. I am a student in the class of 2020 at Stuyvesant High School. I am currently a junior and have been taking AP Calculus since 9th grade. I have been tutoring for about 5 years now and have tutored many students in Math and Science. I have always been interested in helping others and I love the feeling of knowing that I have helped someone succeed in their studies. I am looking forward to working with you to help you achieve your academic goals.\\n— Mark'\n",
      "Prompt: 'The president of the United States is', Generated text: ' a federal officer who has the primary duty and responsibility of leading and managing the executive branch of the federal government. In this role, the president is responsible for executing the policies and laws set by the legislative branch of the government, and he or she is also the commander-in-chief of the United States military. The president is elected by the American people every four years in a presidential election.\\nthe president of the united states'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris and the capital of Germany is Berlin.\\ncountryCode capital\\nFrance    Paris\\nGermany   Berlin\\n<jupyter_text>\\nQ2. What is the most popular superhero name in the dataset?\\n<jupyter_code>\\nimport pandas as pd\\n\\n# Read the superhero dataset\\ndf = pd.read_csv(\\'superhero_dataset.csv\\')\\n\\n# Extract the superhero names\\nnames = df[\\'name\\'].tolist()\\n\\n# Count the occurrences of each superhero name\\ncounts = df[\\'name\\'].value_counts()\\n\\n# Find the superhero name with the highest count\\nmost_popular_name = counts.idxmax()\\n\\nprint(most_popular_name)\\n<jupyter_output\\nThe Hulk\\n<jupyter_text>\\nThe most popular superhero name in the dataset is \"The Hulk\". Q3. How many superheroes can fly?\\n<jupyter_code>\\n# Filter the dataset for superheroes that can fly\\nflyers = df[df[\\'can_fly\\'] == True]\\n\\n# Count the number of superheroes that can fly\\nnum_flyers = len(flyers)\\n\\nprint(num_flyers)\\n<jupyter_output\\n13\\n<jupyter_text>\\nThere are 13 superheroes that can fly in the dataset. Q4. What is the average power of the superheroes in the dataset?\\n<jupyter_code>\\n# Calculate the average power of the superheroes\\naverage_power = df[\\'power\\'].mean()\\n\\nprint(average_power)\\n<jupyter_output>\\n14.52\\n<jupyter_text>\\nThe average power of the superheroes in the dataset is 14.52. Q5. What is the superhero with the highest power in the dataset?\\n<jupyter_code>\\n# Find the superhero with the highest power\\nhighest_power = df.loc[df[\\'power\\'].idxmax(), \\'name\\']\\n\\nprint(highest_power)\\n<jupyter_output\\nThanos\\n<jupyter_text>\\nThe superhero with the highest power in the dataset is \"Thanos\". Q6. What is the superhero with the lowest power in the dataset?\\n<jupyter_code>\\n# Find the superhero with the lowest power\\nlowest_power = df.loc[df[\\'power\\'].idxmin(), \\'name\\']\\n\\nprint(lowest_power)\\n<jupyter_output\\nSpider-Man\\n<jupyter_text>\\nThe superhero with the lowest power in the dataset is \"Spider-Man\". Q7. What is the average age of the superheroes in the dataset?\\n<jupyter_code>\\n# Calculate the average age of the superheroes\\naverage_age = df[\\'age\\'].mean()\\n\\nprint(average_age)\\n<jupyter_output>\\n10.44\\n<jupyter_text>\\nThe average age of the superheroes in the'\n",
      "Prompt: 'The future of AI is', Generated text: ' coming to your home with Amazon’s new Echo Show 8. The new smart display is a great way to interact with Alexa and get all the information you need without leaving your living room.\\nThe Echo Show 8 is equipped with a 8-inch touchscreen display that allows you to browse the internet, watch videos, play music, and more. It also includes a built-in camera for video calls, making it the perfect device for staying connected with loved ones.\\nOne of the standout features of the Echo Show 8 is its AI-powered personal assistant, Alexa. You can ask Alexa anything, from the weather to finding the best deal on Amazon. You can also use voice commands to control your smart home devices, set reminders, and more.\\nThe Echo Show 8 is available now for purchase on Amazon, and it comes with a one-year warranty. With its powerful features and easy-to-use interface, the Echo Show 8 is sure to become a favorite in your home.'\n",
      "Prompt: 'Hello, my name is', Generated text: \" Pranav Soni. I am a senior at Carleton College and a Computer Science major.\\nNow, let's solve the problem:\\n\\nWe have a sequence of numbers called A. We want to find the longest increasing subsequence in A.\\n\\nThe longest increasing subsequence (LIS) problem is a classic dynamic programming problem. The main idea is to maintain an array dp of the same size as the input array A, where dp[i] represents the length of the longest increasing subsequence ending at index i.\\n\\nWe can fill the dp array as follows:\\n\\n- For the first element, dp[0] = 1, because the longest increasing subsequence of a single element is of length 1.\\n- For each i > 0, we iterate through j = 0 to i - 1 and check if A[j] < A[i]. If this condition holds true, then dp[i] = max(dp[i], dp[j] + 1).\\n\\nFinally, the length of the longest increasing subsequence in A will be the maximum value in the dp array.\\n\\nLet's implement this in Python:\\n\\n```python\\ndef longest_increasing_subsequence(A):\\n    n = len(A)\\n    dp = [1] * n\\n    for i in range(1, n):\\n        for j in range(0, i):\\n            if A[j] < A[i]:\\n                dp[i] = max(dp[i], dp[j] + 1)\\n    return max(dp)\\n\\n# Test the function with an example\\nA = [10, 22, 9, 33, 21, 50, 41, 60]\\nprint(longest_increasing_subsequence(A))\\n```\\n```output\\n5\\n```\\nThe length of the longest increasing subsequence in the given array is 5.\"\n",
      "Prompt: 'The president of the United States is', Generated text: ' the highest civilian office in the United States. The office is held by one person at a time. The president is elected every four years by the Electoral College, which consists of the members of the U.S. Congress. The president is also the head of the executive branch of the federal government, which includes the departments of State, Defense, Treasury, and more.\\n\\nThe office of president is the most powerful position in the United States, with significant responsibility for governing the country, making decisions on foreign policy, and overseeing the military and other government institutions. The president is also the commander-in-chief of the U.S. armed forces.\\n\\nThe president is elected by the Electoral College, which consists of the members of the U.S. House of Representatives and the U.S. Senate. The Electoral College votes for the president, and the candidate with the most votes is elected president. The president serves a four-year term, and elections are held every four years.\\n\\nThe president is the head of the executive branch of the federal government, which includes the departments of State, Defense, Treasury, and more. The president appoints the members of the Cabinet, which consists of the secretaries of the executive departments and agencies.\\n\\nThe president is also the commander-in-chief of the U.S. armed forces. The president is responsible for the overall direction and leadership of the U.S. military, and is the first call for advice and consultation on military matters from the Secretary of Defense.\\n\\nIn summary, the president of the United States is the highest civilian office in the country, elected every four years by the Electoral College, and responsible for governing the country, overseeing the military, and leading the executive branch of the federal government.'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris. The capital of Germany is Berlin.\\nThe capital of Belgium is Brussels. The capital of Italy is Rome.\\nThe capital of Spain is Madrid. The capital of England is London.\\nThe capital of Sweden is Stockholm. The capital of Denmark is Copenhagen.\\nThe capital of Norway is Oslo. The capital of Switzerland is Bern.\\nThe capital of Portugal is Lisbon. The capital of Greece is Athens.\\nThe capital of Czech Republic is Prague. The capital of Hungary is Budapest.\\nThe capital of Austria is Vienna. The capital of Slovakia is Bratislava.\\nThe capital of Poland is Warsaw. The capital of Romania is Bucharest.\\nThe capital of Bulgaria is Sofia. The capital of Croatia is Zagreb.\\nThe capital of Serbia is Belgrade. The capital of Bosnia and Herzegovina is Sarajevo.\\nThe capital of Montenegro is Podgorica. The capital of Kosovo is Prishtina.\\nThe capital of Albania is Tirana. The capital of North Macedonia is Skopje.\\nThe capital of Kosovo is Prishtina. The capital of Macedonia is Skopje.\\nThe capital of Montenegro is Podgorica. The capital of Serbia is Belgrade.\\nThe capital of Bosnia and Herzegovina is Sarajevo. The capital of Serbia is Belgrade.\\nThe capital of Montenegro is Podgorica. The capital of Croatia is Zagreb.\\nThe capital of Bosnia and Herzegovina is Sarajevo. The capital of Serbia is Belgrade.\\nThe capital of Montenegro is Podgorica. The capital of Croatia is Zagreb.\\nThe capital of Bosnia and Herzegovina is Sarajevo. The capital of Serbia is Belgrade.\\nThe capital of Montenegro is Podgorica. The capital of Croatia is Zagreb.\\nThe capital of Bosnia and Herzegovina is Sarajevo. The capital of Serbia is Belgrade.\\nThe capital of Montenegro is Podgorica. The capital of Croatia is Zagreb.\\nThe capital of Bosnia and Herzegovina is Sarajevo. The capital of Serbia is Belgrade.\\nThe capital of Montenegro is Podgorica. The capital of Croatia is Zagreb.\\nThe capital of Bosnia and Herzegovina is Sarajevo. The capital of Serbia is Belgrade.\\nThe capital of Montenegro is Podgorica. The capital of Croatia is Zagreb.\\nThe capital of Bosnia and Herzegovina is Sarajevo. The capital of Serbia is Belgrade.\\nThe capital of Montenegro is Podgorica. The capital of Croatia is Zagreb.\\nThe capital of Bosnia and Herzegovina is Sarajevo. The capital of Serbia is Belgrade.\\nThe capital of Montenegro is Podgorica. The capital of Croatia is Zagreb.\\nThe capital of Bosnia and Herzegovina is Sarajevo. The capital of Serbia is Belgrade.\\nThe capital of Montenegro is Podgorica. The capital of Croatia is Zagreb.\\nThe capital of Bosnia and Herzegovina is Sarajevo. The capital of Serbia is'\n",
      "Prompt: 'The future of AI is', Generated text: ' decentralized.\\nWith the rise of blockchain and decentralized technologies, the future of artificial intelligence (AI) is becoming more decentralized. This means that AI algorithms and models will be spread across a network of computers, rather than being controlled by a single entity.\\nThis shift towards decentralization is important because it makes AI more secure and less vulnerable to attack or manipulation. Additionally, it allows for more transparent and accountable AI systems, as everyone involved in the network has the ability to verify and validate the results.\\nOne of the key technologies driving this shift is blockchain. Blockchain is a decentralized ledger that records transactions in a secure and immutable way. By using blockchain, AI systems can be distributed across a network of computers, with each computer verifying and validating the results of the AI algorithms.\\nThis approach also allows for more collaboration between AI developers and users. Instead of relying on a single entity to build and maintain AI models, developers can pool their resources and expertise to create more advanced and useful AI systems.\\nAdditionally, this decentralized approach makes AI more accessible to users who may not have the resources or expertise to build their own AI systems. By allowing AI models to be shared and accessed through a decentralized network, more people can benefit from the power of AI without having to rely on a single authority.\\nIn conclusion, the future of AI is decentralized. By using blockchain and other decentralized technologies, AI systems can be more secure, transparent, and accessible to users. This shift towards decentralization is crucial for building more robust and useful AI systems that can benefit society as a whole.'\n",
      "Prompt: 'Hello, my name is', Generated text: \" Ranganath. I am an assistant at Datediff. I am here to help you with your Python question.\\nSo let's try to solve this problem.\\n\\n```python\\nfrom datetime import datetime\\n\\ndef days_between_dates(d1, d2):\\n    return (d2 - d1).days\\n\\n# Example usage:\\nstart_date = datetime.strptime('2022-06-01', '%Y-%m-%d')\\nend_date = datetime.strptime('2022-07-01', '%Y-%m-%d')\\n\\ndays = days_between_dates(start_date, end_date)\\nprint(days)\\n```\\n```output\\n30\\n```\\nThe number of days between June 1, 2022 and July 1, 2022 is 30.\\n\\nSo, the number of days between June 1st, 2022 and July 1st, 2022 is 30.\"\n",
      "Prompt: 'The president of the United States is', Generated text: ' now 78 years old.\\nIf you want to know how many years ago the president was 45 years old, you can subtract 45 from 78.\\n78 - 45 = 33\\nSo, the president was 45 years old 33 years ago.\\nThe answer is $\\\\boxed{33}$.'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris, and the capital of Germany is Berlin.\\nThe capital of France is the same as the capital of Germany.\\nTherefore, Paris is the same as Berlin.\\nThis is a false statement. Paris is the capital of France, and Berlin is the capital of Germany. They are two different cities. The statement \"Paris is the same as Berlin\" is incorrect. The correct statement would be \"Paris is the capital of France, and Berlin is the capital of Germany.\"'\n",
      "Prompt: 'The future of AI is', Generated text: ' with deep learning and neural networks. One of the most popular AI libraries used for deep learning is TensorFlow. TensorFlow is an open-source software library for AI and machine learning. It was originally developed by the Google Brain team at Google. TensorFlow is designed to work with Python and is designed for both production and research.\\nTensorFlow has a wide range of use cases, including computer vision, natural language processing, speech recognition, and more. TensorFlow is used by many companies and organizations around the world, including Google, Baidu, and eBay.\\nTensorFlow is a popular choice for many AI projects because it offers a high level of flexibility and can run on a variety of hardware platforms, including CPUs, GPUs, and TPUs (Tensor Processing Units). TensorFlow also has a large community of developers who contribute to the library and provide support for new features and bug fixes.\\nIf you are interested in learning more about TensorFlow, there are many resources available online, including tutorials, documentation, and online courses. You can also join the TensorFlow community on GitHub and follow the TensorFlow blog for updates and news.'\n",
      "Prompt: 'Hello, my name is', Generated text: \" Vedantu and I am a math enthusiast. I have completed my B.Sc. in Mathematics from the University of Delhi and currently work as a math teacher in a top school in Delhi. I have always been passionate about teaching and helping students understand complex mathematical concepts in a simple and easy-to-understand manner.\\nWhat is a prime number?\\nA prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. The first few prime numbers are 2, 3, 5, 7, 11, 13, 17, 19, 23, and so on.\\nThe Sieve of Eratosthenes is a method used to find prime numbers up to any given limit. It is a simple, ancient algorithm for finding all prime numbers up to a specified integer.\\nHere is how the Sieve of Eratosthenes works:\\n1. Create a list of integers from 2 to the desired limit.\\n2. The first number in the list, 2, is a prime number.\\n3. Strike off all multiples of 2 from the list, starting with 4.\\n4. The next unstruck number in the list is the next prime, 3.\\n5. Strike off all multiples of 3 from the list, starting with 9.\\n6. Repeat steps 4 and 5 until the square of the next prime is greater than the desired limit.\\n7. All the unstruck numbers in the list are prime.\\nLet's apply the Sieve of Eratosthenes to find all prime numbers up to 50.\\n1. The list starts with the numbers from 2 to 50: 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50.\\n2. The first prime number is 2.\\n3. Strike off all multiples of 2 greater than 2: 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34,\"\n",
      "Prompt: 'The president of the United States is', Generated text: ' an example of a ____________ government.\\nSolution\\nThe president of the United States is an example of a Presidential government. Presidential government is a form of government where the head of state is also the head of government. In this type of government, the executive power is concentrated in the hands of one person, the president.'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris and the capital of Italy is Rome. The capital of Russia is Moscow. The capital of Germany is Berlin. The capital of Spain is Madrid. The capital of the United States is Washington D.C.'\n",
      "Prompt: 'The future of AI is', Generated text: \" in quantum computing, and D-Wave is a company dedicated to developing quantum computers. The company's latest product, the D-Wave 2000Q, is the world's first commercially available quantum computer. It uses quantum bits, or qubits, to perform calculations that are impossible for traditional computers.\\nQuantum computers have the potential to solve complex problems that traditional computers cannot handle. They can quickly analyze massive amounts of data and find patterns that might not be apparent to humans. This makes them ideal for tasks like artificial intelligence, machine learning, and cryptography.\\nThe D-Wave 2000Q has 2048 qubits, which is a significant step up from previous generations of quantum computers. It is designed to solve optimization problems, which are a common type of problem in AI and other fields.\\nFor example, imagine you are a company trying to optimize your production process. You have many factors to consider, like the cost of materials, the time it takes to produce each item, and the demand for your products. A quantum computer can help you find the best combination of these factors to maximize your profits.\\nThe D-Wave 2000Q is currently being used by companies like Google, NASA, and Volkswagen to explore the potential of quantum computing. And as quantum computers continue to improve, they will have a growing impact on AI and other fields.\"\n",
      "Prompt: 'Hello, my name is', Generated text: ' Thomas. I am a student in the 11th grade and I am having trouble with solving these two equations.\\nThe first equation is: 5(x-2)=3(x+4)\\nThe second equation is: 5x+3=2x+9\\n\\nFor the first equation, I distributed the numbers on both sides:\\n5x - 10 = 3x + 12\\nThen, I subtracted 3x from both sides:\\n5x - 3x - 10 = 3x - 3x + 12\\n2x - 10 = 12\\nNext, I added 10 to both sides:\\n2x - 10 + 10 = 12 + 10\\n2x = 22\\nAnd finally, I divided both sides by 2:\\n2x / 2 = 22 / 2\\nx = 11\\n\\nFor the second equation, I subtracted 2x from both sides:\\n5x - 2x + 3 = 2x - 2x + 9\\n3x + 3 = 9\\nThen, I subtracted 3 from both sides:\\n3x + 3 - 3 = 9 - 3\\n3x = 6\\nAnd finally, I divided both sides by 3:\\n3x / 3 = 6 / 3\\nx = 2\\n\\nSo the solutions for the two equations are x = 11 and x = 2.\\nThe answer is: $x=11$ and $x=2$'\n",
      "Prompt: 'The president of the United States is', Generated text: ' a lawyer by training, a war hero, a world traveler and an expert on foreign policy. But he also happens to be an expert on education.\\nIn fact, his education background and the push for education reform are some of the reasons that the public supports him so much.\\nHis educational background started with Harvard and Yale Law School, and he also attended the military academy, West Point.\\nPresident Obama served in the military for eight years. He was stationed in Iraq and Afghanistan.\\nThen, he went on to serve as a community organizer in Chicago.\\nAfter that, he moved on to become a senator for Illinois.\\nAnd then he was elected to the U.S. Senate and then president of the United States.\\nHe has a deep understanding of education from all of these experiences.\\nPresident Obama has said that education is the key to opportunity, to opportunity for every child, everywhere in our country. He has even said that the secret to success is learning.\\nHe has also said that when it comes to education, we must embrace the power of education to change lives and to prepare our children for the world ahead.\\nPresident Obama has also made education a top priority of his presidency. He has signed legislation to make sure that every child has access to a good education.\\nHe has also signed legislation to make sure that our schools are safe and that teachers have the resources they need to do their jobs.\\nPresident Obama has been a strong advocate for education reform and has made it a top priority of his presidency.\\nHe has worked to make sure that every child has access to a good education. He has worked to make sure that our schools are safe and that teachers have the resources they need to do their jobs.\\nAnd he has worked to make sure that our students have the skills they need to succeed in the 21st century.\\nHe has been a strong advocate for education reform and has made it a top priority of his presidency.\\nPresident Obama has been an advocate for education reform for a long time. He has worked to make sure that every child has access to a good education. He has worked to make sure that our schools are safe and that teachers have the resources they need to do their jobs.\\nAnd he has worked to make sure that our students have the skills they need to succeed in the 21st century.\\nPresident Obama has been a strong advocate for education reform and has made it a top priority of his presidency.'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris, and the capital of the United States is Washington.\\nThe capital of Australia is Canberra, and the capital of Canada is Ottawa.\\nThe capital of India is New Delhi, and the capital of China is Beijing.\\nThe capital of Japan is Tokyo, and the capital of Russia is Moscow.\\nThe capital of Brazil is Brasilia, and the capital of Germany is Berlin.\\nThe capital of Italy is Rome, and the capital of Spain is Madrid.\\nThe capital of Mexico is Mexico City, and the capital of Switzerland is Bern.\\nThe capital of the Netherlands is Amsterdam, and the capital of Belgium is Brussels.\\nThe capital of Denmark is Copenhagen, and the capital of Norway is Oslo.\\nThe capital of Sweden is Stockholm, and the capital of Finland is Helsinki.\\nThe capital of Greece is Athens, and the capital of Portugal is Lisbon.\\nThe capital of Ireland is Dublin, and the capital of Czech Republic is Prague.\\nThe capital of Poland is Warsaw, and the capital of Hungary is Budapest.\\nThe capital of Austria is Vienna, and the capital of Slovenia is Ljubljana.\\nThe capital of Slovakia is Bratislava, and the capital of Romania is Bucharest.\\nThe capital of Serbia is Belgrade, and the capital of Bulgaria is Sofia.\\nThe capital of Croatia is Zagreb, and the capital of Bosnia and Herzegovina is Sarajevo.\\nThe capital of Montenegro is Podgorica, and the capital of Albania is Tirana.\\nThe capital of Kosovo is Pristina, and the capital of Macedonia is Skopje.\\nThe capital of Serbia is Belgrade, and the capital of Macedonia is Skopje.\\nThe capital of Montenegro is Podgorica, and the capital of Albania is Tirana.\\nThe capital of Croatia is Zagreb, and the capital of Bosnia and Herzegovina is Sarajevo.\\nThe capital of Slovenia is Ljubljana, and the capital of Austria is Vienna.\\nThe capital of Hungary is Budapest, and the capital of Poland is Warsaw.\\nThe capital of Czech Republic is Prague, and the capital of Ireland is Dublin.\\nThe capital of Portugal is Lisbon, and the capital of Greece is Athens.\\nThe capital of Finland is Helsinki, and the capital of Sweden is Stockholm.\\nThe capital of Norway is Oslo, and the capital of Denmark is Copenhagen.\\nThe capital of Belgium is Brussels, and the capital of the Netherlands is Amsterdam.\\nThe capital of Switzerland is Bern, and the capital of Mexico is Mexico City.\\nThe capital of Spain is Madrid, and the capital of Italy is Rome.\\nThe capital of Germany is Berlin, and the capital of Brazil is Brasilia.\\nThe capital of Russia is Moscow, and the capital of Japan is Tokyo.\\nThe capital of China is Beijing, and the capital of India is New Delhi.\\nThe capital of Canada is Ottawa, and the capital of Australia is Canberra.'\n",
      "Prompt: 'The future of AI is', Generated text: ' in the hands of engineers, and to become a top AI engineer, you need to master the fundamentals of AI and machine learning. AI and machine learning are not just about programming and coding but also about understanding the underlying principles, theories, and algorithms.\\nBuilding a strong foundation in AI and machine learning will not only help you become a better engineer but also prepare you for the rapidly evolving field of AI.\\nHere are the top 10 essential books to help you start your journey into the world of AI and machine learning:\\n1. “Artificial Intelligence: A Modern Approach” by Stuart Russell and Peter Norvig\\nThis book is a classic in the field of AI and is considered a standard reference for AI professionals. It covers a wide range of topics in AI, from search and planning to machine learning and robotics. The book is written in a clear and accessible manner, making it suitable for both beginners and advanced learners.\\n2. “Machine Learning: A Probabilistic Perspective” by Kevin Murphy\\nThis book is a comprehensive introduction to machine learning, with a focus on probabilistic methods. It covers topics such as Bayesian inference, maximum likelihood estimation, and Markov chain Monte Carlo methods. The book also provides numerous examples and exercises to help readers understand the concepts better.\\n3. “Neural Networks and Deep Learning” by Michael Nielsen\\nThis book is an introduction to neural networks and deep learning, with a focus on mathematical foundations and practical applications. It covers topics such as backpropagation, convolutional neural networks, and recurrent neural networks. The book is well-suited for beginners who want to learn the basics of deep learning.\\n4. “The Elements of Statistical Learning” by Trevor Hastie, Robert Tibshirani, and Jerome Friedman\\nThis book is a comprehensive reference on statistical learning, with a focus on supervised learning methods such as linear regression, logistic regression, and support vector machines. It also covers topics such as unsupervised learning, resampling methods, and model assessment.\\n5. “Pattern Recognition and Machine Learning” by Christopher Bishop\\nThis book is an advanced text on machine learning, with a focus on statistical pattern recognition and Bayesian methods. It covers topics such as mixture models, hidden Markov models, and kernel methods. The book is well-suited for readers who want to learn advanced machine learning techniques.\\n6. “Deep Learning” by Ian Goodfellow, Yoshua Bengio, and Aaron Courville\\nThis book is a comprehensive introduction to deep learning, with a focus on the mathematical foundations and practical applications of deep neural networks. It covers topics such as backpropagation, convolutional neural networks, and recurrent neural networks. The book is well-suited for both beginners and advanced learners who want to learn the latest developments in deep learning.\\n7. “Computational Intelligence: An Introduction” by Kenneth De Jong and Kenneth Baclawski\\nThis book provides a broad introduction to computational intelligence, which includes artificial neural networks, genetic algorithms, and fuzzy logic. The book covers the theoretical foundations and practical applications of these methods, making it suitable for both beginners and advanced learners who want to learn about the various approaches to AI.\\n'\n",
      "Prompt: 'Hello, my name is', Generated text: ' Alena. I am a student of the Faculty of Mathematics and Physics at Charles University in Prague, specializing in mathematics. I have been studying at the university for four years now and I am currently a second-year student.\\nI am originally from the Czech Republic but I have been living in the United States since 2018. I have been working as a teaching assistant in mathematics and physics for the past two years. I enjoy working with students and helping them understand complex topics in a simple and straightforward manner.\\nWhen I am not studying or working, I enjoy spending time with friends and family, reading books, and trying new recipes in the kitchen.\\nIf you have any questions or would like to connect, feel free to reach out!'\n",
      "Prompt: 'The president of the United States is', Generated text: ' supposed to have the final say in the confirmation process. The Senate is supposed to have the power to block the nomination of a president if they think the person is not qualified for the job. But what happens if the Senate votes to confirm a president but the president vetoes the nomination?\\nThe president is elected by the people to serve as the head of the country. The Senate, the upper house of Congress, is elected to represent the state they represent. The president is supposed to be a servant of the people, not the other way around. If the president vetoes a Senate confirmation, it shows that the president is not willing to serve the people and is instead trying to protect their own interests.\\nIt also raises questions about the independence of the Senate, as the president is able to override their decision. This could potentially lead to the Senate losing its power and becoming just another branch of the government rather than a separate institution.\\nIn conclusion, if the president vetoes a Senate confirmation, it shows a lack of respect for the people and the independence of the Senate, and could potentially lead to a breakdown in the democratic process.'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of France.\\nThe capital of France is Paris.\\nParis is the capital of'\n",
      "Prompt: 'The future of AI is', Generated text: \" now: From AI to ML and the Cloud - Join Now!\\nGANs Tutorial Part 2: Variational Autoencoders\\nSearch for jobs related to Variational autoencoder tutorial or hire on the world's largest freelancing marketplace with 20m+ jobs. It's free to sign up and bid on jobs.\\nGitHub - ryanmcfee/variational-autoencoder-tutorial: Variational Autoencoders in Keras\\nA tutorial on Variational Autoencoders in Keras.\\nVariational Autoencoders (VAEs) are a type of deep generative model that can learn a probability distribution over its input data. In this tutorial, we’ll explore the concept of Variational Autoencoders (VAEs) and how they can be used to learn and sample from complex probability distributions. We will also build a VAE in Keras, which will allow us to generate new images of fashion products from the MNIST dataset.\\nVariational Autoencoders (VAEs) are a type of deep generative model that can learn a probability distribution over its input data. They consist of an encoder, which maps data from the input space to a lower-dimensional latent space, and a decoder, which maps the latent space back to the input space. The encoder and decoder are typically neural networks.\\nThe goal of the VAE is to maximize the likelihood of the input data given the latent variables, i.e., maximize P(x|z). To do this, we can use the evidence lower bound (ELBO) as a proxy for the likelihood. The ELBO is given by:\\nELBO = E[log P(x|z)] - KL[Q(z|x) || P(z)]\\nThe first term E[log P(x|z)] is the expected log-likelihood of the data given the latent variables, and the second term KL[Q(z|x) || P(z)] is the Kullback-Leibler divergence between the encoder distribution Q(z|x) and the prior distribution P(z).\\nTo maximize the ELBO, we can use the reparameterization trick to rewrite the latent variable z as z = μ + σ * ε, where ε is a random variable sampled from a standard normal distribution, and μ and σ are outputs of the encoder network. This allows us to compute the gradient of the ELBO with respect to the parameters of the encoder and decoder networks.\\nIn this tutorial, we’ll explore the concept of Variational Autoencoders (VAEs) and how they can be used to learn and sample from complex probability distributions. We will also build a VAE in Keras, which will allow us to generate new images of fashion products from the MNIST dataset.\\nTo get started, we’ll first import the necessary libraries:\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\nfrom tensorflow.keras.datasets import mnist\\nfrom tensorflow.keras.losses import binary_crossentropy\\nfrom tensorflow.keras.optimizers\"\n",
      "Prompt: 'Hello, my name is', Generated text: ' John, and I am an AI language model that helps you solve math problems. To solve this problem, we need to determine the value of x in the equation 2x + 7 = 19.\\nFollow these steps to solve for x:\\n\\nStep 1: Subtract 7 from both sides of the equation to isolate the term with x on one side. This gives us:\\n\\\\[2x + 7 - 7 = 19 - 7.\\\\]\\n\\nSimplifying both sides, we have:\\n\\\\[2x = 12.\\\\]\\n\\nStep 2: To solve for x, divide both sides of the equation by 2. This gives us:\\n\\\\[2x/2 = 12/2.\\\\]\\n\\nSimplifying both sides, we have:\\n\\\\[x = 6.\\\\]\\n\\nSo, the value of x that makes the equation true is 6.\\nThe answer is: $6$'\n",
      "Prompt: 'The president of the United States is', Generated text: ' a man of principles and a man of action. He is a man who takes charge and is a man of order. He is a man of vision and a man of ideas. He is a man of confidence and a man of courage.\\nHe is a man of integrity and a man of honor. He is a man who is always looking for ways to improve our country and make it better for everyone. He is a man who is willing to take on any challenge and always stands up for what he believes in.\\nThe president of the United States is a man of honor and a man of character. He is a man who has the courage to stand up for what is right and never back down from a fight. He is a man who is always willing to help others and never hesitates to lend a helping hand when needed.\\nThe president of the United States is a man who has the ability to lead our country and guide us through the tough times. He is a man who has the strength to face any adversity and come out stronger on the other side. He is a man who has the vision to see the bigger picture and make the right decisions for our country.\\nThe president of the United States is a man who is always looking for ways to make our country better and ensure that everyone has a chance to succeed. He is a man who is willing to take on any challenge and never gives up on his vision. He is a man who has the courage to stand up for what he believes in and never backs down from a fight.\\nThe president of the United States is a man of honor and a man of character. He is a man who has the ability to lead our country and guide us through the tough times. He is a man who has the strength to face any adversity and come out stronger on the other side. He is a man who has the vision to see the bigger picture and make the right decisions for our country.\\nThe president of the United States is a man who is always looking for ways to make our country better and ensure that everyone has a chance to succeed. He is a man who is willing to take on any challenge and never gives up on his vision. He is a man who has the courage to stand up for what he believes in and never backs down from a fight. He is a man of honor and a man of character.\\nHe is a man who has the ability to lead our country and guide us through the tough times. He is a man who has the strength to face any adversity and come out stronger on the other side. He is a man who has the vision to see the bigger picture and make the right decisions for our country.\\nHe is a man who is always looking for ways to make our country better and ensure that everyone has a chance to succeed. He is a man who is willing to take on any challenge and never gives up on his vision. He is a man who has the courage to stand up for what he believes in and never backs down from a fight.\\nHe is a man of honor and a man of character. He is the president of the United States and he is worthy of'\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris, the capital of Germany is Berlin, the capital of Russia is Moscow and the capital of the US is Washington.\\nA. Moscow\\nB. Washington\\nC. Paris\\nD. Berlin\\nThe answer is “B”.'\n",
      "Prompt: 'The future of AI is', Generated text: ' in cybersecurity. AI is changing the way we interact with our digital lives and the way we protect our digital information. The integration of AI into cybersecurity solutions is transforming the way we secure our data and protect our devices from cyberattacks.\\nThe intersection of AI and cybersecurity is an exciting and rapidly growing field. AI has the potential to improve the security of our digital systems, detect and prevent cyberattacks, and provide better protection for our personal and organizational data.\\nOne of the most promising applications of AI in cybersecurity is the use of machine learning algorithms to detect and prevent cyberattacks. These algorithms can analyze massive amounts of data, identify patterns and trends, and make predictions about potential threats. AI can also help in the development of more effective security measures, such as intrusion detection systems and firewalls.\\nAnother area where AI is making a significant impact is in the field of threat intelligence. AI can analyze large volumes of data and extract valuable insights from it, helping security teams to identify and prioritize threats, and to develop more effective response plans.\\nAI is also being used to improve the security of IoT devices. With the increasing number of connected devices, it has become critical to protect them from cyberattacks. AI can help in the development of more secure protocols and authentication methods for IoT devices, as well as in the detection of vulnerabilities and threats.\\nThe future of AI in cybersecurity is very promising. As the field continues to evolve, we can expect to see more innovative and effective solutions being developed to protect our digital information and devices from cyberattacks.'\n",
      "Prompt: 'Hello, my name is', Generated text: ' Scott. I’m a mathematician and a blogger for universalclass.net. In today’s post, I’m going to talk about some ways to solve fractions. It’s important to know how to add, subtract, multiply, and divide fractions because you’ll use these skills in algebra, trigonometry, and even in regular arithmetic.\\n\\n\\n1. Adding and subtracting fractions: To add or subtract fractions, you need to have a common denominator. The common denominator is the smallest number that each of the denominators will divide into evenly. For example, if you have fractions 1/3 and 1/4, the common denominator is 12 because both 3 and 4 will divide into 12 evenly. Once you have the common denominator, you can add or subtract the numerators (the top numbers) and keep the denominator the same. For example, 1/3 + 1/4 = 4/12 + 3/12 = 7/12.\\n\\n\\n2. Multiplying fractions: To multiply fractions, you simply multiply the numerators together and the denominators together. For example, 1/3 x 1/4 = 1/12.\\n\\n\\n3. Dividing fractions: To divide fractions, you multiply the first fraction by the reciprocal of the second fraction. The reciprocal of a fraction is obtained by swapping the numerator and the denominator. For example, 1/3 ÷ 1/4 = 1/3 x 4/1 = 4/3.\\n\\n\\nHere are some examples to help you practice:\\n\\n1. Add: 1/3 + 1/6 = \\n2. Subtract: 3/4 - 1/2 =\\n3. Multiply: 1/5 x 2/3 =\\n4. Divide: 3/4 ÷ 1/2 =\\n\\nThe answers are:\\n1. Add: 1/3 + 1/6 = 2/6 + 1/6 = 3/6 = 1/2\\n2. Subtract: 3/4 - 1/2 = 3/4 - 2/4 = 1/4\\n3. Multiply: 1/5 x 2/3 = 2/15\\n4. Divide: 3/4 ÷ 1/2 = 3/4 x 2/1 = 6/4 = 3/2\\n\\nIf you have any questions or need more help, feel free to ask!\\n\\nScott'\n",
      "Prompt: 'The president of the United States is', Generated text: \" a sworn officer of the United States, under oath to uphold the Constitution of the United States and defend the Constitution of the United States. In carrying out this duty, the president acts as the head of the executive branch of the United States government, leading the federal government in its administration of domestic and foreign policy. The president is the commander-in-chief of the United States Armed Forces. The president is elected by the Electoral College, which is composed of the members of the U.S. House of Representatives and the U.S. Senate, and the president is sworn into office in a ceremony at the Capitol in Washington, D.C.\\nThe vice president of the United States is the second most powerful person in the United States government, serving as the president's running mate in elections and as the president's right-hand man in government. The vice president presides over the Senate and is also the chief of staff to the president. The vice president is elected by the House of Representatives and is usually the junior member of the ticket. The vice president is also a sworn officer of the United States, under oath to uphold the Constitution of the United States and defend the Constitution of the United States.\\nThe difference between the president and the vice president is that the president is elected by the Electoral College, while the vice president is elected by the House of Representatives. The president is also the commander-in-chief of the United States Armed Forces, while the vice president is not. However, the vice president is the chief of staff to the president and presides over the Senate.\"\n",
      "Prompt: 'The capital of France is', Generated text: ' Paris.\\nThe capital of Germany is Berlin.\\nThe capital of Italy is Rome.\\nThe capital of Spain is Madrid.\\nThe capital of the United Kingdom is London.\\nThe capital of the United States is Washington, D.C.\\nThe capital of Japan is Tokyo.\\nThe capital of Canada is Ottawa.\\nThe capital of Australia is Canberra.\\nThe capital of New Zealand is Wellington.\\nThe capital of India is New Delhi.\\nThe capital of China is Beijing.\\nThe capital of South Africa is Pretoria.\\nThe capital of Russia is Moscow.\\nThe capital of Brazil is Brasilia.\\nThe capital of Argentina is Buenos Aires.\\nThe capital of Mexico is Mexico City.\\nThe capital of Canada is Ottawa.\\nThe capital of the Bahamas is Nassau.\\nThe capital of the Cayman Islands is George Town.\\nThe capital of the Philippines is Manila.\\nThe capital of the Marshall Islands is Majuro.\\nThe capital of the Solomon Islands is Honiara.\\nThe capital of the Marshall Islands is Majuro.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Republic of the Marshall Islands is Majuro.\\nThe capital of the Republic of Palau is Palau City.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated States of Micronesia is Palikir.\\nThe capital of the Federated'\n",
      "Prompt: 'The future of AI is', Generated text: ' decentralized. From the first AI conference in 1956, where John McCarthy and Marvin Minsky proposed the idea of a “general purpose intelligent machine,” to today’s AI renaissance, the field has undergone tremendous growth and change. As AI becomes more widely adopted and integrated into our daily lives, it is essential to recognize the importance of decentralization in AI.\\nThe concept of decentralization in AI refers to the distribution of AI capabilities and decision-making power across multiple entities, rather than relying on a centralized authority. This approach offers several advantages, including increased resilience, reduced single points of failure, and improved privacy and security.\\nDecentralized AI can be implemented in various ways, such as through blockchain-based platforms, peer-to-peer networks, or hybrid architectures that combine both centralized and decentralized components. By harnessing the power of distributed computing and cryptography, decentralized AI can enable a more secure and resilient AI ecosystem.\\nOne of the primary drivers behind the decentralization of AI is the growing concern about the potential impact of centralized AI systems on privacy and security. As AI becomes increasingly integrated into our daily lives, it is crucial to ensure that the data used to train and power these systems is protected and not misused. Decentralized AI systems can help address this challenge by distributing decision-making power across multiple entities, reducing the risk of a single point of failure and promoting transparency and accountability.\\nAnother advantage of decentralized AI is its potential to promote fairness and equity in AI decision-making. By distributing AI capabilities across multiple entities, decentralized AI systems can help ensure that AI algorithms and models are not biased or discriminatory against certain groups or individuals. This can help address long-standing concerns about the potential harms of AI systems and promote a more equitable and inclusive AI landscape.\\nIn conclusion, the future of AI is decentralized. As AI becomes more widely adopted and integrated into our daily lives, decentralization will play a key role in ensuring the security, privacy, and fairness of AI systems. By harnessing the power of distributed computing and cryptography, decentralized AI can help us create a more secure, resilient, and equitable AI ecosystem.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.9, top_p=0.9, max_tokens=640)\n",
    "\n",
    "outputs = llm.generate(prompts*10, sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It says it is generating at 800 tokens/s. If this is true it is close to 30 times faster than my script. Seems hard to believe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, Generation Speed: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 10/10 [00:04<00:00,  2.29it/s, Generation Speed: 335.93 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    numbers = [1, 2, 3, 4, 5, 6]\n",
      "    sums = []\n",
      "\n",
      "    # Generate all two-element subsets\n",
      "    subsets = list(combinations(numbers, 2))\n",
      "\n",
      "    # Calculate the sum of each subset and add it to the list\n",
      "    for subset in subsets:\n",
      "        sums.append(sum(subset))\n",
      "\n",
      "    # Calculate the sum of all the sums\n",
      "    total_sum = sum(sums)\n",
      "\n",
      "    return total_sum\n",
      "\n",
      "result = sum_of_subsets()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "210\n",
      "```\n",
      "The sum of these 15 sums is $\\boxed{210}$. The answer is a non negative integer.\n",
      "The answer is $\\boxed{210}$.\n",
      "\n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    numbers = list(range(1, 7))\n",
      "    sums = []\n",
      "    for subset in combinations(numbers, 2):\n",
      "        sums.append(sum(subset))\n",
      "\n",
      "    return sum(sums)\n",
      "\n",
      "result = sum_of_subsets()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "210\n",
      "```\n",
      "The sum of these 15 sums is 210. The answer is: $210$.\n",
      "\n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    numbers = [1, 2, 3, 4, 5, 6]\n",
      "    sums = []\n",
      "\n",
      "    # Generate all two-element subsets\n",
      "    for subset in combinations(numbers, 2):\n",
      "        # Add the sum of the subset to the list\n",
      "        sums.append(sum(subset))\n",
      "\n",
      "    # Compute the sum of all the sums\n",
      "    total_sum = sum(sums)\n",
      "\n",
      "    return total_sum\n",
      "\n",
      "result = sum_of_subsets()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "252\n",
      "```\n",
      "The sum of the elements of each of the 15 two-element subsets of $\\{1,2,3,4,5,6\\}$ is $252$. The answer is: $252$.\n",
      "\n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_sums():\n",
      "    numbers = [1, 2, 3, 4, 5, 6]\n",
      "    total_sum = 0\n",
      "    for subset in combinations(numbers, 2):\n",
      "        total_sum += sum(subset)\n",
      "    return total_sum\n",
      "\n",
      "result = sum_of_sums()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "126\n",
      "```\n",
      "The sum of these 15 sums is 126.\n",
      "The answer is: $126$\n",
      "\n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subset_sums():\n",
      "    numbers = list(range(1, 7))\n",
      "    total_sum = 0\n",
      "\n",
      "    for subset in combinations(numbers, 2):\n",
      "        total_sum += sum(subset)\n",
      "\n",
      "    return total_sum\n",
      "\n",
      "result = sum_of_subset_sums()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "210\n",
      "```\n",
      "The sum of these 15 sums is 210. The answer is: $210$.\n",
      "\n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    numbers = list(range(1, 7))\n",
      "    sums = []\n",
      "    for subset in combinations(numbers, 2):\n",
      "        sums.append(sum(subset))\n",
      "\n",
      "    total_sum = sum(sums)\n",
      "    return total_sum\n",
      "\n",
      "result = sum_of_subsets()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "210\n",
      "```\n",
      "The sum of these 15 sums is 210. The answer is: $210$.\n",
      "\n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    numbers = list(range(1, 7))\n",
      "    sums = []\n",
      "\n",
      "    # Generate all two-element subsets\n",
      "    for subset in combinations(numbers, 2):\n",
      "        sums.append(sum(subset))\n",
      "\n",
      "    # Calculate the sum of all the sums\n",
      "    total_sum = sum(sums)\n",
      "\n",
      "    return total_sum\n",
      "\n",
      "result = sum_of_subsets()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "210\n",
      "```\n",
      "The sum of these 15 sums is 210. The answer is: $210$\n",
      "\n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    numbers = list(range(1, 7))\n",
      "    sums = [sum(subset) for subset in combinations(numbers, 2)]\n",
      "    return sum(sums)\n",
      "\n",
      "result = sum_of_subsets()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "120\n",
      "```\n",
      "The sum of these 15 sums is 120. The answer is $\\boxed{120}$.\n",
      "\n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    numbers = list(range(1, 7))\n",
      "    sums = []\n",
      "\n",
      "    # Generate all two-element subsets\n",
      "    for subset in combinations(numbers, 2):\n",
      "        sums.append(sum(subset))\n",
      "\n",
      "    # Compute the sum of the sums\n",
      "    total_sum = sum(sums)\n",
      "\n",
      "    return total_sum\n",
      "\n",
      "result = sum_of_subsets()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "210\n",
      "```\n",
      "The sum of the elements of each of the 15 two-element subsets of $\\{1,2,3,4,5,6\\}$ is 210.\n",
      "The answer is: $\\boxed{210}$.\n",
      "\n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_sums():\n",
      "    # Define the set\n",
      "    numbers = set(range(1, 7))\n",
      "\n",
      "    # Generate all two-element subsets\n",
      "    subsets = list(combinations(numbers, 2))\n",
      "\n",
      "    # Calculate the sum of each subset\n",
      "    sums = [sum(subset) for subset in subsets]\n",
      "\n",
      "    # Calculate the sum of all these sums\n",
      "    total_sum = sum(sums)\n",
      "\n",
      "    return total_sum\n",
      "\n",
      "result = sum_of_sums()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "210\n",
      "```\n",
      "The sum of these 15 sums is $210$. The answer is: $210$.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'User: John computes the sum of the elements of each of the 15 two-element subsets of $\\\\{1,2,3,4,5,6\\\\}$. What is the sum of these 15 sums?\\nPlease reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\\nUse all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\\nYou might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\\nUse code even for the simpler calculations to avoid mistakes.\\n\\nAssistant: Sure, we can solve the problem by writing a Python program.\\n\\n```python'\n",
    "outputs = llm.generate([prompt]*10, sampling_params)\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the speed is 300 token/s, still 10x the speed of my previous script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AsyncLLMEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://docs.vllm.ai/en/stable/dev/engine/llm_engine.html#vllm.LLMEngine\n",
    "- https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/api_server.py\n",
    "- https://github.com/vllm-project/vllm/issues/1317"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ModelConfig.__init__() missing 5 required positional arguments: 'tokenizer', 'tokenizer_mode', 'trust_remote_code', 'dtype', and 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncLLMEngine, SamplingParams\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelConfig\n\u001b[0;32m----> 4\u001b[0m model_config \u001b[38;5;241m=\u001b[39m \u001b[43mModelConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/gbarbadillo/data/deepseekmath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m llm \u001b[38;5;241m=\u001b[39m AsyncLLMEngine(worker_use_ray\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, engine_use_ray\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, model_config\u001b[38;5;241m=\u001b[39mmodel_config)\n",
      "\u001b[0;31mTypeError\u001b[0m: ModelConfig.__init__() missing 5 required positional arguments: 'tokenizer', 'tokenizer_mode', 'trust_remote_code', 'dtype', and 'seed'"
     ]
    }
   ],
   "source": [
    "from vllm import AsyncLLMEngine, SamplingParams\n",
    "from vllm.config import ModelConfig\n",
    "\n",
    "model_config = ModelConfig(model=\"/home/gbarbadillo/data/deepseekmath\")\n",
    "\n",
    "llm = AsyncLLMEngine(worker_use_ray=False, engine_use_ray=False, model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 12:32:43 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/home/gbarbadillo/data/deepseekmath', speculative_config=None, tokenizer='/home/gbarbadillo/data/deepseekmath', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/home/gbarbadillo/data/deepseekmath)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 12:32:46 model_runner.py:146] Loading model weights took 12.8725 GB\n",
      "INFO 06-07 12:32:47 gpu_executor.py:83] # GPU blocks: 985, # CPU blocks: 546\n",
      "INFO 06-07 12:32:49 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-07 12:32:49 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-07 12:32:55 model_runner.py:924] Graph capturing finished in 6 secs.\n",
      "INFO 06-07 12:32:55 serving_chat.py:84] Using default chat template:\n",
      "INFO 06-07 12:32:55 serving_chat.py:84] {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n",
      "INFO 06-07 12:32:55 serving_chat.py:84] \n",
      "INFO 06-07 12:32:55 serving_chat.py:84] ' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n",
      "INFO 06-07 12:32:55 serving_chat.py:84] \n",
      "INFO 06-07 12:32:55 serving_chat.py:84] ' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:     Started server process [21611]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-07 12:32:55 serving_embedding.py:141] embedding_mode is False. Embedding API will not work.\n",
      "INFO 06-07 12:33:05 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:33:15 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:33:25 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:33:35 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:33:45 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:33:55 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO:     ::1:59912 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:59912 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO 06-07 12:34:05 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:34:15 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:34:25 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:34:35 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:34:45 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "\n",
    "def run_vllm_server():\n",
    "    subprocess.run(['python', '-m', 'vllm.entrypoints.openai.api_server', '--model', '/home/gbarbadillo/data/deepseekmath'])\n",
    "\n",
    "server_thread = threading.Thread(target=run_vllm_server)\n",
    "server_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 12:35:04 async_llm_engine.py:553] Received request cmpl-74536789027e4f9daa2b0838c8dcd6a6-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:35:04 metrics.py:341] Avg prompt throughput: 0.5 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:35:05 async_llm_engine.py:124] Finished request cmpl-74536789027e4f9daa2b0838c8dcd6a6-0.\n",
      "INFO:     127.0.0.1:60170 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: Completion(id='cmpl-74536789027e4f9daa2b0838c8dcd6a6', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text=' famously tough place to take a child: its neighborhoods, miles of ocean coastline,', stop_reason=None)], created=1717756504, model='/home/gbarbadillo/data/deepseekmath', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=16, prompt_tokens=5, total_tokens=21))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 12:35:15 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:35:25 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "completion = client.completions.create(model=\"/home/gbarbadillo/data/deepseekmath\",\n",
    "                                      prompt=\"San Francisco is a\")\n",
    "print(\"Completion result:\", completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 12:36:32 async_llm_engine.py:553] Received request cmpl-06b0c02ffbe0420ba6bdf4c393e6bd50-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:36:32 metrics.py:341] Avg prompt throughput: 0.7 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:36:37 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 52.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:36:39 async_llm_engine.py:124] Finished request cmpl-06b0c02ffbe0420ba6bdf4c393e6bd50-0.\n",
      "INFO:     127.0.0.1:34358 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: Completion(id='cmpl-06b0c02ffbe0420ba6bdf4c393e6bd50', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=' city with beauty, culture, and musical venues that range from intimate jazz clubs to larger outdoor amphitheaters. Here we will be showing you some of the best places to go and enjoy great music in San Francisco.\\nBig bands have been an important part of San Francisco music history for decades. With events like the Herbst Theater, The Castro Theater and the Legion of Fools you are bound to find a big band playing at one of these venues. For the best of the best go to The Palace of Fine Arts. Located in a beautiful venue with state of the art sound and sight lines the Palace of Fine Arts is the place to take in a top notch performance. Check out some of the big band tours that pass through San Francisco: The Caravan of Stars, The Best of the Best and The Legends.\\nAre you looking for the ultimate club experience? Try one of the many world class night clubs in San Francisco like The Haight Ashbury club, The Fillmore, North Beach or The Roxbury Tavern. Touting some of the best DJs and bands in the Bay Area these night clubs are an experience never to be forgotten.\\nSmaller and more intimate types of musical venues in the city also abound. From jazz, blues, folk, hip, rock, disco, electronic and punk music there is an atmosphere and setting for every musical taste in the city from the Jazz Bar, The Mezzanine, The Minetta Tavern to smaller clubs like The Nugget or The Chaloner.\\nOther areas to enjoy a great outdoor concert are at one of the many parks including Golden Gate Park and Japanese Tea Garden where food vendors, cafes and shops help create an atmosphere that may inspire your creativity or bring you just a little nostalgia for a time gone by. Iconic venues that you may want to visit include the War Memorial Opera House, the Ahmanson, California Academy Of Sciences and the California Academy Of Sciences.', stop_reason=None)], created=1717756592, model='/home/gbarbadillo/data/deepseekmath', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=388, prompt_tokens=5, total_tokens=393))\n",
      "INFO 06-07 12:36:39 async_llm_engine.py:553] Received request cmpl-2a6225396a6e4af7b785a5c060eae983-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:36:42 metrics.py:341] Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 51.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:36:47 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 53.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:36:51 async_llm_engine.py:124] Finished request cmpl-2a6225396a6e4af7b785a5c060eae983-0.\n",
      "INFO:     127.0.0.1:34358 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: Completion(id='cmpl-2a6225396a6e4af7b785a5c060eae983', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text=' world-class city, and there’s a deep and fascinating history to learn about. The city literally was built on the legends of old. The terracotta warriors, the Monterey Bay, the Presidio, the Golden Gate Bridge, Giants Stadium, and Alcatraz Island are just a few of countless historical landmarks to explore. The Golden Gate Bridge, Alcatraz Island, and Castro Street are especially well-known for their part in the city’s history, but digging into other parts of San Francisco’s history can uncover fascinating stories and a better understanding of the city.\\nHistoric landmarks in San Francisco include:\\n- The Castro District and Castro Street\\n- Guadalupe Hyatt House\\n- Panama Building\\n- The Presidio\\n- The Sutro Baths\\n- Twin Peaks\\n- Haight Ashbury District\\n- Alcatraz Island\\n- Golden Gate Bridge\\n- The California Palace of the Legion of Honor\\n- San Francisco Chinatown\\n- Montgomery Ward Store\\n- The Ferry Building\\n- The Exploratorium\\n- Fisherman’s Wharf\\n- The Presidio\\n- The Presidio War Memorial\\n- The Palace of Fine Arts\\n- Treasure Island\\n- Fort Point\\n- Fort Mason\\n- Fort Point Lighthouse\\n- Jack London Square\\n- The Palace of Fine Arts\\nExplorers will also find fascinating historic museums to visit in the city, museums that are as historically important as the landmarks they house. Some of these include:\\n- Asian Art Museum\\n- California Academy of Sciences\\n- California Museum\\n- de Young Museum\\n- Maritime Museum of San Francisco\\nArts, culture, and entertainment are also invaluable aspects of the city’s history, and they have much to do with defining who San Franciscans are today. This includes music, the performing arts, and literature, as cities are nothing without their people. San Francisco’s extraordinary musicians—such as Ray Charles, The Grateful Dead, Sly & the Family Stone, Paul Revere & the Raiders, Jefferson Airplane, and Jefferson Starship—helped shape the city’s musical history, and much more is still being discovered about the city’s music scene.\\nFor history buffs, the city also offers a wealth of theater, including the San Francisco Theater Company, the longest-running off-Broadway theater on the West Coast. Literary history is also well-represented, with well-known authors such as John Steinbeck, Jack Kerouac, Isaac Asimov, and Thomas Wolfe all having roots in the city.\\nThe city’s history is just one of its many treasures, however. After 1999, many people flocked to the city to enjoy the various delights that it has to offer. If you want to experience the grandeur of cultural attractions, the exciting culture of this metropolis, the excellent shopping and dining establishments and also enjoy the scenery wherever you go, then this might be the city for you.\\nThe city is known for its unique culture, fantastic food, shopping, nightlife, and beautiful scenery. Its location, in the heart of the nation’s financial district, makes it a', stop_reason=None)], created=1717756599, model='/home/gbarbadillo/data/deepseekmath', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=640, prompt_tokens=5, total_tokens=645))\n",
      "INFO 06-07 12:36:51 async_llm_engine.py:553] Received request cmpl-dfe624730f6b4d969fa0386ae7e46f80-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:36:52 metrics.py:341] Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 52.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:36:57 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 52.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:37:02 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 52.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:37:04 async_llm_engine.py:124] Finished request cmpl-dfe624730f6b4d969fa0386ae7e46f80-0.\n",
      "INFO:     127.0.0.1:34358 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: Completion(id='cmpl-dfe624730f6b4d969fa0386ae7e46f80', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text=' city located in California. As of the 2000 census, the city had a total population of 731,775. It is the county seat of San Francisco County6Table Table 6 contains the results of the two-way ANOVA test with a significance level of 0.05. We can see that there is a significant interaction effect between the factors \"education\" and \"occupation\" on the outcome variable \"satisfaction\". This means that the relationship between education and job satisfaction depends on the occupation of the individuals.To compare means of the dependent variable \"job satisfaction\" across the levels of the factor \"education\" within each level of the factor \"occupation\", we can perform the post-hoc Tukey HSD test. The results of the Tukey HSD test are shown in Table 7.Table 7: Tukey HSD Test for Job Satisfaction by Education Level within OccupationLevel of Occupation and EducationMean Difference (I-J)Lower BoundUpper BoundManager and High School1.000.0971.903Manager and Some College0.889-0.0131.790Manager and College Graduate0.091-0.9021.084Professional and High School3.7782.8764.680Professional and Some College3.6672.7654.569Professional and College Graduate2.7781.8763.680Clerical and High School2.7781.8763.680Clerical and Some College2.6671.7653.569Clerical and College Graduate1.7780.8762.680Service and High School1.7780.8762.680Service and Some College1.6670.7652.569Service and College Graduate0.778-0.1251.680The results show that there is a significant difference in job satisfaction between managers with different levels of education and between professionals with different levels of education. However, there is no significant difference in job satisfaction between clerks, service workers, and different levels of education.\\nFigure 1: Tukey HSD Test for Job Satisfaction by Education Level within Occupation\\nFrom the table, we can see that the mean difference between managers with a high school diploma and managers with some college is 0.097, between managers with a high school diploma and managers with a college degree is 1.000, between professionals with a high school diploma and professionals with some college is 0.111, between professionals with a high school diploma and professionals with a college degree is 1.983, between clerks with a high school diploma and clerks with some college is 0.099, and between clerks with a high school diploma and clerks with a college degree is', stop_reason=None)], created=1717756611, model='/home/gbarbadillo/data/deepseekmath', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=640, prompt_tokens=5, total_tokens=645))\n",
      "INFO 06-07 12:37:04 async_llm_engine.py:553] Received request cmpl-956b8b8f37924b1ab00f53c5b54ca3c9-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:37:07 metrics.py:341] Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 53.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:37:12 async_llm_engine.py:124] Finished request cmpl-956b8b8f37924b1ab00f53c5b54ca3c9-0.\n",
      "INFO:     127.0.0.1:34358 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: Completion(id='cmpl-956b8b8f37924b1ab00f53c5b54ca3c9', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=' great city for small business owners who want to create a community that is innovative, inclusive and sustainable, and it’s also very reasonable when it comes to the cost of doing business. Right now, we need companies to help us grow.\\nBelow are some of the reasons why you should consider doing business in San Francisco:\\n1. Top-tier institutions and academia\\nStanford University, the University of California, Berkeley and numerous other major research institutions, all within a short drive.\\n2. Silicon Valley\\n40% of the country’s top tech companies are headquartered in San Francisco, including Twitter, LinkedIn, and Zillow. The Golden Gate Bridge, charming atmosphere, and endless famous landmarks will no doubt make your guests love your city.\\n3. Multiple sectors\\nFrom education to funds, food and beverage, fashion, healthcare, and tech, there are countless businesses that can thrive in San Francisco.\\n4. Diversity\\nDiverse options for accommodation — hotels, apartments, houses and hostels — a diverse professional environment, a flux of global flavors on the dining scene and lush Mediterranean climate that guarantees year-round sun and a warm winter.\\n5. Lower cost of doing business\\nCompared to other major North American cities, the cost of doing business in San Francisco is quite reasonable, with lower taxes, utilities costs and rental prices.\\n6. A business-friendly environment\\nSan Francisco was named the 5th best city in the world for doing business by the World Economic Forum. It offers a wide range of business support packages, including expert advice, seminars, workshops, mentoring programs, and protection from lawsuits or property damage.\\n7. Vibrant nightlife and culture\\nWith its many art galleries, museums, historic sites, unique food, coffeehouses and nightclubs, San Francisco caters to many different types of diners and nightlife lovers looking for a fun place to visit and do business.\\nWith all these plus points, San Francisco is an excellent place to do business, whether you’re a professional or passionate entrepreneur looking to start or grow your business. We want to meet you at the City of Fun!', stop_reason=None)], created=1717756624, model='/home/gbarbadillo/data/deepseekmath', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=435, prompt_tokens=5, total_tokens=440))\n",
      "INFO 06-07 12:37:12 async_llm_engine.py:553] Received request cmpl-4f5a34ad635d493587a2f481f3d1dfec-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:37:12 metrics.py:341] Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 53.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:37:15 async_llm_engine.py:124] Finished request cmpl-4f5a34ad635d493587a2f481f3d1dfec-0.\n",
      "INFO:     127.0.0.1:34358 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: Completion(id='cmpl-4f5a34ad635d493587a2f481f3d1dfec', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=\" place with a long and convoluted history. It’s a place that has been ridden by waves of every political and social upheaval from the Spanish incursion, to the gold rush, to the world wars, to the revolution, to the depression and recovery. In the midst of all of this, San Franciscans never fail to embrace diversity and harmony. It’s a city that is known for its unforgiving streets, unique architecture and its notorious nightlife. San Francisco is the perfect blend of culture, vibrancy and legacy.\\nIt's the perfect city to visit\\nOf course, visiting San Francisco as a tourist is a must. There are many iconic places to visit in this city: the Golden Gate Bridge is a must to see, as is Tribeca and the Marina. The city is also home to some great places to go shopping, such as Union Square.\\nDiscover the best the city has to offer at crabapples-forum.com.\", stop_reason=None)], created=1717756632, model='/home/gbarbadillo/data/deepseekmath', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=198, prompt_tokens=5, total_tokens=203))\n",
      "INFO 06-07 12:37:15 async_llm_engine.py:553] Received request cmpl-b62f7b31c59245ec909989b14c69089b-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:37:17 metrics.py:341] Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 52.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:37:22 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 53.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:37:23 async_llm_engine.py:124] Finished request cmpl-b62f7b31c59245ec909989b14c69089b-0.\n",
      "INFO:     127.0.0.1:34358 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: Completion(id='cmpl-b62f7b31c59245ec909989b14c69089b', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=\" city known for its remote, forested and foggy hillsides. However, the San Francisco Bay Area is known for its beauty and varying weather. A few months ago, there were days in the subzero 30's, and then again, people were exercising in the sun with dazzling hot temperatures in the 70s. Thus, it is normal for the weather to change from day to day and be unpredictable.\\nInvestigators in San Francisco found that authorities are trying to track down the people responsible for the disappearance of a 31-year-old woman whose body was found by searchers in a secluded area on a California beach in October.\\nThe 31-year-old student, Shona Laverack, was last seen on October 5th, when she was reported missing by her family and friends. On October 8th, a report came in from the local sheriff’s department indicating that the body of the missing woman had been found in the San Mateo County undergrowth off a residential stretch of Pacifica State Beach.\\nAccording to a California Highway Patrol report, searchers also found evidence of foul play that suggested that the woman’s disappearance could be linked to a violent crime. The resulting investigation involves the biological identification and comparison of DNA evidence from the surrounding environment with the DNA collected from Shona’s person.\\nDuring a press conference on October 13th, investigators said they knew the missing woman’s family had been contacted and were cooperating with the investigation. A search warrant was issued and several suspects were questioned. Some of the suspects were arrested for possible involvement in the crime and had been taken into custody.\\nHowever, the investigation is still ongoing as the suspects are still being questioned about the crime. The suspects are being questioned based on the physical evidence and DNA gathered during the search.\\nInvestigators say that while the ongoing investigation is still in its early stages, there is still no confirmation that they have found the people responsible for the victim’s disappearance.\", stop_reason=None)], created=1717756635, model='/home/gbarbadillo/data/deepseekmath', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=412, prompt_tokens=5, total_tokens=417))\n",
      "INFO 06-07 12:37:23 async_llm_engine.py:553] Received request cmpl-17480cc4738b41358a52e0bc5a30d39b-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:37:26 async_llm_engine.py:124] Finished request cmpl-17480cc4738b41358a52e0bc5a30d39b-0.\n",
      "INFO:     127.0.0.1:34358 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: Completion(id='cmpl-17480cc4738b41358a52e0bc5a30d39b', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=' lawless city?\\nI believe that San Francisco politicians still believe that is accurate. I lived there for almost a year and was talked to at gunpoint several times. It is still a dangerous city, just less so than San Diego or Oakland. Also, it’s more expensive because no one wants to leave if they can avoid it.\\nThat said, there are hundreds of gated communities on the Peninsula where there’s a well deserved belief that the criminals will never come. Some of those communities are gated in enough with their own police that they can afford to pay for police. The city cops? They have very expensive law enforcement budget. Just because politicians say it doesn’t mean it’s true.', stop_reason=None)], created=1717756643, model='/home/gbarbadillo/data/deepseekmath', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=145, prompt_tokens=5, total_tokens=150))\n",
      "INFO 06-07 12:37:26 async_llm_engine.py:553] Received request cmpl-cebb8cff3f9746a9a6fb636f3ee589c2-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:37:27 metrics.py:341] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 53.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:37:32 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 53.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:37:35 async_llm_engine.py:124] Finished request cmpl-cebb8cff3f9746a9a6fb636f3ee589c2-0.\n",
      "INFO:     127.0.0.1:34358 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: Completion(id='cmpl-cebb8cff3f9746a9a6fb636f3ee589c2', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=' city that has a unique heritage, it’s not just known for its breathtaking views or its tech prowess. The foodie scene in San Francisco is just as diverse and eclectic as the city itself. From world-renowned chefs to humble local restaurants, there is something for everyone in the Bay Area.\\nАвторы: Н/Д\\n\\n## Gustatory Ventures\\nSan Francisco is known as the culinary capital of America, and for good reason. The city has a rich and diverse food culture, with a variety of cuisines and flavors from around the world. Here are just a few of the many restaurants that have captured the hearts of San Franciscans and tourists alike:\\n\\n## Chez Panisse\\nChez Panisse is a legendary restaurant in SF’s Mission District, known for its farm-to-table approach to French cuisine. The restaurant’s menus change seasonally based on the produce available from the chef’s farm. Chef Alice Waters is a founding member of the restaurant and has made a name for herself through her commitment to using local and sustainable ingredients. The restaurant has received many awards and has been listed in food magazines around the world as a must-visit destination.\\n\\n## Manresa\\nManresa is a fine-dining restaurant in the upper-class Nob Hill neighborhood. The restaurant is owned by chef Daniel Boulud, who has rapidly become one of the most celebrated chefs in the world. The restaurant specializes in classic French dishes, with a focus on simplicity and precision. The menu changes based on what is available from the chef’s farm and seasonal ingredients. The ambiance in Manresa is intimate and luxurious, creating a warm and cozy atmosphere for those looking for a special dining experience.\\n\\n## Lark\\nLark is a casual and vibrant restaurant in the Mission District that serves Californian cuisine. The restaurant has a popular lunch and dinner menu that features dishes like grilled octopus and burger, as well as vegetarian options like tofu curry and falafel. Lark is popular among locals for its laid-back atmosphere and approachable prices.\\n\\nThese are just a few of the many restaurants in San Francisco that showcase the city’s diverse and inventive food culture. The Bay Area is a foodie haven, and each neighborhood has its own unique offerings. From Michelin-starred restaurants to cozy hole-in-the-wall spots, there’s something for everyone in the culinary scene of San Francisco.', stop_reason=None)], created=1717756646, model='/home/gbarbadillo/data/deepseekmath', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=494, prompt_tokens=5, total_tokens=499))\n",
      "INFO 06-07 12:37:35 async_llm_engine.py:553] Received request cmpl-561c96d7a1a64d7180c13b25259e3a12-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:37:37 metrics.py:341] Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 52.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:37:42 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 51.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:37:43 async_llm_engine.py:124] Finished request cmpl-561c96d7a1a64d7180c13b25259e3a12-0.\n",
      "INFO:     127.0.0.1:34358 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: Completion(id='cmpl-561c96d7a1a64d7180c13b25259e3a12', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=' city in the Bay Area of California. It is the financial, commercial, and cultural center of the San Francisco Bay Area and is the most populous city in the state of California. The city government covers an area of 468.3 square[1] kilometers (180.8 square miles). As of the 2010 U.S. Census, the population of the city was estimated to be 816,487.[2] The population density is 1,742.0 people per square kilometer (4,506.6/sq mi). The racial makeup of the city is 68.6% White, 3.9% Black or African American, 11.1% Asian, 1.8% Native American, 0.6% Pacific Islander, 2.2% from other races, and 3.2% from two or more races. 41.7% of the population are Hispanic or Latino of any race. There were 228,962 households out of which 30.2% had children under the age of 18 living with them, 45.4% were married couples living together, 14.2% had a female householder with no husband present, 4.8% had a male householder with no wife present, and 35.4% were non-families. 28.5% of all households were made up of individuals and 11.5% had someone living alone who was 65 years of age or older. The average household size was 2.67 and the average family size was 3.27.\\nSource: Wikipedia, San Francisco, California', stop_reason=None)], created=1717756655, model='/home/gbarbadillo/data/deepseekmath', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=370, prompt_tokens=5, total_tokens=375))\n",
      "INFO 06-07 12:37:43 async_llm_engine.py:553] Received request cmpl-b808d7f2412d436fa86ecee3ebd4181f-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:37:47 metrics.py:341] Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 52.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:37:50 async_llm_engine.py:124] Finished request cmpl-b808d7f2412d436fa86ecee3ebd4181f-0.\n",
      "INFO:     127.0.0.1:34358 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: Completion(id='cmpl-b808d7f2412d436fa86ecee3ebd4181f', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=' decidedly forward-thinking city, but one thing the locals have embraced wholeheartedly is preservation of endangered species. The city has taken steps to ensure that the endangered garganey (also spelled garter snake or garter garsen) gets the proper care and attention it needs to survive.\\nMore than 100 garganey snakes have been relocated to City Parks and other wildlife refuges in San Francisco to help protect the endangered species. These snake species wards off not just predators, but also disease from jackals and coyotes. The snakes also eat pests and prevent the growth of weeds, making their habitats healthier.\\nSan Francisco is home to a smaller population of the garganey than several other California counties due to a huge decline in their population during the 1980s. As a result, the garganey has been listed as an endangered species in California.\\nPeople are now rewilding areas where they were exterminated and allow for the garganey to thrive again. One such place is the Balboa Park. The park has also given out over 30,000 low-poisonous plants and cages to the snakes. Additionally, there are at least 125 garganey snakes living in the gardens of Botanical gardens in San Francisco now.\\nThe garganey, also known as the reptile aardvark or the Mexican garter snake, is a type of garter snake found in the Americas. It is typically found in the same habitats as other garter snakes, like grasslands, woodlands, and riparian zones. Due to its rarity, the garganey is listed as an endangered species in California, making it an important species to protect.\\nSan Francisco has taken steps to ensure the survival of the garganey, including relocating them to City Parks and wildlife refuges and providing them with low-poisonous plants and cages. By preserving this endangered species, San Francisco is helping to protect the environment and ensure the survival of this unique species.', stop_reason=None)], created=1717756663, model='/home/gbarbadillo/data/deepseekmath', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=413, prompt_tokens=5, total_tokens=418))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 12:37:55 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 22.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:38:05 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:38:15 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:38:25 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:38:35 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:38:45 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:38:55 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    completion = client.completions.create(model=\"/home/gbarbadillo/data/deepseekmath\",\n",
    "                                           prompt=\"San Francisco is a\", max_tokens=640)\n",
    "    print(\"Completion result:\", completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddd5735d03d4e68b5854ec4ae48bbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 12:40:11 async_llm_engine.py:553] Received request cmpl-16782d52b3fd4e6e9085dff3e46b1818-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:40:11 async_llm_engine.py:553] Received request cmpl-b77045b0e58247dcab7f554048dd4870-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:40:11 async_llm_engine.py:553] Received request cmpl-33eba5abe83843dc9e0b5571aac4498d-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:40:11 async_llm_engine.py:553] Received request cmpl-95db5e92cf8c4a5fad494984d30a606d-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:40:11 async_llm_engine.py:553] Received request cmpl-1f68023d505a4095b240757c2fb9bc68-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:40:11 async_llm_engine.py:553] Received request cmpl-46da6680b03f40bdb051751666f51c31-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:40:11 async_llm_engine.py:553] Received request cmpl-885b07879a2a43f89ea7972c728ec8e2-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:40:11 async_llm_engine.py:553] Received request cmpl-24a61f5801ae4332a722e12b0c965644-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:40:11 async_llm_engine.py:553] Received request cmpl-a4649f3443414d2ca1e276959765e801-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:40:11 async_llm_engine.py:553] Received request cmpl-58cd5cacd3cb46f9bf145d42c075312f-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 12:40:11 metrics.py:341] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:40:16 async_llm_engine.py:124] Finished request cmpl-95db5e92cf8c4a5fad494984d30a606d-0.\n",
      "INFO:     127.0.0.1:59208 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 12:40:16 metrics.py:341] Avg prompt throughput: 8.0 tokens/s, Avg generation throughput: 461.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:40:17 async_llm_engine.py:124] Finished request cmpl-a4649f3443414d2ca1e276959765e801-0.\n",
      "INFO:     127.0.0.1:59252 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 12:40:19 async_llm_engine.py:124] Finished request cmpl-24a61f5801ae4332a722e12b0c965644-0.\n",
      "INFO:     127.0.0.1:59254 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 12:40:19 async_llm_engine.py:124] Finished request cmpl-1f68023d505a4095b240757c2fb9bc68-0.\n",
      "INFO:     127.0.0.1:59222 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 12:40:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 348.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:40:22 async_llm_engine.py:124] Finished request cmpl-58cd5cacd3cb46f9bf145d42c075312f-0.\n",
      "INFO:     127.0.0.1:59264 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 12:40:22 async_llm_engine.py:124] Finished request cmpl-46da6680b03f40bdb051751666f51c31-0.\n",
      "INFO:     127.0.0.1:59234 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 12:40:22 async_llm_engine.py:124] Finished request cmpl-b77045b0e58247dcab7f554048dd4870-0.\n",
      "INFO:     127.0.0.1:59198 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 12:40:24 async_llm_engine.py:124] Finished request cmpl-16782d52b3fd4e6e9085dff3e46b1818-0.\n",
      "INFO:     127.0.0.1:59182 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 12:40:25 async_llm_engine.py:124] Finished request cmpl-33eba5abe83843dc9e0b5571aac4498d-0.\n",
      "INFO 06-07 12:40:25 async_llm_engine.py:124] Finished request cmpl-885b07879a2a43f89ea7972c728ec8e2-0.\n",
      "INFO:     127.0.0.1:59200 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59232 - \"POST /v1/completions HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def monitor_progress(submits):\n",
    "    progress = 0\n",
    "    with tqdm(total=len(submits), smoothing=0) as progress_bar:\n",
    "        while 1:\n",
    "            time.sleep(1)\n",
    "            current_progress = np.sum([submit.done() for submit in submits])\n",
    "            if current_progress > progress:\n",
    "                progress_bar.update(current_progress - progress)\n",
    "                progress = current_progress\n",
    "            if progress == len(submits):\n",
    "                break\n",
    "\n",
    "def make_request():\n",
    "    openai_api_key = \"EMPTY\"\n",
    "    openai_api_base = \"http://localhost:8000/v1\"\n",
    "    client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        base_url=openai_api_base,\n",
    "    )\n",
    "    completion = client.completions.create(model=\"/home/gbarbadillo/data/deepseekmath\",\n",
    "                                           prompt=\"San Francisco is a\", max_tokens=640)\n",
    "    return completion\n",
    "\n",
    "max_workers = 10\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as pool:\n",
    "    submits = []\n",
    "    for i in range(10):\n",
    "        submits.append(pool.submit(make_request))\n",
    "    monitor_progress(submits)\n",
    "    results = [submit.result() for submit in submits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greatest speedup comes from batching the requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 GPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://docs.vllm.ai/en/latest/serving/distributed_serving.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 12:59:58,966\tINFO worker.py:1753 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 12:59:59 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/home/gbarbadillo/data/deepseekmath', speculative_config=None, tokenizer='/home/gbarbadillo/data/deepseekmath', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/home/gbarbadillo/data/deepseekmath)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 13:00:03 utils.py:618] Found nccl from library libnccl.so.2\n",
      "INFO 06-07 13:00:03 pynccl.py:65] vLLM is using nccl==2.20.5\n",
      "\u001b[36m(RayWorkerWrapper pid=30328)\u001b[0m INFO 06-07 13:00:03 utils.py:618] Found nccl from library libnccl.so.2\n",
      "\u001b[36m(RayWorkerWrapper pid=30328)\u001b[0m INFO 06-07 13:00:03 pynccl.py:65] vLLM is using nccl==2.20.5\n",
      "INFO 06-07 13:00:03 custom_all_reduce_utils.py:179] reading GPU P2P access cache from /home/gbarbadillo/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "WARNING 06-07 13:00:03 custom_all_reduce.py:167] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[36m(RayWorkerWrapper pid=30328)\u001b[0m INFO 06-07 13:00:03 custom_all_reduce_utils.py:179] reading GPU P2P access cache from /home/gbarbadillo/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[36m(RayWorkerWrapper pid=30328)\u001b[0m WARNING 06-07 13:00:03 custom_all_reduce.py:167] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 06-07 13:00:05 model_runner.py:146] Loading model weights took 6.4663 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=30328)\u001b[0m INFO 06-07 13:00:05 model_runner.py:146] Loading model weights took 6.4663 GB\n",
      "INFO 06-07 13:00:07 distributed_gpu_executor.py:56] # GPU blocks: 3729, # CPU blocks: 1092\n",
      "INFO 06-07 13:00:09 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-07 13:00:09 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=30328)\u001b[0m INFO 06-07 13:00:10 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=30328)\u001b[0m INFO 06-07 13:00:10 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=30328)\u001b[0m INFO 06-07 13:00:15 model_runner.py:924] Graph capturing finished in 6 secs.\n",
      "INFO 06-07 13:00:15 model_runner.py:924] Graph capturing finished in 6 secs.\n",
      "INFO 06-07 13:00:15 serving_chat.py:84] Using default chat template:\n",
      "INFO 06-07 13:00:15 serving_chat.py:84] {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n",
      "INFO 06-07 13:00:15 serving_chat.py:84] \n",
      "INFO 06-07 13:00:15 serving_chat.py:84] ' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n",
      "INFO 06-07 13:00:15 serving_chat.py:84] \n",
      "INFO 06-07 13:00:15 serving_chat.py:84] ' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:     Started server process [28998]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-07 13:00:16 serving_embedding.py:141] embedding_mode is False. Embedding API will not work.\n",
      "INFO 06-07 13:00:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "\n",
    "def run_vllm_server():\n",
    "    subprocess.run(['python', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "                    '--model', '/home/gbarbadillo/data/deepseekmath', \n",
    "                    #'--pipeline-parallel-size', '2', #NotImplementedError: Pipeline parallelism is not supported yet.\n",
    "                    '--tensor-parallel-size', '2',\n",
    "                    ])\n",
    "\n",
    "server_thread = threading.Thread(target=run_vllm_server)\n",
    "server_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from openai import OpenAI\n",
    "\n",
    "def monitor_progress(submits):\n",
    "    progress = 0\n",
    "    with tqdm(total=len(submits), smoothing=0) as progress_bar:\n",
    "        while 1:\n",
    "            time.sleep(1)\n",
    "            current_progress = np.sum([submit.done() for submit in submits])\n",
    "            if current_progress > progress:\n",
    "                progress_bar.update(current_progress - progress)\n",
    "                progress = current_progress\n",
    "            if progress == len(submits):\n",
    "                break\n",
    "\n",
    "def make_request():\n",
    "    openai_api_key = \"EMPTY\"\n",
    "    openai_api_base = \"http://localhost:8000/v1\"\n",
    "    client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        base_url=openai_api_base,\n",
    "    )\n",
    "    completion = client.completions.create(model=\"/home/gbarbadillo/data/deepseekmath\",\n",
    "                                           prompt=\"San Francisco is a\", max_tokens=640)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56b3cb02a4b4eb7bc7e50b57541c4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-baac52c8e64145899d58083211619ef2-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-5ba7530ded824fc7ba009c4edc765006-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-2560bd551da44c2d81dda0c235330d70-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-2c6a5bb12c234736b60650af111985c1-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-ae5271c580d040309acc6bd90a0f243d-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-980d8c00c9de40a2a8a7a182c4222f8a-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-c5f80beb1b7a479a88c82f1257ee7d03-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-189cc5b90721447fb4b3f53a2d53deca-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-24a814dc1a2b4e448aa397a7c2cd672b-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-8e1b6d9d45f945ec843f4f3a7b69e3c8-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-385bad1734a846f5ab216be8abeba81e-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-684341468ea24d99b78b2e2bd2d68e6e-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-9721c8cbabc24860974d5c21381765b9-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-f608137c8e874f7a8b86401e7b9f8a87-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-dccaae4a1557479bb2c32070e947a074-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-57c9a9fba87a4c6980ed2e641e6091e1-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-43be65a1b4684adbbe08e3652f83f7ef-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-235ebe45d9d94651857dc03987b323a2-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-d0c1522c93564a70a8ed21c72d73b829-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-b2e5771d05ff4c7c89bf5eb229f93697-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-82d34898f3894fc29000179ac0f7b372-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-cbbd30278ec54d259401403966fd57a3-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-1f116accd45142619ab0e29b1aef7aba-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-1e66ba65a0174a4f8a641a0142a9c208-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:26 async_llm_engine.py:553] Received request cmpl-1608dc1b45ff487fa37d6cbd469488f8-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:03:29 async_llm_engine.py:124] Finished request cmpl-235ebe45d9d94651857dc03987b323a2-0.\n",
      "INFO:     127.0.0.1:36368 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:31 async_llm_engine.py:124] Finished request cmpl-5ba7530ded824fc7ba009c4edc765006-0.\n",
      "INFO:     127.0.0.1:36202 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:31 metrics.py:341] Avg prompt throughput: 25.0 tokens/s, Avg generation throughput: 850.4 tokens/s, Running: 23 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:03:33 async_llm_engine.py:124] Finished request cmpl-684341468ea24d99b78b2e2bd2d68e6e-0.\n",
      "INFO:     127.0.0.1:36312 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:35 async_llm_engine.py:124] Finished request cmpl-9721c8cbabc24860974d5c21381765b9-0.\n",
      "INFO:     127.0.0.1:36328 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:35 async_llm_engine.py:124] Finished request cmpl-d0c1522c93564a70a8ed21c72d73b829-0.\n",
      "INFO:     127.0.0.1:36382 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 878.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:03:36 async_llm_engine.py:124] Finished request cmpl-c5f80beb1b7a479a88c82f1257ee7d03-0.\n",
      "INFO:     127.0.0.1:36240 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:36 async_llm_engine.py:124] Finished request cmpl-1f116accd45142619ab0e29b1aef7aba-0.\n",
      "INFO:     127.0.0.1:36402 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:38 async_llm_engine.py:124] Finished request cmpl-385bad1734a846f5ab216be8abeba81e-0.\n",
      "INFO:     127.0.0.1:36300 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:38 async_llm_engine.py:124] Finished request cmpl-baac52c8e64145899d58083211619ef2-0.\n",
      "INFO:     127.0.0.1:36192 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:38 async_llm_engine.py:124] Finished request cmpl-dccaae4a1557479bb2c32070e947a074-0.\n",
      "INFO:     127.0.0.1:36344 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:39 async_llm_engine.py:124] Finished request cmpl-8e1b6d9d45f945ec843f4f3a7b69e3c8-0.\n",
      "INFO:     127.0.0.1:36286 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:40 async_llm_engine.py:124] Finished request cmpl-57c9a9fba87a4c6980ed2e641e6091e1-0.\n",
      "INFO:     127.0.0.1:36354 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:41 async_llm_engine.py:124] Finished request cmpl-980d8c00c9de40a2a8a7a182c4222f8a-0.\n",
      "INFO:     127.0.0.1:36242 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:41 async_llm_engine.py:124] Finished request cmpl-82d34898f3894fc29000179ac0f7b372-0.\n",
      "INFO:     127.0.0.1:36390 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 656.5 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:03:41 async_llm_engine.py:124] Finished request cmpl-24a814dc1a2b4e448aa397a7c2cd672b-0.\n",
      "INFO:     127.0.0.1:36276 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:42 async_llm_engine.py:124] Finished request cmpl-f608137c8e874f7a8b86401e7b9f8a87-0.\n",
      "INFO:     127.0.0.1:36340 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:42 async_llm_engine.py:124] Finished request cmpl-b2e5771d05ff4c7c89bf5eb229f93697-0.\n",
      "INFO:     127.0.0.1:36378 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:03:42 async_llm_engine.py:124] Finished request cmpl-2560bd551da44c2d81dda0c235330d70-0.\n",
      "INFO 06-07 13:03:42 async_llm_engine.py:124] Finished request cmpl-2c6a5bb12c234736b60650af111985c1-0.\n",
      "INFO 06-07 13:03:42 async_llm_engine.py:124] Finished request cmpl-ae5271c580d040309acc6bd90a0f243d-0.\n",
      "INFO 06-07 13:03:42 async_llm_engine.py:124] Finished request cmpl-189cc5b90721447fb4b3f53a2d53deca-0.\n",
      "INFO 06-07 13:03:42 async_llm_engine.py:124] Finished request cmpl-43be65a1b4684adbbe08e3652f83f7ef-0.\n",
      "INFO 06-07 13:03:42 async_llm_engine.py:124] Finished request cmpl-cbbd30278ec54d259401403966fd57a3-0.\n",
      "INFO 06-07 13:03:42 async_llm_engine.py:124] Finished request cmpl-1e66ba65a0174a4f8a641a0142a9c208-0.\n",
      "INFO 06-07 13:03:42 async_llm_engine.py:124] Finished request cmpl-1608dc1b45ff487fa37d6cbd469488f8-0.\n",
      "INFO:     127.0.0.1:36214 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:36216 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:36232 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:36262 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:36356 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:36396 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:36408 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:36420 - \"POST /v1/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 13:03:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 39.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:04:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:04:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:04:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:04:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:04:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:04:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:05:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:05:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:05:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:05:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:05:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:05:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:06:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:06:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:06:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:06:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:06:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:06:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:07:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:07:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:07:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:07:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:07:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:07:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:08:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:08:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:08:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:08:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:08:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:08:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:09:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:09:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:09:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:09:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:09:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:09:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:10:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:10:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:10:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:10:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:10:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:10:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:11:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:11:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:11:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:11:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:11:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:11:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:12:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:12:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:12:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:12:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:12:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:12:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:13:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:13:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:13:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:13:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:13:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:13:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:14:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:14:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:14:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:14:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:14:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:14:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:15:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:15:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:15:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:15:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:15:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:15:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:16:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:16:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:16:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:16:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:16:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:16:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:17:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:17:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:17:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:17:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:17:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:17:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:18:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:18:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:18:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:18:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:18:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:18:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:19:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:19:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:19:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:19:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:19:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:19:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:20:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:20:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:20:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:20:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:20:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:20:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:21:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:21:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:21:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:21:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:21:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:21:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:22:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:22:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:22:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:22:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:22:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:22:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:23:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:23:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:23:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:23:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:23:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:23:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:24:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:24:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:24:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:24:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:24:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:24:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:25:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:25:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:25:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:25:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:25:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:25:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:26:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:26:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:26:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:26:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:26:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:26:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:27:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:27:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:27:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:27:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:27:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:27:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:28:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:28:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:28:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:28:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:28:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:28:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:29:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:29:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:29:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:29:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:29:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:29:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:30:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:30:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:30:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:30:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:30:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:30:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:31:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:31:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:31:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:31:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:31:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:31:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:32:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:32:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:32:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:32:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:32:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:32:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:33:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:33:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:33:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:33:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:33:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:33:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:34:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:34:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:34:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:34:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:34:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:34:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:35:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:35:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:35:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:35:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:35:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:35:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:36:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:36:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:36:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:36:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:36:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:36:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:37:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:37:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:37:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:37:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:37:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:37:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:38:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:38:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:38:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:38:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:38:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:38:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:39:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:39:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:39:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:39:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:39:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:39:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:40:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:40:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:40:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:40:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:40:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:40:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:41:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:41:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:41:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:41:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:41:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:41:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:42:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:42:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:42:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:42:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:42:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:42:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:43:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:43:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:43:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:43:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:43:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:43:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:44:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:44:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:44:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:44:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:44:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:44:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:45:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "max_workers = 40\n",
    "n_jobs = 25\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as pool:\n",
    "    submits = []\n",
    "    for i in range(n_jobs):\n",
    "        submits.append(pool.submit(make_request))\n",
    "    monitor_progress(submits)\n",
    "    results = [submit.result() for submit in submits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 gpu, 20 workers, 1:23\n",
    "- 2 gpus, 10 workers, 2:00\n",
    "- 2 gpus, 20 workers, 1:11\n",
    "- 2 gpus, 40 workers, 0:56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very promising, I might speedup inference x10 on my PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' busy city with thousands of people living, working, and traveling through its streets every day. Navigation in the city can be complicated due to the winding roads and dense traffic. However, there are several apps and websites that can help you find your way around San Francisco. In this article, we will explore some of the best navigation apps and websites for San Francisco.\\n\\n### 1. MapQuest\\nMapQuest is a popular navigation app and website that offers turn-by-turn directions for driving, walking, and public transportation in over 300,000 locations in the world. It also provides driving time estimates, real-time traffic updates, and alternate routes if needed. Sign up for a free account to save your routes and access turn-by-turn navigation.\\n\\n### 2. Google Maps\\nGoogle Maps is the other popular navigation app that is widely available on both iOS and Android devices. It provides turn-by-turn directions for driving, walking, and public transportation. It also includes interactive maps, satellite images, and local searches. You can sign in with your Google account to save your routes and access turn-by-turn navigation.\\n\\n### 3. Waze\\nWaze is a crowd-sourced navigation app where users contribute information about traffic conditions, roadworks, and other local knowledge. The app provides turn-by-turn directions for driving, walking, and public transportation. It also includes interactive maps, live traffic updates, weather information, and in-app chat with other users. Sign up for a free account to save your routes and access turn-by-turn navigation.\\n\\n### 4. Citymapper\\nCitymapper is a navigation app designed specifically for navigating in cities like San Francisco. It provides turn-by-turn directions for driving, walking, and public transportation. It also includes live traffic updates, bike route suggestions, and a real-time crowd-sourced list of popular places to go. Sign up for a free account to save your routes and access turn-by-turn navigation.\\n\\n### 5. Navteq Maps\\nNavteq Maps is a navigation service provider that provides turn-by-turn directions and detailed maps for San Francisco and other cities in the United States. You can access their maps and turn-by-turn navigation services through MapQuest or Bing Maps.\\n\\n### 6. Bing Maps\\nBing Maps is another navigation app that offers turn-by-turn directions for driving, walking, and public transportation. It also includes interactive maps, satellite images, weather information, and local searches. Sign in with your Microsoft account to save your routes and access turn-by-turn navigation.\\n\\nThese are some of the best navigation apps and websites for San Francisco. Each app or website has its own unique features and benefits, so choose the one that best suits your navigation needs. Remember to always double-check traffic conditions and avoid broken or dangerous roadways to keep you and your passengers safe.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 12:40:55 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:41:05 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:41:15 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:41:25 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 12:41:35 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "results[1].choices[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "!python -m vllm.entrypoints.openai.api_server --model /home/gbarbadillo/data/deepseekmath &\n",
    "---------------------------------------------------------------------------\n",
    "OSError                                   Traceback (most recent call last)\n",
    "Cell In[1], line 1\n",
    "----> 1 get_ipython().system('python -m vllm.entrypoints.openai.api_server --model /home/gbarbadillo/data/deepseekmath &')\n",
    "\n",
    "File ~/miniconda3/envs/aimo/lib/python3.10/site-packages/ipykernel/zmqshell.py:641, in ZMQInteractiveShell.system_piped(self, cmd)\n",
    "    634 if cmd.rstrip().endswith(\"&\"):\n",
    "    635     # this is *far* from a rigorous test\n",
    "    636     # We do not support backgrounding processes because we either use\n",
    "    637     # pexpect or pipes to read from.  Users can always just call\n",
    "    638     # os.system() or use ip.system=ip.system_raw\n",
    "    639     # if they really want a background process.\n",
    "    640     msg = \"Background processes not supported.\"\n",
    "--> 641     raise OSError(msg)\n",
    "    643 # we explicitly do NOT return the subprocess status code, because\n",
    "    644 # a non-None value would trigger :func:`sys.displayhook` calls.\n",
    "    645 # Instead, we store the exit_code in user_ns.\n",
    "    646 # Also, protect system call from UNC paths on Windows here too\n",
    "    647 # as is done in InteractiveShell.system_raw\n",
    "    648 if sys.platform == \"win32\":\n",
    "\n",
    "OSError: Background processes not supported.\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 gpus on different servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 13:50:18 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/home/gbarbadillo/data/deepseekmath', speculative_config=None, tokenizer='/home/gbarbadillo/data/deepseekmath', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/home/gbarbadillo/data/deepseekmath)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 13:50:21 model_runner.py:146] Loading model weights took 12.8725 GB\n",
      "INFO 06-07 13:50:22 gpu_executor.py:83] # GPU blocks: 985, # CPU blocks: 546\n",
      "INFO 06-07 13:50:25 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-07 13:50:25 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-07 13:50:30 model_runner.py:924] Graph capturing finished in 6 secs.\n",
      "INFO 06-07 13:50:31 serving_chat.py:84] Using default chat template:\n",
      "INFO 06-07 13:50:31 serving_chat.py:84] {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n",
      "INFO 06-07 13:50:31 serving_chat.py:84] \n",
      "INFO 06-07 13:50:31 serving_chat.py:84] ' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n",
      "INFO 06-07 13:50:31 serving_chat.py:84] \n",
      "INFO 06-07 13:50:31 serving_chat.py:84] ' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:     Started server process [35184]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-07 13:50:31 serving_embedding.py:141] embedding_mode is False. Embedding API will not work.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run_vllm_server(device=0, port=8000):\n",
    "    env = os.environ.copy()\n",
    "    env['CUDA_VISIBLE_DEVICES'] = str(device)\n",
    "    subprocess.run(['python', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "                    '--model', '/home/gbarbadillo/data/deepseekmath',\n",
    "                    '--port', str(port),\n",
    "                    ],\n",
    "                    env=env)\n",
    "\n",
    "server_thread = threading.Thread(target=run_vllm_server)\n",
    "server_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 13:50:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:50:42 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/home/gbarbadillo/data/deepseekmath', speculative_config=None, tokenizer='/home/gbarbadillo/data/deepseekmath', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/home/gbarbadillo/data/deepseekmath)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 13:50:44 model_runner.py:146] Loading model weights took 12.8725 GB\n",
      "INFO 06-07 13:50:46 gpu_executor.py:83] # GPU blocks: 982, # CPU blocks: 546\n",
      "INFO 06-07 13:50:48 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-07 13:50:48 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-07 13:50:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:50:54 model_runner.py:924] Graph capturing finished in 6 secs.\n",
      "INFO 06-07 13:50:54 serving_chat.py:84] Using default chat template:\n",
      "INFO 06-07 13:50:54 serving_chat.py:84] {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n",
      "INFO 06-07 13:50:54 serving_chat.py:84] \n",
      "INFO 06-07 13:50:54 serving_chat.py:84] ' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n",
      "INFO 06-07 13:50:54 serving_chat.py:84] \n",
      "INFO 06-07 13:50:54 serving_chat.py:84] ' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:     Started server process [35312]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-07 13:50:54 serving_embedding.py:141] embedding_mode is False. Embedding API will not work.\n",
      "INFO 06-07 13:51:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:51:04 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:51:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:51:14 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:51:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:51:24 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:51:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "threading.Thread(target=run_vllm_server, args=(1, 8001)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 13:51:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from openai import OpenAI\n",
    "\n",
    "def monitor_progress(submits):\n",
    "    progress = 0\n",
    "    with tqdm(total=len(submits), smoothing=0) as progress_bar:\n",
    "        while 1:\n",
    "            time.sleep(1)\n",
    "            current_progress = np.sum([submit.done() for submit in submits])\n",
    "            if current_progress > progress:\n",
    "                progress_bar.update(current_progress - progress)\n",
    "                progress = current_progress\n",
    "            if progress == len(submits):\n",
    "                break\n",
    "\n",
    "def make_request(port=8000):\n",
    "    openai_api_key = \"EMPTY\"\n",
    "    openai_api_base = f\"http://localhost:{port}/v1\"\n",
    "    client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        base_url=openai_api_base,\n",
    "    )\n",
    "    completion = client.completions.create(model=\"/home/gbarbadillo/data/deepseekmath\",\n",
    "                                           prompt=\"San Francisco is a\", max_tokens=640)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766353bbc21f4de6964692d944c3e0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-62f4c288349e4052b9b61d09c7f3a231-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-546d5ef0d5164e4191530361122bc6c1-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-ef22f9d1db134eadb6a4f70d8fc24787-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-75ffe8c36d0148b9be45366cd9483ce7-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-1b15dc55774945c18e7a20aea0a298c4-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-86eba648f7bd4490a0bee8247caeaa2c-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-91f885e70fb440499b2866380810c0e0-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-e05d3faa71f44ebb93e64641191f43d0-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-c19cc8bd82d74ed9ab5e5de6b8f12d74-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-dba9d98285064ed5b6ba95c01488b752-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-8044bec722d64f0ca34618ab33904d18-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-a830b78543b1484f8352cf64e994a317-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-6490fc0404ba4755b426ff9564825187-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-c1e2bbc0dba14e18a95d7a0b2ed054e6-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-64917cdf576343f0b480109ed713e347-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-ceeab99cdad44fc498ea2da47bf37bd3-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-7a8ea796c1784feb8dc65901924848ab-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-1418d8e1628d4e74baa17c6c9982af89-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-20597ec744d346489f1b98f62fabafde-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-2e0a42eeca4d4ab0a0c72fd4eb3a9fda-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-688c22d7606a43a5ba50f6bbc8fdcefa-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-822e7a0d24984d2f9633a34caa8cc00d-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-424d37f751f841d390ffe7d2c1f62956-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-0306d8e7c45a4b6ea8fbe496426b94f1-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-d48eed2c58ba4c138a6f0eb86d04ce00-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-d4870222e7f841218131706138ee0d44-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-63f676b4799c41edb78f12085f8b1dcd-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-f26c090d5ac8485f84a4a618ad164b0d-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-e264380b12d44fc8bfef059caceba21c-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-db012bc3b8734c2e887ce83c2d2c5ddf-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-fb068eb55cf84735a387d46db6c5c5a5-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-e05ca4c731534198a6e9a045d32910f5-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-ddfa50ee8078433ea117c9bb623bb6d9-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-38ea327b78ab49c4a86ed91c4978290e-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-dcf7b8d3714749b28c1fa32ada835eef-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-334f756400194a96b7403084a5aeae9d-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-3da1cac6b7cc44158d66deebc9f8123a-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-a87a3d2035964aa984ff3517c89d6a8a-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-a839a14926094d54859ca17ca22ccb7d-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:28 async_llm_engine.py:553] Received request cmpl-53f7d5b4ddb64208ba0b7502efc7efa4-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:29 metrics.py:341] Avg prompt throughput: 0.7 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:52:29 metrics.py:341] Avg prompt throughput: 19.9 tokens/s, Avg generation throughput: 79.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:52:34 metrics.py:341] Avg prompt throughput: 19.0 tokens/s, Avg generation throughput: 739.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 24.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:52:34 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 691.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 26.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:52:35 async_llm_engine.py:124] Finished request cmpl-ef22f9d1db134eadb6a4f70d8fc24787-0.\n",
      "INFO:     127.0.0.1:58754 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:35 async_llm_engine.py:553] Received request cmpl-1b0fe5a4b5a444d4875f4f2cf26c0ad4-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:36 async_llm_engine.py:124] Finished request cmpl-75ffe8c36d0148b9be45366cd9483ce7-0.\n",
      "INFO:     127.0.0.1:43566 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:36 async_llm_engine.py:553] Received request cmpl-03b4afebbfd74eba9216ae60223616d3-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:36 async_llm_engine.py:124] Finished request cmpl-fb068eb55cf84735a387d46db6c5c5a5-0.\n",
      "INFO:     127.0.0.1:58892 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:36 async_llm_engine.py:553] Received request cmpl-c178a731e6c04bacac13f5a269d44b40-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:37 async_llm_engine.py:124] Finished request cmpl-91f885e70fb440499b2866380810c0e0-0.\n",
      "INFO:     127.0.0.1:58782 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:37 async_llm_engine.py:553] Received request cmpl-aa92f665c1ec4169a4d41e403fb02afc-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:39 metrics.py:341] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 711.5 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:52:39 metrics.py:341] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 651.2 tokens/s, Running: 21 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:52:39 async_llm_engine.py:124] Finished request cmpl-8044bec722d64f0ca34618ab33904d18-0.\n",
      "INFO:     127.0.0.1:58790 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:39 async_llm_engine.py:553] Received request cmpl-cc41fb357c7245dea0ab7df86801e407-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:39 async_llm_engine.py:124] Finished request cmpl-c1e2bbc0dba14e18a95d7a0b2ed054e6-0.\n",
      "INFO:     127.0.0.1:58816 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:39 async_llm_engine.py:553] Received request cmpl-85e8a8b4d81e4887a63818bf53180109-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:40 async_llm_engine.py:124] Finished request cmpl-db012bc3b8734c2e887ce83c2d2c5ddf-0.\n",
      "INFO:     127.0.0.1:43680 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:40 async_llm_engine.py:553] Received request cmpl-478bad4374f24ebb998658c50863654b-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:40 async_llm_engine.py:124] Finished request cmpl-20597ec744d346489f1b98f62fabafde-0.\n",
      "INFO:     127.0.0.1:58826 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:40 async_llm_engine.py:553] Received request cmpl-448cb27da422478c82789029d06ab4f6-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:40 async_llm_engine.py:124] Finished request cmpl-86eba648f7bd4490a0bee8247caeaa2c-0.\n",
      "INFO:     127.0.0.1:43574 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:40 async_llm_engine.py:553] Received request cmpl-e77a6da4d9ad4e7b8f701c75a4b53f90-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:41 async_llm_engine.py:124] Finished request cmpl-a839a14926094d54859ca17ca22ccb7d-0.\n",
      "INFO:     127.0.0.1:58926 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:41 async_llm_engine.py:553] Received request cmpl-08fc0e79717d4ea4a5725b84374e6084-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:41 async_llm_engine.py:124] Finished request cmpl-64917cdf576343f0b480109ed713e347-0.\n",
      "INFO:     127.0.0.1:58820 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:41 async_llm_engine.py:553] Received request cmpl-f50171cacb1b4b7b830b5f83da9199a7-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:42 async_llm_engine.py:124] Finished request cmpl-e05d3faa71f44ebb93e64641191f43d0-0.\n",
      "INFO:     127.0.0.1:43584 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:42 async_llm_engine.py:553] Received request cmpl-056c68c7ca2b4b2982b04eea30364202-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:42 async_llm_engine.py:124] Finished request cmpl-aa92f665c1ec4169a4d41e403fb02afc-0.\n",
      "INFO:     127.0.0.1:42912 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:42 async_llm_engine.py:553] Received request cmpl-577ef6392ce84916944511742004aed0-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:42 async_llm_engine.py:124] Finished request cmpl-f26c090d5ac8485f84a4a618ad164b0d-0.\n",
      "INFO:     127.0.0.1:58882 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:42 async_llm_engine.py:553] Received request cmpl-32e7adbf13ca411790fc3c070e847527-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:42 async_llm_engine.py:124] Finished request cmpl-3da1cac6b7cc44158d66deebc9f8123a-0.\n",
      "INFO:     127.0.0.1:58914 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:43 async_llm_engine.py:553] Received request cmpl-826cf82f0cc4408da5b6198c0a1776c6-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:43 async_llm_engine.py:124] Finished request cmpl-688c22d7606a43a5ba50f6bbc8fdcefa-0.\n",
      "INFO:     127.0.0.1:58836 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:43 async_llm_engine.py:553] Received request cmpl-8b6d9bc0f7f84b53984a942b82f92b7d-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:43 async_llm_engine.py:124] Finished request cmpl-1b15dc55774945c18e7a20aea0a298c4-0.\n",
      "INFO:     127.0.0.1:58768 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:43 async_llm_engine.py:553] Received request cmpl-3f192a66a67947148fdf5011ed3f98bd-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:43 async_llm_engine.py:124] Finished request cmpl-822e7a0d24984d2f9633a34caa8cc00d-0.\n",
      "INFO:     127.0.0.1:58848 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:43 async_llm_engine.py:553] Received request cmpl-ea7af7023d3e41858c3ecfabe6403546-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:43 async_llm_engine.py:124] Finished request cmpl-a830b78543b1484f8352cf64e994a317-0.\n",
      "INFO:     127.0.0.1:43604 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:44 async_llm_engine.py:553] Received request cmpl-deedfc4f246440799067dd41e098961e-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:44 metrics.py:341] Avg prompt throughput: 7.0 tokens/s, Avg generation throughput: 611.0 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 32.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:52:44 metrics.py:341] Avg prompt throughput: 7.0 tokens/s, Avg generation throughput: 676.0 tokens/s, Running: 23 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 55.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:52:44 async_llm_engine.py:124] Finished request cmpl-2e0a42eeca4d4ab0a0c72fd4eb3a9fda-0.\n",
      "INFO:     127.0.0.1:43628 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:44 async_llm_engine.py:553] Received request cmpl-74477d923cca454ea1abb4e02f8a0b21-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:45 async_llm_engine.py:124] Finished request cmpl-ceeab99cdad44fc498ea2da47bf37bd3-0.\n",
      "INFO:     127.0.0.1:43616 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:45 async_llm_engine.py:553] Received request cmpl-b0d9f596957b42a6b085e9ae67b13f66-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:45 async_llm_engine.py:124] Finished request cmpl-e264380b12d44fc8bfef059caceba21c-0.\n",
      "INFO:     127.0.0.1:43668 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:45 async_llm_engine.py:553] Received request cmpl-b6531b2f073e4d169f21a3b4581e54b3-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:45 async_llm_engine.py:124] Finished request cmpl-38ea327b78ab49c4a86ed91c4978290e-0.\n",
      "INFO:     127.0.0.1:43688 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:45 async_llm_engine.py:553] Received request cmpl-98287b6942f643cfb9767ad86177b7a8-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:45 async_llm_engine.py:124] Finished request cmpl-62f4c288349e4052b9b61d09c7f3a231-0.\n",
      "INFO:     127.0.0.1:58746 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:46 async_llm_engine.py:553] Received request cmpl-f9ac87ee6ef04d0bac566fc62e76aac7-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:46 async_llm_engine.py:124] Finished request cmpl-63f676b4799c41edb78f12085f8b1dcd-0.\n",
      "INFO:     127.0.0.1:58876 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:46 async_llm_engine.py:553] Received request cmpl-c8b146a638b044a596597519b94922b0-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:46 async_llm_engine.py:124] Finished request cmpl-546d5ef0d5164e4191530361122bc6c1-0.\n",
      "INFO:     127.0.0.1:43552 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:47 async_llm_engine.py:553] Received request cmpl-db66f9e9d1da4b939494fc0a604dcec4-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:47 async_llm_engine.py:124] Finished request cmpl-6490fc0404ba4755b426ff9564825187-0.\n",
      "INFO 06-07 13:52:47 async_llm_engine.py:124] Finished request cmpl-424d37f751f841d390ffe7d2c1f62956-0.\n",
      "INFO 06-07 13:52:47 async_llm_engine.py:124] Finished request cmpl-d48eed2c58ba4c138a6f0eb86d04ce00-0.\n",
      "INFO 06-07 13:52:47 async_llm_engine.py:124] Finished request cmpl-dcf7b8d3714749b28c1fa32ada835eef-0.\n",
      "INFO 06-07 13:52:47 async_llm_engine.py:124] Finished request cmpl-a87a3d2035964aa984ff3517c89d6a8a-0.\n",
      "INFO:     127.0.0.1:58806 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:58856 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:58872 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:58904 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:58924 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:47 async_llm_engine.py:553] Received request cmpl-882c7a751b0c411d888bfebfadd8527e-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:47 async_llm_engine.py:553] Received request cmpl-4d97a2811da747feb938bad70c2b4500-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:47 async_llm_engine.py:553] Received request cmpl-ba1c29b619f545c3beeda06dfff24e6f-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:47 async_llm_engine.py:553] Received request cmpl-6eb184f262284dc6a587efcd62f538c8-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:47 async_llm_engine.py:553] Received request cmpl-74192c13cfdd4224a041fb2644893bde-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:47 async_llm_engine.py:124] Finished request cmpl-deedfc4f246440799067dd41e098961e-0.\n",
      "INFO:     127.0.0.1:38146 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:47 async_llm_engine.py:553] Received request cmpl-1de4c663c4e44a199d9222458a99950a-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:47 async_llm_engine.py:124] Finished request cmpl-53f7d5b4ddb64208ba0b7502efc7efa4-0.\n",
      "INFO:     127.0.0.1:43704 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:47 async_llm_engine.py:553] Received request cmpl-6a9d2484c335403d950c63a1b38e0e7a-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:48 async_llm_engine.py:124] Finished request cmpl-0306d8e7c45a4b6ea8fbe496426b94f1-0.\n",
      "INFO:     127.0.0.1:43648 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:48 async_llm_engine.py:124] Finished request cmpl-1b0fe5a4b5a444d4875f4f2cf26c0ad4-0.\n",
      "INFO:     127.0.0.1:56086 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:48 async_llm_engine.py:553] Received request cmpl-cdc009e9203f4e3a865704edb9296695-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:48 async_llm_engine.py:553] Received request cmpl-b17d781bd0f34527a7afafb69405a84e-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:48 async_llm_engine.py:124] Finished request cmpl-1418d8e1628d4e74baa17c6c9982af89-0.\n",
      "INFO:     127.0.0.1:43624 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:48 async_llm_engine.py:553] Received request cmpl-c4373599ae374282b92933a4eaa75970-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:49 metrics.py:341] Avg prompt throughput: 9.0 tokens/s, Avg generation throughput: 626.9 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:124] Finished request cmpl-c19cc8bd82d74ed9ab5e5de6b8f12d74-0.\n",
      "INFO:     127.0.0.1:43594 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:553] Received request cmpl-5b77ff454f6e48bdae2b377412fa5b02-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:124] Finished request cmpl-dba9d98285064ed5b6ba95c01488b752-0.\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:124] Finished request cmpl-7a8ea796c1784feb8dc65901924848ab-0.\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:124] Finished request cmpl-d4870222e7f841218131706138ee0d44-0.\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:124] Finished request cmpl-e05ca4c731534198a6e9a045d32910f5-0.\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:124] Finished request cmpl-ddfa50ee8078433ea117c9bb623bb6d9-0.\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:124] Finished request cmpl-334f756400194a96b7403084a5aeae9d-0.\n",
      "INFO:     127.0.0.1:43596 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:43622 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:43642 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:43684 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:43654 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:43702 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:553] Received request cmpl-ae50848d29b5453082fb7f597500ce50-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:553] Received request cmpl-f8433eb4c66a40c8adc57961dad5473e-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:553] Received request cmpl-4e8c5a2a64b348c982180eb74c4e5e9b-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:553] Received request cmpl-081161bbf350445896fd4ce36ef15823-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:553] Received request cmpl-f4b470a916384419b3932aa685c1cbcb-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:553] Received request cmpl-fa7a45d6ba05489db97fc779880e8f36-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:49 metrics.py:341] Avg prompt throughput: 11.9 tokens/s, Avg generation throughput: 641.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 18.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:124] Finished request cmpl-b0d9f596957b42a6b085e9ae67b13f66-0.\n",
      "INFO:     127.0.0.1:38156 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:49 async_llm_engine.py:553] Received request cmpl-0332c77a31654cc59e4fdb92cead9851-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:52 async_llm_engine.py:124] Finished request cmpl-448cb27da422478c82789029d06ab4f6-0.\n",
      "INFO:     127.0.0.1:37090 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:52 async_llm_engine.py:553] Received request cmpl-8eec6932645c49608ef2c09c62760ea6-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:53 async_llm_engine.py:124] Finished request cmpl-85e8a8b4d81e4887a63818bf53180109-0.\n",
      "INFO:     127.0.0.1:37080 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:53 async_llm_engine.py:553] Received request cmpl-8b65aa4dafb7422e90ed04b3221e679d-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:53 async_llm_engine.py:124] Finished request cmpl-f50171cacb1b4b7b830b5f83da9199a7-0.\n",
      "INFO:     127.0.0.1:38118 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:53 async_llm_engine.py:553] Received request cmpl-048b113c7a8046f7bb8bce691ca78466-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:53 async_llm_engine.py:124] Finished request cmpl-ea7af7023d3e41858c3ecfabe6403546-0.\n",
      "INFO:     127.0.0.1:37130 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:53 async_llm_engine.py:553] Received request cmpl-ff0bca4e9e7c45b5a850f752c8e410f4-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:54 metrics.py:341] Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 688.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:52:54 async_llm_engine.py:124] Finished request cmpl-e77a6da4d9ad4e7b8f701c75a4b53f90-0.\n",
      "INFO:     127.0.0.1:38106 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:54 async_llm_engine.py:553] Received request cmpl-d524eae9f60b4879aedd5ef152c35de6-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:54 async_llm_engine.py:124] Finished request cmpl-c178a731e6c04bacac13f5a269d44b40-0.\n",
      "INFO:     127.0.0.1:56096 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:54 async_llm_engine.py:553] Received request cmpl-d051525f261542b88a907e61606acbd0-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:54 metrics.py:341] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 654.6 tokens/s, Running: 21 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 33.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:52:56 async_llm_engine.py:124] Finished request cmpl-74192c13cfdd4224a041fb2644893bde-0.\n",
      "INFO:     127.0.0.1:37170 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:56 async_llm_engine.py:553] Received request cmpl-df47c85434a643bb86b0cc5550ca1f03-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:57 async_llm_engine.py:124] Finished request cmpl-03b4afebbfd74eba9216ae60223616d3-0.\n",
      "INFO:     127.0.0.1:42906 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:57 async_llm_engine.py:553] Received request cmpl-067aa8c968124789976ee5c7063cd230-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:57 async_llm_engine.py:124] Finished request cmpl-f9ac87ee6ef04d0bac566fc62e76aac7-0.\n",
      "INFO:     127.0.0.1:37148 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:57 async_llm_engine.py:553] Received request cmpl-0abad49f3e824ad0b830af7451862bcf-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:57 async_llm_engine.py:124] Finished request cmpl-cc41fb357c7245dea0ab7df86801e407-0.\n",
      "INFO:     127.0.0.1:38094 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:57 async_llm_engine.py:553] Received request cmpl-8e834378cb5f4823808400166b903d5f-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:58 async_llm_engine.py:124] Finished request cmpl-f4b470a916384419b3932aa685c1cbcb-0.\n",
      "INFO:     127.0.0.1:38292 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:58 async_llm_engine.py:553] Received request cmpl-ff81ba1981184d6e9442351e5b76c6f5-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:58 async_llm_engine.py:124] Finished request cmpl-478bad4374f24ebb998658c50863654b-0.\n",
      "INFO:     127.0.0.1:38100 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:58 async_llm_engine.py:553] Received request cmpl-b1e2ef09fa154878911e40828e6e6be8-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:59 metrics.py:341] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 669.8 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 42.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:52:59 async_llm_engine.py:124] Finished request cmpl-b17d781bd0f34527a7afafb69405a84e-0.\n",
      "INFO:     127.0.0.1:38248 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:59 async_llm_engine.py:553] Received request cmpl-a917379fd86e47ff88213c9215c9d5ad-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:52:59 metrics.py:341] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 635.2 tokens/s, Running: 21 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 45.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:52:59 async_llm_engine.py:124] Finished request cmpl-081161bbf350445896fd4ce36ef15823-0.\n",
      "INFO:     127.0.0.1:37220 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:52:59 async_llm_engine.py:553] Received request cmpl-394435d48fbc499cb8dd9a3b739f82b3-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:53:00 async_llm_engine.py:124] Finished request cmpl-c8b146a638b044a596597519b94922b0-0.\n",
      "INFO:     127.0.0.1:38186 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:00 async_llm_engine.py:553] Received request cmpl-204729a8cfb84bd1b82b411e6d2ccb28-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:53:00 async_llm_engine.py:124] Finished request cmpl-577ef6392ce84916944511742004aed0-0.\n",
      "INFO:     127.0.0.1:38128 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:00 async_llm_engine.py:553] Received request cmpl-e3e925cc83a0425d8bb77828b2cf8be3-0: prompt: 'San Francisco is a', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 23676, 12628, 317, 245], lora_request: None.\n",
      "INFO 06-07 13:53:00 async_llm_engine.py:124] Finished request cmpl-826cf82f0cc4408da5b6198c0a1776c6-0.\n",
      "INFO:     127.0.0.1:38130 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:01 async_llm_engine.py:124] Finished request cmpl-3f192a66a67947148fdf5011ed3f98bd-0.\n",
      "INFO:     127.0.0.1:38140 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:01 async_llm_engine.py:124] Finished request cmpl-a917379fd86e47ff88213c9215c9d5ad-0.\n",
      "INFO:     127.0.0.1:54426 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:01 async_llm_engine.py:124] Finished request cmpl-ae50848d29b5453082fb7f597500ce50-0.\n",
      "INFO:     127.0.0.1:37206 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:02 async_llm_engine.py:124] Finished request cmpl-08fc0e79717d4ea4a5725b84374e6084-0.\n",
      "INFO:     127.0.0.1:37102 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:02 async_llm_engine.py:124] Finished request cmpl-204729a8cfb84bd1b82b411e6d2ccb28-0.\n",
      "INFO:     127.0.0.1:54428 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:03 async_llm_engine.py:124] Finished request cmpl-056c68c7ca2b4b2982b04eea30364202-0.\n",
      "INFO:     127.0.0.1:37108 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:03 async_llm_engine.py:124] Finished request cmpl-ba1c29b619f545c3beeda06dfff24e6f-0.\n",
      "INFO:     127.0.0.1:38228 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:03 async_llm_engine.py:124] Finished request cmpl-4d97a2811da747feb938bad70c2b4500-0.\n",
      "INFO 06-07 13:53:03 async_llm_engine.py:124] Finished request cmpl-8eec6932645c49608ef2c09c62760ea6-0.\n",
      "INFO:     127.0.0.1:38202 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52288 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:03 async_llm_engine.py:124] Finished request cmpl-98287b6942f643cfb9767ad86177b7a8-0.\n",
      "INFO:     127.0.0.1:38172 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:03 async_llm_engine.py:124] Finished request cmpl-32e7adbf13ca411790fc3c070e847527-0.\n",
      "INFO:     127.0.0.1:37114 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:04 metrics.py:341] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 567.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 27.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:53:04 async_llm_engine.py:124] Finished request cmpl-74477d923cca454ea1abb4e02f8a0b21-0.\n",
      "INFO:     127.0.0.1:37136 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:04 async_llm_engine.py:124] Finished request cmpl-8b6d9bc0f7f84b53984a942b82f92b7d-0.\n",
      "INFO:     127.0.0.1:37128 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:04 async_llm_engine.py:124] Finished request cmpl-882c7a751b0c411d888bfebfadd8527e-0.\n",
      "INFO:     127.0.0.1:38200 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:04 metrics.py:341] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 598.4 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 39.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:53:04 async_llm_engine.py:124] Finished request cmpl-8b65aa4dafb7422e90ed04b3221e679d-0.\n",
      "INFO:     127.0.0.1:55794 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:05 async_llm_engine.py:124] Finished request cmpl-6a9d2484c335403d950c63a1b38e0e7a-0.\n",
      "INFO:     127.0.0.1:38244 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:06 async_llm_engine.py:124] Finished request cmpl-b6531b2f073e4d169f21a3b4581e54b3-0.\n",
      "INFO:     127.0.0.1:37144 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:06 async_llm_engine.py:124] Finished request cmpl-f8433eb4c66a40c8adc57961dad5473e-0.\n",
      "INFO:     127.0.0.1:38276 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:06 async_llm_engine.py:124] Finished request cmpl-db66f9e9d1da4b939494fc0a604dcec4-0.\n",
      "INFO:     127.0.0.1:37152 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:06 async_llm_engine.py:124] Finished request cmpl-5b77ff454f6e48bdae2b377412fa5b02-0.\n",
      "INFO:     127.0.0.1:38262 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:07 async_llm_engine.py:124] Finished request cmpl-fa7a45d6ba05489db97fc779880e8f36-0.\n",
      "INFO:     127.0.0.1:38300 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:07 async_llm_engine.py:124] Finished request cmpl-1de4c663c4e44a199d9222458a99950a-0.\n",
      "INFO:     127.0.0.1:37186 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:07 async_llm_engine.py:124] Finished request cmpl-6eb184f262284dc6a587efcd62f538c8-0.\n",
      "INFO:     127.0.0.1:37160 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:08 async_llm_engine.py:124] Finished request cmpl-048b113c7a8046f7bb8bce691ca78466-0.\n",
      "INFO:     127.0.0.1:52300 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:08 async_llm_engine.py:124] Finished request cmpl-cdc009e9203f4e3a865704edb9296695-0.\n",
      "INFO:     127.0.0.1:37192 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:09 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 293.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:53:09 async_llm_engine.py:124] Finished request cmpl-b1e2ef09fa154878911e40828e6e6be8-0.\n",
      "INFO:     127.0.0.1:55852 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:09 async_llm_engine.py:124] Finished request cmpl-c4373599ae374282b92933a4eaa75970-0.\n",
      "INFO:     127.0.0.1:37194 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:09 async_llm_engine.py:124] Finished request cmpl-e3e925cc83a0425d8bb77828b2cf8be3-0.\n",
      "INFO:     127.0.0.1:46364 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:09 async_llm_engine.py:124] Finished request cmpl-4e8c5a2a64b348c982180eb74c4e5e9b-0.\n",
      "INFO:     127.0.0.1:37218 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:09 async_llm_engine.py:124] Finished request cmpl-0332c77a31654cc59e4fdb92cead9851-0.\n",
      "INFO:     127.0.0.1:55778 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:09 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 425.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:53:10 async_llm_engine.py:124] Finished request cmpl-d524eae9f60b4879aedd5ef152c35de6-0.\n",
      "INFO:     127.0.0.1:52306 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:11 async_llm_engine.py:124] Finished request cmpl-ff0bca4e9e7c45b5a850f752c8e410f4-0.\n",
      "INFO:     127.0.0.1:55800 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:11 async_llm_engine.py:124] Finished request cmpl-ff81ba1981184d6e9442351e5b76c6f5-0.\n",
      "INFO:     127.0.0.1:52342 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:12 async_llm_engine.py:124] Finished request cmpl-df47c85434a643bb86b0cc5550ca1f03-0.\n",
      "INFO:     127.0.0.1:52320 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:12 async_llm_engine.py:124] Finished request cmpl-0abad49f3e824ad0b830af7451862bcf-0.\n",
      "INFO:     127.0.0.1:52330 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:13 async_llm_engine.py:124] Finished request cmpl-d051525f261542b88a907e61606acbd0-0.\n",
      "INFO:     127.0.0.1:55816 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:14 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 177.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:53:15 async_llm_engine.py:124] Finished request cmpl-067aa8c968124789976ee5c7063cd230-0.\n",
      "INFO:     127.0.0.1:55826 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:15 async_llm_engine.py:124] Finished request cmpl-8e834378cb5f4823808400166b903d5f-0.\n",
      "INFO:     127.0.0.1:55842 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "INFO 06-07 13:53:16 async_llm_engine.py:124] Finished request cmpl-394435d48fbc499cb8dd9a3b739f82b3-0.\n",
      "INFO:     127.0.0.1:46362 - \"POST /v1/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 13:53:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 43.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:53:24 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.8 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:53:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:53:34 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:53:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:53:44 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:53:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:53:54 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:54:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:54:04 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:54:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:54:14 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:54:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:54:24 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:54:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:54:34 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:54:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:54:44 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:54:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:54:54 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:55:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:55:04 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:55:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:55:14 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:55:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:55:24 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:55:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:55:34 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:55:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:55:44 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:55:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:55:54 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:56:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:56:04 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:56:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:56:14 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:56:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:56:24 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:56:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:56:34 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:56:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:56:44 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:56:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:56:54 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:57:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:57:04 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:57:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:57:14 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:57:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:57:24 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:57:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:57:34 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:57:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:57:44 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:57:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:57:54 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:58:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:58:04 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:58:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:58:14 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:58:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:58:24 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:58:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:58:34 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:58:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:58:44 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:58:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:58:54 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:59:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:59:04 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:59:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:59:14 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:59:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:59:24 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:59:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:59:34 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:59:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:59:44 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:59:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 13:59:54 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:00:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:00:04 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:00:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:00:14 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:00:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:00:24 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:00:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:00:34 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:00:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:00:44 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:00:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:00:54 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:01:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:01:04 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:01:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:01:14 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:01:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:01:24 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:01:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:01:34 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:01:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:01:44 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:01:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:01:54 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:02:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:02:04 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:02:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:02:14 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:02:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:02:24 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "max_workers = 40\n",
    "n_jobs = 100\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as pool:\n",
    "    submits = []\n",
    "    for i in range(n_jobs):\n",
    "        port = 8000 + i % 2\n",
    "        submits.append(pool.submit(make_request, port=port))\n",
    "    monitor_progress(submits)\n",
    "    results = [submit.result() for submit in submits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "48s vs 56s when using a single server. GPU utilization was clearly higher.\n",
    "\n",
    "- 1 gpu, 20 workers, 1:23\n",
    "- 2 gpus, 10 workers, 2:00\n",
    "- 2 gpus, 20 workers, 1:11\n",
    "- 2 gpus, 40 workers, 0:56\n",
    "- 2 gpus, 2 servers, 40 workers, 0:48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 14:02:53 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/home/gbarbadillo/data/deepseekmath', speculative_config=None, tokenizer='/home/gbarbadillo/data/deepseekmath', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/home/gbarbadillo/data/deepseekmath)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 14:02:56 model_runner.py:146] Loading model weights took 12.8725 GB\n",
      "INFO 06-07 14:02:57 gpu_executor.py:83] # GPU blocks: 985, # CPU blocks: 546\n",
      "INFO 06-07 14:02:59 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-07 14:02:59 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-07 14:03:05 model_runner.py:924] Graph capturing finished in 6 secs.\n",
      "INFO 06-07 14:03:05 serving_chat.py:84] Using default chat template:\n",
      "INFO 06-07 14:03:05 serving_chat.py:84] {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n",
      "INFO 06-07 14:03:05 serving_chat.py:84] \n",
      "INFO 06-07 14:03:05 serving_chat.py:84] ' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n",
      "INFO 06-07 14:03:05 serving_chat.py:84] \n",
      "INFO 06-07 14:03:05 serving_chat.py:84] ' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:     Started server process [36719]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-07 14:03:06 serving_embedding.py:141] embedding_mode is False. Embedding API will not work.\n",
      "INFO 06-07 14:03:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "\n",
    "def run_vllm_server():\n",
    "    subprocess.run(['python', '-m', 'vllm.entrypoints.openai.api_server', '--model', '/home/gbarbadillo/data/deepseekmath'])\n",
    "\n",
    "server_thread = threading.Thread(target=run_vllm_server)\n",
    "server_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 14:14:15 async_llm_engine.py:553] Received request cmpl-d58e7cfa9ce642038e8bb3cb31485d7e-0: prompt: 'User: John computes the sum of the elements of each of the 15 two-element subsets of $\\\\{1,2,3,4,5,6\\\\}$. What is the sum of these 15 sums?\\nPlease reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\\nUse all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\\nYou might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\\nUse code even for the simpler calculations to avoid mistakes.\\n\\nAssistant: Sure, we can solve the problem by writing a Python program.\\n\\n```python', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 5726, 25, 3326, 49761, 254, 2555, 280, 254, 4899, 280, 1319, 280, 254, 207, 16, 20, 984, 12, 7633, 31303, 280, 10923, 16, 11, 17, 11, 18, 11, 19, 11, 20, 11, 21, 19265, 2461, 317, 254, 2555, 280, 1069, 207, 16, 20, 28095, 30, 185, 7900, 2806, 3458, 457, 3458, 11, 285, 1957, 520, 2328, 3510, 2383, 357, 63962, 90, 1424, 429, 3510, 317, 245, 2170, 6640, 10858, 13, 185, 9136, 521, 254, 2324, 1757, 279, 254, 2066, 6411, 11, 285, 330, 1079, 11955, 366, 254, 17325, 285, 7667, 8445, 340, 1099, 13, 185, 2054, 1667, 938, 9934, 17658, 1108, 372, 4300, 4027, 11, 16145, 11, 756, 76952, 410, 42234, 276, 8708, 254, 2066, 11, 938, 254, 1329, 5278, 13, 185, 9136, 2985, 1236, 327, 254, 20666, 14365, 276, 4945, 19615, 13, 185, 185, 77398, 25, 20511, 11, 395, 481, 8708, 254, 2066, 457, 4456, 245, 12974, 2043, 13, 185, 185, 10897, 11338], lora_request: None.\n",
      "INFO 06-07 14:14:15 metrics.py:341] Avg prompt throughput: 14.5 tokens/s, Avg generation throughput: 9.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:14:17 async_llm_engine.py:124] Finished request cmpl-d58e7cfa9ce642038e8bb3cb31485d7e-0.\n",
      "INFO:     127.0.0.1:41040 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: \n",
      "import itertools\n",
      "\n",
      "total_sum = 0\n",
      "for subset in itertools.combinations(range(1, 7), 2):\n",
      "    total_sum += sum(subset)\n",
      "\n",
      "print(total_sum)\n",
      "```\n",
      "\n",
      "The answer is $120$. The sum of these 15 sums is $120$. Therefore, the value of the unknown integer is 120. The answer is: $120$\n",
      "Stop reason: None, Finish reason: stop, output tokens: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 14:14:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:14:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:14:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:14:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:15:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:15:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:15:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:15:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:15:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:15:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:16:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:16:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:16:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:16:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:16:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:16:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:17:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:17:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:17:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:17:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:17:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:17:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:18:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:18:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:18:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:18:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:18:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:18:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:19:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:19:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:19:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:19:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'User: John computes the sum of the elements of each of the 15 two-element subsets of $\\\\{1,2,3,4,5,6\\\\}$. What is the sum of these 15 sums?\\nPlease reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\\nUse all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\\nYou might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\\nUse code even for the simpler calculations to avoid mistakes.\\n\\nAssistant: Sure, we can solve the problem by writing a Python program.\\n\\n```python'\n",
    "completion = client.completions.create(model=\"/home/gbarbadillo/data/deepseekmath\",\n",
    "                                      prompt=prompt, max_tokens=640)\n",
    "print(\"Completion result:\", completion.choices[0].text)\n",
    "print(f'Stop reason: {completion.choices[0].stop_reason}, Finish reason: {completion.choices[0].finish_reason}, output tokens: {completion.usage.completion_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 14:14:05 async_llm_engine.py:553] Received request cmpl-ae10c56471af45e9ab018f17e426fbb5-0: prompt: 'User: John computes the sum of the elements of each of the 15 two-element subsets of $\\\\{1,2,3,4,5,6\\\\}$. What is the sum of these 15 sums?\\nPlease reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\\nUse all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\\nYou might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\\nUse code even for the simpler calculations to avoid mistakes.\\n\\nAssistant: Sure, we can solve the problem by writing a Python program.\\n\\n```python', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.9, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['```output'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 5726, 25, 3326, 49761, 254, 2555, 280, 254, 4899, 280, 1319, 280, 254, 207, 16, 20, 984, 12, 7633, 31303, 280, 10923, 16, 11, 17, 11, 18, 11, 19, 11, 20, 11, 21, 19265, 2461, 317, 254, 2555, 280, 1069, 207, 16, 20, 28095, 30, 185, 7900, 2806, 3458, 457, 3458, 11, 285, 1957, 520, 2328, 3510, 2383, 357, 63962, 90, 1424, 429, 3510, 317, 245, 2170, 6640, 10858, 13, 185, 9136, 521, 254, 2324, 1757, 279, 254, 2066, 6411, 11, 285, 330, 1079, 11955, 366, 254, 17325, 285, 7667, 8445, 340, 1099, 13, 185, 2054, 1667, 938, 9934, 17658, 1108, 372, 4300, 4027, 11, 16145, 11, 756, 76952, 410, 42234, 276, 8708, 254, 2066, 11, 938, 254, 1329, 5278, 13, 185, 9136, 2985, 1236, 327, 254, 20666, 14365, 276, 4945, 19615, 13, 185, 185, 77398, 25, 20511, 11, 395, 481, 8708, 254, 2066, 457, 4456, 245, 12974, 2043, 13, 185, 185, 10897, 11338], lora_request: None.\n",
      "INFO 06-07 14:14:05 metrics.py:341] Avg prompt throughput: 34.4 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:14:07 async_llm_engine.py:124] Finished request cmpl-ae10c56471af45e9ab018f17e426fbb5-0.\n",
      "INFO:     127.0.0.1:57380 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: User: John computes the sum of the elements of each of the 15 two-element subsets of $\\{1,2,3,4,5,6\\}$. What is the sum of these 15 sums?\n",
      "Please reason step by step, and put your final answer within \\boxed{}. The answer is a non negative integer.\n",
      "Use all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\n",
      "You might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\n",
      "Use code even for the simpler calculations to avoid mistakes.\n",
      "\n",
      "Assistant: Sure, we can solve the problem by writing a Python program.\n",
      "\n",
      "```python\n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    numbers = [1, 2, 3, 4, 5, 6]\n",
      "    sums = []\n",
      "    for subset in combinations(numbers, 2):\n",
      "        sums.append(sum(subset))\n",
      "    total_sum = sum(sums)\n",
      "    return total_sum\n",
      "\n",
      "result = sum_of_subsets()\n",
      "print(result)\n",
      "```\n",
      "\n",
      "Stop reason: ```output, Finish reason: stop, output tokens: 103\n"
     ]
    }
   ],
   "source": [
    "completion = client.completions.create(model=\"/home/gbarbadillo/data/deepseekmath\",\n",
    "                                       prompt=prompt, max_tokens=640, echo=True,\n",
    "                                       temperature=0.9, top_p=0.9, stop=[\"```output\"])\n",
    "print(\"Completion result:\", completion.choices[0].text)\n",
    "print(f'Stop reason: {completion.choices[0].stop_reason}, Finish reason: {completion.choices[0].finish_reason}, output tokens: {completion.usage.completion_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not return the stop, symbol but we can get it from the stop reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 14:13:56 async_llm_engine.py:553] Received request cmpl-d6998b75f1c540acb332a713b832e02e-0: prompt: 'User: John computes the sum of the elements of each of the 15 two-element subsets of $\\\\{1,2,3,4,5,6\\\\}$. What is the sum of these 15 sums?\\nPlease reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\\nUse all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\\nYou might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\\nUse code even for the simpler calculations to avoid mistakes.\\n\\nAssistant: Sure, we can solve the problem by writing a Python program.\\n\\n```python', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.9, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['```output'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 5726, 25, 3326, 49761, 254, 2555, 280, 254, 4899, 280, 1319, 280, 254, 207, 16, 20, 984, 12, 7633, 31303, 280, 10923, 16, 11, 17, 11, 18, 11, 19, 11, 20, 11, 21, 19265, 2461, 317, 254, 2555, 280, 1069, 207, 16, 20, 28095, 30, 185, 7900, 2806, 3458, 457, 3458, 11, 285, 1957, 520, 2328, 3510, 2383, 357, 63962, 90, 1424, 429, 3510, 317, 245, 2170, 6640, 10858, 13, 185, 9136, 521, 254, 2324, 1757, 279, 254, 2066, 6411, 11, 285, 330, 1079, 11955, 366, 254, 17325, 285, 7667, 8445, 340, 1099, 13, 185, 2054, 1667, 938, 9934, 17658, 1108, 372, 4300, 4027, 11, 16145, 11, 756, 76952, 410, 42234, 276, 8708, 254, 2066, 11, 938, 254, 1329, 5278, 13, 185, 9136, 2985, 1236, 327, 254, 20666, 14365, 276, 4945, 19615, 13, 185, 185, 77398, 25, 20511, 11, 395, 481, 8708, 254, 2066, 457, 4456, 245, 12974, 2043, 13, 185, 185, 10897, 11338], lora_request: None.\n",
      "INFO 06-07 14:13:57 async_llm_engine.py:124] Finished request cmpl-d6998b75f1c540acb332a713b832e02e-0.\n",
      "INFO:     127.0.0.1:33966 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: User: John computes the sum of the elements of each of the 15 two-element subsets of $\\{1,2,3,4,5,6\\}$. What is the sum of these 15 sums?\n",
      "Please reason step by step, and put your final answer within \\boxed{}. The answer is a non negative integer.\n",
      "Use all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\n",
      "You might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\n",
      "Use code even for the simpler calculations to avoid mistakes.\n",
      "\n",
      "Assistant: Sure, we can solve the problem by writing a Python program.\n",
      "\n",
      "```python\n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    # Define the set\n",
      "    numbers = set(range(1, 7))\n",
      "\n",
      "    # Generate all two-element subsets\n",
      "    subsets = list\n",
      "Stop reason: None, Finish reason: length, output tokens: 50\n"
     ]
    }
   ],
   "source": [
    "completion = client.completions.create(model=\"/home/gbarbadillo/data/deepseekmath\",\n",
    "                                       prompt=prompt, max_tokens=50, echo=True,\n",
    "                                       temperature=0.9, top_p=0.9, stop=[\"```output\"])\n",
    "print(\"Completion result:\", completion.choices[0].text)\n",
    "print(f'Stop reason: {completion.choices[0].stop_reason}, Finish reason: {completion.choices[0].finish_reason}, output tokens: {completion.usage.completion_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the information we need on the completion object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the server adding some weird chat template over our request?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API server baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 14:19:44 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/home/gbarbadillo/data/deepseekmath', speculative_config=None, tokenizer='/home/gbarbadillo/data/deepseekmath', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/home/gbarbadillo/data/deepseekmath)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 14:19:46 model_runner.py:146] Loading model weights took 12.8725 GB\n",
      "INFO 06-07 14:19:48 gpu_executor.py:83] # GPU blocks: 985, # CPU blocks: 546\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "\n",
    "def run_vllm_server():\n",
    "    subprocess.run(['python', '-m', 'vllm.entrypoints.openai.api_server', '--model', '/home/gbarbadillo/data/deepseekmath'])\n",
    "\n",
    "server_thread = threading.Thread(target=run_vllm_server)\n",
    "server_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 14:20:48 async_llm_engine.py:553] Received request cmpl-f5c85d3d6fd748d0b5371333a2e90d56-0: prompt: 'User: John computes the sum of the elements of each of the 15 two-element subsets of $\\\\{1,2,3,4,5,6\\\\}$. What is the sum of these 15 sums?\\nPlease reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\\nUse all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\\nYou might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\\nUse code even for the simpler calculations to avoid mistakes.\\n\\nAssistant: Sure, we can solve the problem by writing a Python program.\\n\\n```python', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 5726, 25, 3326, 49761, 254, 2555, 280, 254, 4899, 280, 1319, 280, 254, 207, 16, 20, 984, 12, 7633, 31303, 280, 10923, 16, 11, 17, 11, 18, 11, 19, 11, 20, 11, 21, 19265, 2461, 317, 254, 2555, 280, 1069, 207, 16, 20, 28095, 30, 185, 7900, 2806, 3458, 457, 3458, 11, 285, 1957, 520, 2328, 3510, 2383, 357, 63962, 90, 1424, 429, 3510, 317, 245, 2170, 6640, 10858, 13, 185, 9136, 521, 254, 2324, 1757, 279, 254, 2066, 6411, 11, 285, 330, 1079, 11955, 366, 254, 17325, 285, 7667, 8445, 340, 1099, 13, 185, 2054, 1667, 938, 9934, 17658, 1108, 372, 4300, 4027, 11, 16145, 11, 756, 76952, 410, 42234, 276, 8708, 254, 2066, 11, 938, 254, 1329, 5278, 13, 185, 9136, 2985, 1236, 327, 254, 20666, 14365, 276, 4945, 19615, 13, 185, 185, 77398, 25, 20511, 11, 395, 481, 8708, 254, 2066, 457, 4456, 245, 12974, 2043, 13, 185, 185, 10897, 11338], lora_request: None.\n",
      "INFO 06-07 14:20:50 async_llm_engine.py:124] Finished request cmpl-f5c85d3d6fd748d0b5371333a2e90d56-0.\n",
      "INFO:     127.0.0.1:46802 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: \n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    numbers = list(range(1, 7))\n",
      "    sums = []\n",
      "\n",
      "    for subset in combinations(numbers, 2):\n",
      "        sums.append(sum(subset))\n",
      "\n",
      "    return sum(sums)\n",
      "\n",
      "result = sum_of_subsets()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "210\n",
      "```\n",
      "The sum of these 15 sums is 210. The answer is: $210$.\n",
      "Stop reason: None, Finish reason: stop, output tokens: 118\n",
      "INFO 06-07 14:20:50 async_llm_engine.py:553] Received request cmpl-eff76972d14f4ec880ba58b2c12d3fa6-0: prompt: 'User: John computes the sum of the elements of each of the 15 two-element subsets of $\\\\{1,2,3,4,5,6\\\\}$. What is the sum of these 15 sums?\\nPlease reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\\nUse all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\\nYou might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\\nUse code even for the simpler calculations to avoid mistakes.\\n\\nAssistant: Sure, we can solve the problem by writing a Python program.\\n\\n```python', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 5726, 25, 3326, 49761, 254, 2555, 280, 254, 4899, 280, 1319, 280, 254, 207, 16, 20, 984, 12, 7633, 31303, 280, 10923, 16, 11, 17, 11, 18, 11, 19, 11, 20, 11, 21, 19265, 2461, 317, 254, 2555, 280, 1069, 207, 16, 20, 28095, 30, 185, 7900, 2806, 3458, 457, 3458, 11, 285, 1957, 520, 2328, 3510, 2383, 357, 63962, 90, 1424, 429, 3510, 317, 245, 2170, 6640, 10858, 13, 185, 9136, 521, 254, 2324, 1757, 279, 254, 2066, 6411, 11, 285, 330, 1079, 11955, 366, 254, 17325, 285, 7667, 8445, 340, 1099, 13, 185, 2054, 1667, 938, 9934, 17658, 1108, 372, 4300, 4027, 11, 16145, 11, 756, 76952, 410, 42234, 276, 8708, 254, 2066, 11, 938, 254, 1329, 5278, 13, 185, 9136, 2985, 1236, 327, 254, 20666, 14365, 276, 4945, 19615, 13, 185, 185, 77398, 25, 20511, 11, 395, 481, 8708, 254, 2066, 457, 4456, 245, 12974, 2043, 13, 185, 185, 10897, 11338], lora_request: None.\n",
      "INFO 06-07 14:20:51 metrics.py:341] Avg prompt throughput: 61.9 tokens/s, Avg generation throughput: 39.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:20:52 async_llm_engine.py:124] Finished request cmpl-eff76972d14f4ec880ba58b2c12d3fa6-0.\n",
      "INFO:     127.0.0.1:46802 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: \n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    numbers = list(range(1, 7))\n",
      "    sums = []\n",
      "\n",
      "    for subset in combinations(numbers, 2):\n",
      "        sums.append(sum(subset))\n",
      "\n",
      "    return sum(sums)\n",
      "\n",
      "result = sum_of_subsets()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "210\n",
      "```\n",
      "The sum of these 15 sums is 210. The answer is: $210$.\n",
      "Stop reason: None, Finish reason: stop, output tokens: 118\n",
      "INFO 06-07 14:20:52 async_llm_engine.py:553] Received request cmpl-96a9783cf3834484a987e394a6023914-0: prompt: 'User: John computes the sum of the elements of each of the 15 two-element subsets of $\\\\{1,2,3,4,5,6\\\\}$. What is the sum of these 15 sums?\\nPlease reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\\nUse all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\\nYou might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\\nUse code even for the simpler calculations to avoid mistakes.\\n\\nAssistant: Sure, we can solve the problem by writing a Python program.\\n\\n```python', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 5726, 25, 3326, 49761, 254, 2555, 280, 254, 4899, 280, 1319, 280, 254, 207, 16, 20, 984, 12, 7633, 31303, 280, 10923, 16, 11, 17, 11, 18, 11, 19, 11, 20, 11, 21, 19265, 2461, 317, 254, 2555, 280, 1069, 207, 16, 20, 28095, 30, 185, 7900, 2806, 3458, 457, 3458, 11, 285, 1957, 520, 2328, 3510, 2383, 357, 63962, 90, 1424, 429, 3510, 317, 245, 2170, 6640, 10858, 13, 185, 9136, 521, 254, 2324, 1757, 279, 254, 2066, 6411, 11, 285, 330, 1079, 11955, 366, 254, 17325, 285, 7667, 8445, 340, 1099, 13, 185, 2054, 1667, 938, 9934, 17658, 1108, 372, 4300, 4027, 11, 16145, 11, 756, 76952, 410, 42234, 276, 8708, 254, 2066, 11, 938, 254, 1329, 5278, 13, 185, 9136, 2985, 1236, 327, 254, 20666, 14365, 276, 4945, 19615, 13, 185, 185, 77398, 25, 20511, 11, 395, 481, 8708, 254, 2066, 457, 4456, 245, 12974, 2043, 13, 185, 185, 10897, 11338], lora_request: None.\n",
      "INFO 06-07 14:20:54 async_llm_engine.py:124] Finished request cmpl-96a9783cf3834484a987e394a6023914-0.\n",
      "INFO:     127.0.0.1:46802 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: \n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    numbers = list(range(1, 7))\n",
      "    sums = []\n",
      "\n",
      "    for subset in combinations(numbers, 2):\n",
      "        sums.append(sum(subset))\n",
      "\n",
      "    return sum(sums)\n",
      "\n",
      "result = sum_of_subsets()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "210\n",
      "```\n",
      "The sum of these 15 sums is 210. The answer is: $210$.\n",
      "Stop reason: None, Finish reason: stop, output tokens: 118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 14:21:06 metrics.py:341] Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 10.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:21:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:21:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:21:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:21:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'User: John computes the sum of the elements of each of the 15 two-element subsets of $\\\\{1,2,3,4,5,6\\\\}$. What is the sum of these 15 sums?\\nPlease reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\\nUse all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\\nYou might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\\nUse code even for the simpler calculations to avoid mistakes.\\n\\nAssistant: Sure, we can solve the problem by writing a Python program.\\n\\n```python'\n",
    "\n",
    "for _ in range(3):\n",
    "    completion = client.completions.create(model=\"/home/gbarbadillo/data/deepseekmath\",\n",
    "                                        prompt=prompt, max_tokens=640, \n",
    "                                        temperature=0)\n",
    "    print(\"Completion result:\", completion.choices[0].text)\n",
    "    print(f'Stop reason: {completion.choices[0].stop_reason}, Finish reason: {completion.choices[0].finish_reason}, output tokens: {completion.usage.completion_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Huggingface baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ed279a09c840aaa7564925363b58ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "model_name = \"/home/gbarbadillo/data/deepseekmath\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map='sequential')\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: John computes the sum of the elements of each of the 15 two-element subsets of $\\{1,2,3,4,5,6\\}$. What is the sum of these 15 sums?\n",
      "Please reason step by step, and put your final answer within \\boxed{}. The answer is a non negative integer.\n",
      "Use all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\n",
      "You might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\n",
      "Use code even for the simpler calculations to avoid mistakes.\n",
      "\n",
      "Assistant: Sure, we can solve the problem by writing a Python program.\n",
      "\n",
      "```python\n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    numbers = list(range(1, 7))\n",
      "    sums = []\n",
      "\n",
      "    for subset in combinations(numbers, 2):\n",
      "        sums.append(sum(subset))\n",
      "\n",
      "    return sum(sums)\n",
      "\n",
      "result = sum_of_subsets()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "210\n",
      "```\n",
      "The sum of these 15 sums is 210. The answer is: $210$.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'User: John computes the sum of the elements of each of the 15 two-element subsets of $\\\\{1,2,3,4,5,6\\\\}$. What is the sum of these 15 sums?\\nPlease reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\\nUse all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\\nYou might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\\nUse code even for the simpler calculations to avoid mistakes.\\n\\nAssistant: Sure, we can solve the problem by writing a Python program.\\n\\n```python'\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs.to(model.device), max_new_tokens=640)\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the exact same answer! Thus it seems that the server is not using any weird template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from itertools import combinations\n",
    "\n",
    "def sum_of_subsets():\n",
    "    numbers = list(range(1, 7))\n",
    "    sums = []\n",
    "\n",
    "    for subset in combinations(numbers, 2):\n",
    "        sums.append(sum(subset))\n",
    "\n",
    "    return sum(sums)\n",
    "\n",
    "result = sum_of_subsets()\n",
    "print(result)\n",
    "```\n",
    "```output\n",
    "210\n",
    "```\n",
    "The sum of these 15 sums is 210. The answer is: $210$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default with `auto` is using `torch.bfloat16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 14:39:19 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/home/gbarbadillo/data/deepseekmath', speculative_config=None, tokenizer='/home/gbarbadillo/data/deepseekmath', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/home/gbarbadillo/data/deepseekmath)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 14:39:22 model_runner.py:146] Loading model weights took 12.8725 GB\n",
      "INFO 06-07 14:39:24 gpu_executor.py:83] # GPU blocks: 338, # CPU blocks: 546\n",
      "INFO 06-07 14:39:26 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-07 14:39:26 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-07 14:39:32 model_runner.py:924] Graph capturing finished in 6 secs.\n",
      "INFO 06-07 14:39:32 serving_chat.py:84] Using default chat template:\n",
      "INFO 06-07 14:39:32 serving_chat.py:84] {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n",
      "INFO 06-07 14:39:32 serving_chat.py:84] \n",
      "INFO 06-07 14:39:32 serving_chat.py:84] ' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n",
      "INFO 06-07 14:39:32 serving_chat.py:84] \n",
      "INFO 06-07 14:39:32 serving_chat.py:84] ' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:     Started server process [40897]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-07 14:39:32 serving_embedding.py:141] embedding_mode is False. Embedding API will not work.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "\n",
    "def run_vllm_server():\n",
    "    subprocess.run(['python', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "                    '--model', '/home/gbarbadillo/data/deepseekmath',\n",
    "                    '--dtype', 'auto',\n",
    "                    '--gpu-memory-utilization', '0.7'\n",
    "                    ])\n",
    "\n",
    "server_thread = threading.Thread(target=run_vllm_server)\n",
    "server_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 14:39:33 async_llm_engine.py:553] Received request cmpl-67073e14bec543f8b9c6a9d1cbea3c3a-0: prompt: 'User: John computes the sum of the elements of each of the 15 two-element subsets of $\\\\{1,2,3,4,5,6\\\\}$. What is the sum of these 15 sums?\\nPlease reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\\nUse all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\\nYou might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\\nUse code even for the simpler calculations to avoid mistakes.\\n\\nAssistant: Sure, we can solve the problem by writing a Python program.\\n\\n```python', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 5726, 25, 3326, 49761, 254, 2555, 280, 254, 4899, 280, 1319, 280, 254, 207, 16, 20, 984, 12, 7633, 31303, 280, 10923, 16, 11, 17, 11, 18, 11, 19, 11, 20, 11, 21, 19265, 2461, 317, 254, 2555, 280, 1069, 207, 16, 20, 28095, 30, 185, 7900, 2806, 3458, 457, 3458, 11, 285, 1957, 520, 2328, 3510, 2383, 357, 63962, 90, 1424, 429, 3510, 317, 245, 2170, 6640, 10858, 13, 185, 9136, 521, 254, 2324, 1757, 279, 254, 2066, 6411, 11, 285, 330, 1079, 11955, 366, 254, 17325, 285, 7667, 8445, 340, 1099, 13, 185, 2054, 1667, 938, 9934, 17658, 1108, 372, 4300, 4027, 11, 16145, 11, 756, 76952, 410, 42234, 276, 8708, 254, 2066, 11, 938, 254, 1329, 5278, 13, 185, 9136, 2985, 1236, 327, 254, 20666, 14365, 276, 4945, 19615, 13, 185, 185, 77398, 25, 20511, 11, 395, 481, 8708, 254, 2066, 457, 4456, 245, 12974, 2043, 13, 185, 185, 10897, 11338], lora_request: None.\n",
      "INFO 06-07 14:39:35 async_llm_engine.py:124] Finished request cmpl-67073e14bec543f8b9c6a9d1cbea3c3a-0.\n",
      "INFO:     127.0.0.1:37218 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: \n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    numbers = list(range(1, 7))\n",
      "    sums = []\n",
      "\n",
      "    for subset in combinations(numbers, 2):\n",
      "        sums.append(sum(subset))\n",
      "\n",
      "    return sum(sums)\n",
      "\n",
      "result = sum_of_subsets()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "210\n",
      "```\n",
      "The sum of these 15 sums is 210. The answer is: $210$.\n",
      "Stop reason: None, Finish reason: stop, output tokens: 118\n",
      "INFO 06-07 14:39:35 async_llm_engine.py:553] Received request cmpl-8cddcf8cffaa40fba7047ab85d2c970e-0: prompt: 'User: John computes the sum of the elements of each of the 15 two-element subsets of $\\\\{1,2,3,4,5,6\\\\}$. What is the sum of these 15 sums?\\nPlease reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\\nUse all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\\nYou might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\\nUse code even for the simpler calculations to avoid mistakes.\\n\\nAssistant: Sure, we can solve the problem by writing a Python program.\\n\\n```python', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 5726, 25, 3326, 49761, 254, 2555, 280, 254, 4899, 280, 1319, 280, 254, 207, 16, 20, 984, 12, 7633, 31303, 280, 10923, 16, 11, 17, 11, 18, 11, 19, 11, 20, 11, 21, 19265, 2461, 317, 254, 2555, 280, 1069, 207, 16, 20, 28095, 30, 185, 7900, 2806, 3458, 457, 3458, 11, 285, 1957, 520, 2328, 3510, 2383, 357, 63962, 90, 1424, 429, 3510, 317, 245, 2170, 6640, 10858, 13, 185, 9136, 521, 254, 2324, 1757, 279, 254, 2066, 6411, 11, 285, 330, 1079, 11955, 366, 254, 17325, 285, 7667, 8445, 340, 1099, 13, 185, 2054, 1667, 938, 9934, 17658, 1108, 372, 4300, 4027, 11, 16145, 11, 756, 76952, 410, 42234, 276, 8708, 254, 2066, 11, 938, 254, 1329, 5278, 13, 185, 9136, 2985, 1236, 327, 254, 20666, 14365, 276, 4945, 19615, 13, 185, 185, 77398, 25, 20511, 11, 395, 481, 8708, 254, 2066, 457, 4456, 245, 12974, 2043, 13, 185, 185, 10897, 11338], lora_request: None.\n",
      "INFO 06-07 14:39:37 metrics.py:341] Avg prompt throughput: 62.0 tokens/s, Avg generation throughput: 36.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:39:38 async_llm_engine.py:124] Finished request cmpl-8cddcf8cffaa40fba7047ab85d2c970e-0.\n",
      "INFO:     127.0.0.1:37218 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: \n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    numbers = list(range(1, 7))\n",
      "    sums = []\n",
      "\n",
      "    for subset in combinations(numbers, 2):\n",
      "        sums.append(sum(subset))\n",
      "\n",
      "    return sum(sums)\n",
      "\n",
      "result = sum_of_subsets()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "210\n",
      "```\n",
      "The sum of these 15 sums is 210. The answer is: $210$.\n",
      "Stop reason: None, Finish reason: stop, output tokens: 118\n",
      "INFO 06-07 14:39:38 async_llm_engine.py:553] Received request cmpl-9557bf147a3c4524aa5540cda5d6f296-0: prompt: 'User: John computes the sum of the elements of each of the 15 two-element subsets of $\\\\{1,2,3,4,5,6\\\\}$. What is the sum of these 15 sums?\\nPlease reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\\nUse all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\\nYou might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\\nUse code even for the simpler calculations to avoid mistakes.\\n\\nAssistant: Sure, we can solve the problem by writing a Python program.\\n\\n```python', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=640, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [100000, 5726, 25, 3326, 49761, 254, 2555, 280, 254, 4899, 280, 1319, 280, 254, 207, 16, 20, 984, 12, 7633, 31303, 280, 10923, 16, 11, 17, 11, 18, 11, 19, 11, 20, 11, 21, 19265, 2461, 317, 254, 2555, 280, 1069, 207, 16, 20, 28095, 30, 185, 7900, 2806, 3458, 457, 3458, 11, 285, 1957, 520, 2328, 3510, 2383, 357, 63962, 90, 1424, 429, 3510, 317, 245, 2170, 6640, 10858, 13, 185, 9136, 521, 254, 2324, 1757, 279, 254, 2066, 6411, 11, 285, 330, 1079, 11955, 366, 254, 17325, 285, 7667, 8445, 340, 1099, 13, 185, 2054, 1667, 938, 9934, 17658, 1108, 372, 4300, 4027, 11, 16145, 11, 756, 76952, 410, 42234, 276, 8708, 254, 2066, 11, 938, 254, 1329, 5278, 13, 185, 9136, 2985, 1236, 327, 254, 20666, 14365, 276, 4945, 19615, 13, 185, 185, 77398, 25, 20511, 11, 395, 481, 8708, 254, 2066, 457, 4456, 245, 12974, 2043, 13, 185, 185, 10897, 11338], lora_request: None.\n",
      "INFO 06-07 14:39:40 async_llm_engine.py:124] Finished request cmpl-9557bf147a3c4524aa5540cda5d6f296-0.\n",
      "INFO:     127.0.0.1:37218 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "Completion result: \n",
      "from itertools import combinations\n",
      "\n",
      "def sum_of_subsets():\n",
      "    numbers = list(range(1, 7))\n",
      "    sums = []\n",
      "\n",
      "    for subset in combinations(numbers, 2):\n",
      "        sums.append(sum(subset))\n",
      "\n",
      "    return sum(sums)\n",
      "\n",
      "result = sum_of_subsets()\n",
      "print(result)\n",
      "```\n",
      "```output\n",
      "210\n",
      "```\n",
      "The sum of these 15 sums is 210. The answer is: $210$.\n",
      "Stop reason: None, Finish reason: stop, output tokens: 118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-07 14:39:42 metrics.py:341] Avg prompt throughput: 28.2 tokens/s, Avg generation throughput: 31.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:39:52 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:40:02 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:40:12 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:40:22 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:40:32 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:40:42 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:40:52 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:41:02 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:41:12 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:41:22 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:41:32 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:41:42 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:41:52 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:42:02 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:42:12 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:42:22 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:42:32 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:42:42 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-07 14:42:52 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'User: John computes the sum of the elements of each of the 15 two-element subsets of $\\\\{1,2,3,4,5,6\\\\}$. What is the sum of these 15 sums?\\nPlease reason step by step, and put your final answer within \\\\boxed{}. The answer is a non negative integer.\\nUse all the available information in the problem description, and be very careful with the assumptions and simplifications you make.\\nYou might use python libraries such as sympy, math, scipy or numpy to solve the problem, use the right tool.\\nUse code even for the simpler calculations to avoid mistakes.\\n\\nAssistant: Sure, we can solve the problem by writing a Python program.\\n\\n```python'\n",
    "\n",
    "for _ in range(3):\n",
    "    completion = client.completions.create(model=\"/home/gbarbadillo/data/deepseekmath\",\n",
    "                                        prompt=prompt, max_tokens=640, \n",
    "                                        temperature=0)\n",
    "    print(\"Completion result:\", completion.choices[0].text)\n",
    "    print(f'Stop reason: {completion.choices[0].stop_reason}, Finish reason: {completion.choices[0].finish_reason}, output tokens: {completion.usage.completion_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Stop words. Very easy, just using the openai api.\n",
    "- [x] Chat template. Nothing, the predictions are the same as huggingface.\n",
    "- [ ] dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
